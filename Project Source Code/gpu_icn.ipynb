{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # Resize images to a uniform size\n",
    "    transforms.ToTensor(),         # Convert images to PyTorch tensors\n",
    "])\n",
    "\n",
    "# Download dataset (CIFAR-10 example)\n",
    "dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class InvertibleConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(InvertibleConvLayer, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "    def inverse(self, y):\n",
    "        # Here you would implement the actual inverse operation\n",
    "        return self.conv(y)  # Placeholder for inverse operation\n",
    "\n",
    "class ICN(nn.Module):\n",
    "    def __init__(self, num_layers, in_channels):\n",
    "        super(ICN, self).__init__()\n",
    "        self.layers = nn.ModuleList([InvertibleConvLayer(in_channels) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def inverse(self, y):\n",
    "        for layer in reversed(self.layers):\n",
    "            y = layer.inverse(y)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.009793438017368317, Validation Loss: 0.009816671579269466\n",
      "Epoch 2, Training Loss: 0.007165220566093922, Validation Loss: 0.006002292048874175\n",
      "Epoch 3, Training Loss: 0.002668702742084861, Validation Loss: 0.0024669591684463865\n",
      "Epoch 4, Training Loss: 0.0021411925554275513, Validation Loss: 0.001988921218824567\n",
      "Epoch 5, Training Loss: 0.001885431818664074, Validation Loss: 0.001521603183547972\n",
      "Epoch 6, Training Loss: 0.0008692426490597427, Validation Loss: 0.0008579007580514499\n",
      "Epoch 7, Training Loss: 0.0007008789107203484, Validation Loss: 0.0006724689168793262\n",
      "Epoch 8, Training Loss: 0.000561113643925637, Validation Loss: 0.0005382985087218368\n",
      "Epoch 9, Training Loss: 0.0004282321024220437, Validation Loss: 0.0004420924394548675\n",
      "Epoch 10, Training Loss: 0.00037906388752162457, Validation Loss: 0.0003724950519583194\n",
      "Epoch 11, Training Loss: 0.00033806124702095985, Validation Loss: 0.00032514386315782594\n",
      "Epoch 12, Training Loss: 0.00025341269792988896, Validation Loss: 0.0002857601867171574\n",
      "Epoch 13, Training Loss: 0.00027130270609632134, Validation Loss: 0.0002535422437136198\n",
      "Epoch 14, Training Loss: 0.00023047048307489604, Validation Loss: 0.0002316268020795931\n",
      "Epoch 15, Training Loss: 0.0001796328870113939, Validation Loss: 0.00020555383609207408\n",
      "Epoch 16, Training Loss: 0.0002061914128717035, Validation Loss: 0.00018433175558761166\n",
      "Epoch 17, Training Loss: 0.0001772556424839422, Validation Loss: 0.00016978495225242107\n",
      "Epoch 18, Training Loss: 0.0001732741657178849, Validation Loss: 0.00015975369807298646\n",
      "Epoch 19, Training Loss: 0.00016963003145065159, Validation Loss: 0.00014840398615175728\n",
      "Epoch 20, Training Loss: 0.00013911655696574599, Validation Loss: 0.00013409241628729306\n",
      "Epoch 21, Training Loss: 0.00012972825788892806, Validation Loss: 0.00012456432655401776\n",
      "Epoch 22, Training Loss: 0.00010863033821806312, Validation Loss: 0.00011414020922135823\n",
      "Epoch 23, Training Loss: 0.00011011919559678063, Validation Loss: 0.00010656071802716891\n",
      "Epoch 24, Training Loss: 0.0001008654580800794, Validation Loss: 9.846420743418146e-05\n",
      "Epoch 25, Training Loss: 0.00011396543413866311, Validation Loss: 9.034318798651717e-05\n",
      "Epoch 26, Training Loss: 8.436760981567204e-05, Validation Loss: 8.25523912119175e-05\n",
      "Epoch 27, Training Loss: 7.276527321664616e-05, Validation Loss: 7.714795353833533e-05\n",
      "Epoch 28, Training Loss: 6.679713987978175e-05, Validation Loss: 7.237965594398772e-05\n",
      "Epoch 29, Training Loss: 7.0920679718256e-05, Validation Loss: 6.79945139155818e-05\n",
      "Epoch 30, Training Loss: 4.676049138652161e-05, Validation Loss: 6.386065201898355e-05\n",
      "Epoch 31, Training Loss: 5.585305189015344e-05, Validation Loss: 5.9789245042454596e-05\n",
      "Epoch 32, Training Loss: 5.520320701180026e-05, Validation Loss: 5.656504809113433e-05\n",
      "Epoch 33, Training Loss: 4.745057594846003e-05, Validation Loss: 5.3719397502466386e-05\n",
      "Epoch 34, Training Loss: 5.7315133744850755e-05, Validation Loss: 6.84896952922717e-05\n",
      "Epoch 35, Training Loss: 4.6407963964156806e-05, Validation Loss: 4.850876663854512e-05\n",
      "Epoch 36, Training Loss: 5.211066309129819e-05, Validation Loss: 4.5283848602750346e-05\n",
      "Epoch 37, Training Loss: 3.8709720683982596e-05, Validation Loss: 4.3247359496438255e-05\n",
      "Epoch 38, Training Loss: 4.4844178773928434e-05, Validation Loss: 4.356502156106274e-05\n",
      "Epoch 39, Training Loss: 3.0549981602234766e-05, Validation Loss: 3.9655212108561156e-05\n",
      "Epoch 40, Training Loss: 3.967916200053878e-05, Validation Loss: 3.8177795953059475e-05\n",
      "Epoch 41, Training Loss: 3.561097400961444e-05, Validation Loss: 3.580641415590053e-05\n",
      "Epoch 42, Training Loss: 3.562162964954041e-05, Validation Loss: 3.420068174923153e-05\n",
      "Epoch 43, Training Loss: 2.970815694425255e-05, Validation Loss: 3.2695712086783166e-05\n",
      "Epoch 44, Training Loss: 3.18186757795047e-05, Validation Loss: 3.16289115984847e-05\n",
      "Epoch 45, Training Loss: 3.423553425818682e-05, Validation Loss: 3.366027282305124e-05\n",
      "Epoch 46, Training Loss: 2.3610762582393363e-05, Validation Loss: 2.8699134209728652e-05\n",
      "Epoch 47, Training Loss: 2.5402519895578735e-05, Validation Loss: 3.534839195210433e-05\n",
      "Epoch 48, Training Loss: 3.332515188958496e-05, Validation Loss: 2.704474710374418e-05\n",
      "Epoch 49, Training Loss: 2.3088763555279e-05, Validation Loss: 2.5595170720094804e-05\n",
      "Epoch 50, Training Loss: 2.9494418413378298e-05, Validation Loss: 3.089689088465904e-05\n",
      "Epoch 51, Training Loss: 2.1057796402601525e-05, Validation Loss: 2.3981434968596765e-05\n",
      "Epoch 52, Training Loss: 3.063285112148151e-05, Validation Loss: 3.0904559512265994e-05\n",
      "Epoch 53, Training Loss: 2.2937505491427146e-05, Validation Loss: 2.2002888385217214e-05\n",
      "Epoch 54, Training Loss: 1.8391125195194036e-05, Validation Loss: 2.1348041807370672e-05\n",
      "Epoch 55, Training Loss: 4.149507003603503e-05, Validation Loss: 3.7457387331594374e-05\n",
      "Epoch 56, Training Loss: 2.516404310881626e-05, Validation Loss: 2.0115113914755148e-05\n",
      "Epoch 57, Training Loss: 2.03772506210953e-05, Validation Loss: 1.9708535071647026e-05\n",
      "Epoch 58, Training Loss: 1.774677002686076e-05, Validation Loss: 1.903351201423908e-05\n",
      "Epoch 59, Training Loss: 1.814944153011311e-05, Validation Loss: 1.8595701256310152e-05\n",
      "Epoch 60, Training Loss: 2.456821675878018e-05, Validation Loss: 2.8862146293458226e-05\n",
      "Epoch 61, Training Loss: 1.8780505342874676e-05, Validation Loss: 1.8665902203723424e-05\n",
      "Epoch 62, Training Loss: 1.5668450942030177e-05, Validation Loss: 1.720714090838932e-05\n",
      "Epoch 63, Training Loss: 1.4630101759394165e-05, Validation Loss: 1.671135578362737e-05\n",
      "Epoch 64, Training Loss: 1.5759911548229866e-05, Validation Loss: 1.656190711603749e-05\n",
      "Epoch 65, Training Loss: 2.1527854187297635e-05, Validation Loss: 2.2213731310886457e-05\n",
      "Epoch 66, Training Loss: 1.8362870832788758e-05, Validation Loss: 1.6459587166270472e-05\n",
      "Epoch 67, Training Loss: 1.4000796909385826e-05, Validation Loss: 1.5262535827300753e-05\n",
      "Epoch 68, Training Loss: 1.40106840262888e-05, Validation Loss: 1.49963664562107e-05\n",
      "Epoch 69, Training Loss: 1.82843541551847e-05, Validation Loss: 1.4558442897766912e-05\n",
      "Epoch 70, Training Loss: 1.5800225810380653e-05, Validation Loss: 1.6587308045985715e-05\n",
      "Epoch 71, Training Loss: 1.5692005035816692e-05, Validation Loss: 1.4035763493307146e-05\n",
      "Epoch 72, Training Loss: 1.4883056792314164e-05, Validation Loss: 1.385706007127668e-05\n",
      "Epoch 73, Training Loss: 1.3585044143837877e-05, Validation Loss: 1.3551254312822574e-05\n",
      "Epoch 74, Training Loss: 1.0927586117759347e-05, Validation Loss: 1.3265761307518186e-05\n",
      "Epoch 75, Training Loss: 1.3377159120864235e-05, Validation Loss: 1.3118457431188683e-05\n",
      "Epoch 76, Training Loss: 1.0433817806188017e-05, Validation Loss: 1.2906259346827879e-05\n",
      "Epoch 77, Training Loss: 1.0362164175603539e-05, Validation Loss: 1.2638483970297372e-05\n",
      "Epoch 78, Training Loss: 2.145321741409134e-05, Validation Loss: 2.8487923413727902e-05\n",
      "Epoch 79, Training Loss: 2.0506828150246292e-05, Validation Loss: 1.2444475429497955e-05\n",
      "Epoch 80, Training Loss: 1.0267568541166838e-05, Validation Loss: 1.2105084876050678e-05\n",
      "Epoch 81, Training Loss: 2.5228995582438074e-05, Validation Loss: 2.8261296983766012e-05\n",
      "Epoch 82, Training Loss: 1.678656008152757e-05, Validation Loss: 1.2792997364784149e-05\n",
      "Epoch 83, Training Loss: 1.1853973774123006e-05, Validation Loss: 1.2287343593971815e-05\n",
      "Epoch 84, Training Loss: 1.2426115972630214e-05, Validation Loss: 1.1369636123180597e-05\n",
      "Epoch 85, Training Loss: 8.899687600205652e-06, Validation Loss: 1.1280297162470068e-05\n",
      "Epoch 86, Training Loss: 1.5825306036276743e-05, Validation Loss: 1.1044895817559238e-05\n",
      "Epoch 87, Training Loss: 9.538154699839652e-06, Validation Loss: 1.13207528771153e-05\n",
      "Epoch 88, Training Loss: 7.798053047736175e-06, Validation Loss: 1.077636304917815e-05\n",
      "Epoch 89, Training Loss: 8.989019988803193e-06, Validation Loss: 1.071261333595575e-05\n",
      "Epoch 90, Training Loss: 1.1262024600000586e-05, Validation Loss: 1.1103523569441086e-05\n",
      "Epoch 91, Training Loss: 8.338702173205093e-06, Validation Loss: 1.0336898395409407e-05\n",
      "Epoch 92, Training Loss: 1.0338559150113724e-05, Validation Loss: 1.0226866802411816e-05\n",
      "Epoch 93, Training Loss: 8.884512681106571e-06, Validation Loss: 1.0188837764454912e-05\n",
      "Epoch 94, Training Loss: 1.6313701053149998e-05, Validation Loss: 1.2333921913986665e-05\n",
      "Epoch 95, Training Loss: 1.1603248822211754e-05, Validation Loss: 1.048347568684899e-05\n",
      "Epoch 96, Training Loss: 2.8295593438087963e-05, Validation Loss: 1.190647320877679e-05\n",
      "Epoch 97, Training Loss: 1.0654744073690381e-05, Validation Loss: 9.750668231141494e-06\n",
      "Epoch 98, Training Loss: 1.0816444955707993e-05, Validation Loss: 9.505989857794343e-06\n",
      "Epoch 99, Training Loss: 1.050120044965297e-05, Validation Loss: 9.403213521268791e-06\n",
      "Epoch 100, Training Loss: 9.52993104874622e-06, Validation Loss: 9.32521045907396e-06\n",
      "Epoch 101, Training Loss: 7.648217433597893e-06, Validation Loss: 9.277733170165448e-06\n",
      "Epoch 102, Training Loss: 8.427958164247684e-06, Validation Loss: 9.100387843857203e-06\n",
      "Epoch 103, Training Loss: 9.979516107705422e-06, Validation Loss: 9.781405974622922e-06\n",
      "Epoch 104, Training Loss: 1.0132584975508507e-05, Validation Loss: 9.063798014646817e-06\n",
      "Epoch 105, Training Loss: 1.1072252164012752e-05, Validation Loss: 8.854746225829998e-06\n",
      "Epoch 106, Training Loss: 7.754682883387432e-06, Validation Loss: 8.947492576579439e-06\n",
      "Epoch 107, Training Loss: 1.02707290352555e-05, Validation Loss: 8.676225959812179e-06\n",
      "Epoch 108, Training Loss: 6.634781129832845e-06, Validation Loss: 9.054750138324622e-06\n",
      "Epoch 109, Training Loss: 1.2361264452920295e-05, Validation Loss: 9.105966594567e-06\n",
      "Epoch 110, Training Loss: 1.0641796507115941e-05, Validation Loss: 8.600634860187572e-06\n",
      "Epoch 111, Training Loss: 1.7296080841333605e-05, Validation Loss: 1.0338577977233346e-05\n",
      "Epoch 112, Training Loss: 7.630515028722584e-06, Validation Loss: 9.432104779062585e-06\n",
      "Epoch 113, Training Loss: 9.64944229053799e-06, Validation Loss: 8.128808484838906e-06\n",
      "Epoch 114, Training Loss: 8.03933562565362e-06, Validation Loss: 8.120129614268359e-06\n",
      "Epoch 115, Training Loss: 6.425118044717237e-06, Validation Loss: 7.991684461054192e-06\n",
      "Epoch 116, Training Loss: 6.354024662869051e-06, Validation Loss: 8.040562554281672e-06\n",
      "Epoch 117, Training Loss: 1.9304294255562127e-05, Validation Loss: 1.1899079234087372e-05\n",
      "Epoch 118, Training Loss: 1.3114840839989483e-05, Validation Loss: 1.161873884357076e-05\n",
      "Epoch 119, Training Loss: 7.671182174817659e-06, Validation Loss: 7.78416920270994e-06\n",
      "Epoch 120, Training Loss: 1.05871931737056e-05, Validation Loss: 7.629906533319898e-06\n",
      "Epoch 121, Training Loss: 6.078400474507362e-06, Validation Loss: 7.611027914283843e-06\n",
      "Epoch 122, Training Loss: 5.025607606512494e-06, Validation Loss: 7.968827793294019e-06\n",
      "Epoch 123, Training Loss: 5.714327471650904e-06, Validation Loss: 7.495560193248798e-06\n",
      "Epoch 124, Training Loss: 6.960302471270552e-06, Validation Loss: 7.4500614231695925e-06\n",
      "Epoch 125, Training Loss: 7.373525932052871e-06, Validation Loss: 7.3021199186421505e-06\n",
      "Epoch 126, Training Loss: 6.017603482177947e-06, Validation Loss: 7.425801781626978e-06\n",
      "Epoch 127, Training Loss: 7.999612535058986e-06, Validation Loss: 7.400572980792264e-06\n",
      "Epoch 128, Training Loss: 8.075618097791448e-06, Validation Loss: 7.285308549201945e-06\n",
      "Epoch 129, Training Loss: 6.644727363891434e-06, Validation Loss: 7.3519983836295825e-06\n",
      "Epoch 130, Training Loss: 7.059654308250174e-06, Validation Loss: 7.033639101525301e-06\n",
      "Epoch 131, Training Loss: 8.028246156754903e-06, Validation Loss: 7.0681795784894e-06\n",
      "Epoch 132, Training Loss: 9.728002623887733e-06, Validation Loss: 6.9800127493511405e-06\n",
      "Epoch 133, Training Loss: 8.400037586397957e-06, Validation Loss: 6.8491981611335e-06\n",
      "Epoch 134, Training Loss: 6.439001481339801e-06, Validation Loss: 6.8169142158329785e-06\n",
      "Epoch 135, Training Loss: 1.410494587616995e-05, Validation Loss: 1.0459396194772844e-05\n",
      "Epoch 136, Training Loss: 4.657761564885732e-06, Validation Loss: 6.782122874741108e-06\n",
      "Epoch 137, Training Loss: 6.994511750235688e-06, Validation Loss: 6.68335808038548e-06\n",
      "Epoch 138, Training Loss: 2.949036206700839e-05, Validation Loss: 9.505767859985673e-06\n",
      "Epoch 139, Training Loss: 7.888135769462679e-06, Validation Loss: 6.836042297100515e-06\n",
      "Epoch 140, Training Loss: 6.127948836365249e-06, Validation Loss: 6.513153337460133e-06\n",
      "Epoch 141, Training Loss: 5.276117917674128e-06, Validation Loss: 6.700680590192802e-06\n",
      "Epoch 142, Training Loss: 6.964934982534032e-06, Validation Loss: 6.672785483987684e-06\n",
      "Epoch 143, Training Loss: 7.165038823586656e-06, Validation Loss: 6.840864469870595e-06\n",
      "Epoch 144, Training Loss: 6.748432497261092e-06, Validation Loss: 6.406284480426679e-06\n",
      "Epoch 145, Training Loss: 5.9186168073210865e-06, Validation Loss: 6.259939086964209e-06\n",
      "Epoch 146, Training Loss: 6.368823051161598e-06, Validation Loss: 6.233999088789269e-06\n",
      "Epoch 147, Training Loss: 9.213921657647006e-06, Validation Loss: 8.54614628531253e-06\n",
      "Epoch 148, Training Loss: 2.244093775516376e-05, Validation Loss: 8.203757084386311e-06\n",
      "Epoch 149, Training Loss: 1.2354040336504113e-05, Validation Loss: 6.283249279760732e-06\n",
      "Epoch 150, Training Loss: 4.481823452806566e-06, Validation Loss: 6.524645298315104e-06\n",
      "Epoch 151, Training Loss: 4.578055722959107e-06, Validation Loss: 6.003098004764249e-06\n",
      "Epoch 152, Training Loss: 4.7376606744364835e-06, Validation Loss: 6.045111896724894e-06\n",
      "Epoch 153, Training Loss: 2.1087118511786684e-05, Validation Loss: 2.8820307891976063e-05\n",
      "Epoch 154, Training Loss: 4.487862497626338e-06, Validation Loss: 5.899248868549557e-06\n",
      "Epoch 155, Training Loss: 6.6474603954702616e-06, Validation Loss: 1.0890416172856745e-05\n",
      "Epoch 156, Training Loss: 7.448373708029976e-06, Validation Loss: 5.8326635492559055e-06\n",
      "Epoch 157, Training Loss: 3.86708370569977e-06, Validation Loss: 5.976446424017251e-06\n",
      "Epoch 158, Training Loss: 1.0881902198889293e-05, Validation Loss: 6.792961714126928e-06\n",
      "Epoch 159, Training Loss: 8.733746653888375e-06, Validation Loss: 1.4203672337061785e-05\n",
      "Epoch 160, Training Loss: 5.635894922306761e-06, Validation Loss: 5.597630556940493e-06\n",
      "Epoch 161, Training Loss: 6.457115432567662e-06, Validation Loss: 5.6555134239721494e-06\n",
      "Epoch 162, Training Loss: 5.05591788169113e-06, Validation Loss: 6.330456660185855e-06\n",
      "Epoch 163, Training Loss: 5.868877451575827e-06, Validation Loss: 6.020743515532298e-06\n",
      "Epoch 164, Training Loss: 5.307109859131742e-06, Validation Loss: 6.058028928607785e-06\n",
      "Epoch 165, Training Loss: 9.055911505129188e-06, Validation Loss: 8.94794409145871e-06\n",
      "Epoch 166, Training Loss: 6.098381163610611e-06, Validation Loss: 5.7027603022460004e-06\n",
      "Epoch 167, Training Loss: 4.120334779145196e-06, Validation Loss: 5.319707119629042e-06\n",
      "Epoch 168, Training Loss: 4.37298967881361e-06, Validation Loss: 5.507581118665002e-06\n",
      "Epoch 169, Training Loss: 5.817448254674673e-06, Validation Loss: 5.276506694797857e-06\n",
      "Epoch 170, Training Loss: 5.267558208288392e-06, Validation Loss: 5.313034288791971e-06\n",
      "Epoch 171, Training Loss: 6.893332738400204e-06, Validation Loss: 5.409424767575422e-06\n",
      "Epoch 172, Training Loss: 5.392537332227221e-06, Validation Loss: 5.501592077226708e-06\n",
      "Epoch 173, Training Loss: 4.829224963032175e-06, Validation Loss: 5.2040102537210684e-06\n",
      "Epoch 174, Training Loss: 3.897229362337384e-06, Validation Loss: 5.0913283859213125e-06\n",
      "Epoch 175, Training Loss: 2.7912072255276144e-05, Validation Loss: 9.796160576828187e-06\n",
      "Epoch 176, Training Loss: 4.9331283662468195e-06, Validation Loss: 5.057754559897681e-06\n",
      "Epoch 177, Training Loss: 2.3209116989164613e-05, Validation Loss: 9.253357652046274e-06\n",
      "Epoch 178, Training Loss: 5.390443220676389e-06, Validation Loss: 1.1736577718670126e-05\n",
      "Epoch 179, Training Loss: 6.77237767376937e-06, Validation Loss: 7.146653031923283e-06\n",
      "Epoch 180, Training Loss: 4.269259989087004e-06, Validation Loss: 4.906251306122259e-06\n",
      "Epoch 181, Training Loss: 3.6392036690813256e-06, Validation Loss: 4.8861354976779576e-06\n",
      "Epoch 182, Training Loss: 5.785613666375866e-06, Validation Loss: 4.880997737487563e-06\n",
      "Epoch 183, Training Loss: 4.509732661972521e-06, Validation Loss: 5.177228208276284e-06\n",
      "Epoch 184, Training Loss: 9.282295650336891e-06, Validation Loss: 4.7713888958751805e-06\n",
      "Epoch 185, Training Loss: 3.474067625575117e-06, Validation Loss: 4.764944372828334e-06\n",
      "Epoch 186, Training Loss: 3.2157399800780695e-06, Validation Loss: 4.725185748217772e-06\n",
      "Epoch 187, Training Loss: 5.668301128025632e-06, Validation Loss: 4.744740779318192e-06\n",
      "Epoch 188, Training Loss: 3.950330665247748e-06, Validation Loss: 4.754340579893324e-06\n",
      "Epoch 189, Training Loss: 3.3184833228006028e-06, Validation Loss: 4.862886829011881e-06\n",
      "Epoch 190, Training Loss: 1.5211385289148893e-05, Validation Loss: 8.575782471403716e-06\n",
      "Epoch 191, Training Loss: 4.425432052812539e-06, Validation Loss: 4.870535118984923e-06\n",
      "Epoch 192, Training Loss: 5.0851740525104105e-06, Validation Loss: 5.052147688712368e-06\n",
      "Epoch 193, Training Loss: 3.886689228238538e-06, Validation Loss: 4.814454123160355e-06\n",
      "Epoch 194, Training Loss: 4.737318704428617e-06, Validation Loss: 6.798800699077109e-06\n",
      "Epoch 195, Training Loss: 9.329348358733114e-06, Validation Loss: 4.454046685185957e-06\n",
      "Epoch 196, Training Loss: 3.197740397808957e-06, Validation Loss: 4.4120120515324475e-06\n",
      "Epoch 197, Training Loss: 6.1135087889852e-06, Validation Loss: 4.669747007717853e-06\n",
      "Epoch 198, Training Loss: 3.12803422275465e-06, Validation Loss: 4.364573946204589e-06\n",
      "Epoch 199, Training Loss: 5.4216448006627616e-06, Validation Loss: 4.394822301883265e-06\n",
      "Epoch 200, Training Loss: 4.376269316708203e-06, Validation Loss: 4.427233260725721e-06\n",
      "Epoch 201, Training Loss: 9.780629625311121e-06, Validation Loss: 1.60798866838064e-05\n",
      "Epoch 202, Training Loss: 3.978407221438829e-06, Validation Loss: 4.7109844026462195e-06\n",
      "Epoch 203, Training Loss: 5.409154255175963e-06, Validation Loss: 4.347492130983964e-06\n",
      "Epoch 204, Training Loss: 4.194882421870716e-06, Validation Loss: 4.3793901666758985e-06\n",
      "Epoch 205, Training Loss: 3.1575591492583044e-06, Validation Loss: 4.325402853723132e-06\n",
      "Epoch 206, Training Loss: 3.439447482378455e-06, Validation Loss: 4.1496000159466955e-06\n",
      "Epoch 207, Training Loss: 6.398450932465494e-05, Validation Loss: 2.5324229908080855e-05\n",
      "Epoch 208, Training Loss: 6.231212864804547e-06, Validation Loss: 4.309074916922918e-06\n",
      "Epoch 209, Training Loss: 4.24693780587404e-06, Validation Loss: 4.263308241276202e-06\n",
      "Epoch 210, Training Loss: 1.7959337128559127e-05, Validation Loss: 3.5823514718587575e-05\n",
      "Epoch 211, Training Loss: 2.4682553885213565e-06, Validation Loss: 4.057166453169705e-06\n",
      "Epoch 212, Training Loss: 3.6693304537038784e-06, Validation Loss: 4.073616388256834e-06\n",
      "Epoch 213, Training Loss: 2.6281975351594156e-06, Validation Loss: 4.881374452220573e-06\n",
      "Epoch 214, Training Loss: 3.1921208574203774e-06, Validation Loss: 4.008490882543886e-06\n",
      "Epoch 215, Training Loss: 3.167026761730085e-06, Validation Loss: 3.958667966972901e-06\n",
      "Epoch 216, Training Loss: 5.494466222444316e-06, Validation Loss: 4.28617416184155e-06\n",
      "Epoch 217, Training Loss: 7.740457476757001e-06, Validation Loss: 5.599780469177406e-06\n",
      "Epoch 218, Training Loss: 2.817357653839281e-06, Validation Loss: 3.883772826307545e-06\n",
      "Epoch 219, Training Loss: 4.038141923956573e-06, Validation Loss: 3.8801838544928315e-06\n",
      "Epoch 220, Training Loss: 4.5201441025710665e-06, Validation Loss: 4.8287092983633895e-06\n",
      "Epoch 221, Training Loss: 6.460850272560492e-06, Validation Loss: 5.176426335183235e-06\n",
      "Epoch 222, Training Loss: 7.2049570007948205e-06, Validation Loss: 7.959876436372932e-06\n",
      "Epoch 223, Training Loss: 3.776877520067501e-06, Validation Loss: 3.8482504802973645e-06\n",
      "Epoch 224, Training Loss: 3.2962841487460537e-06, Validation Loss: 4.211151112621957e-06\n",
      "Epoch 225, Training Loss: 3.995623046648689e-06, Validation Loss: 3.7876180217869317e-06\n",
      "Epoch 226, Training Loss: 5.171999418962514e-06, Validation Loss: 3.9341960366223795e-06\n",
      "Epoch 227, Training Loss: 4.791913397639291e-06, Validation Loss: 9.081588782435713e-06\n",
      "Epoch 228, Training Loss: 2.5459555672568968e-06, Validation Loss: 3.7347638313386963e-06\n",
      "Epoch 229, Training Loss: 7.014879884081893e-06, Validation Loss: 4.087102758030075e-06\n",
      "Epoch 230, Training Loss: 4.386585715110414e-06, Validation Loss: 3.755030929447446e-06\n",
      "Epoch 231, Training Loss: 6.258370376599487e-06, Validation Loss: 8.501322696293715e-06\n",
      "Epoch 232, Training Loss: 2.466010755597381e-06, Validation Loss: 3.895821150270967e-06\n",
      "Epoch 233, Training Loss: 5.38252334081335e-06, Validation Loss: 3.625746306830583e-06\n",
      "Epoch 234, Training Loss: 5.3802900765731465e-06, Validation Loss: 6.12277436039511e-06\n",
      "Epoch 235, Training Loss: 3.5873151773557765e-06, Validation Loss: 3.806788595139337e-06\n",
      "Epoch 236, Training Loss: 4.57087571703596e-06, Validation Loss: 3.5701522556279367e-06\n",
      "Epoch 237, Training Loss: 3.4404117741360096e-06, Validation Loss: 3.7184295990393283e-06\n",
      "Epoch 238, Training Loss: 2.2404378796636593e-06, Validation Loss: 3.526445114078626e-06\n",
      "Epoch 239, Training Loss: 4.320809694036143e-06, Validation Loss: 8.205951199803619e-06\n",
      "Epoch 240, Training Loss: 3.426143621254596e-06, Validation Loss: 3.466451684214187e-06\n",
      "Epoch 241, Training Loss: 2.443298626531032e-06, Validation Loss: 3.5532159197329956e-06\n",
      "Epoch 242, Training Loss: 2.1613209355564322e-06, Validation Loss: 3.468882376147834e-06\n",
      "Epoch 243, Training Loss: 6.25403208687203e-06, Validation Loss: 5.759652978644701e-06\n",
      "Epoch 244, Training Loss: 3.3376172723365016e-06, Validation Loss: 4.085190325287122e-06\n",
      "Epoch 245, Training Loss: 4.449384505278431e-06, Validation Loss: 3.879795635665611e-06\n",
      "Epoch 246, Training Loss: 1.2533615517895669e-05, Validation Loss: 5.716174443947169e-06\n",
      "Epoch 247, Training Loss: 1.331463863607496e-05, Validation Loss: 8.100473348297487e-06\n",
      "Epoch 248, Training Loss: 2.4490971100021852e-06, Validation Loss: 3.344746474725586e-06\n",
      "Epoch 249, Training Loss: 3.37982010023552e-06, Validation Loss: 3.3268019987114894e-06\n",
      "Epoch 250, Training Loss: 3.6897561130899703e-06, Validation Loss: 3.7512067004630068e-06\n",
      "Epoch 251, Training Loss: 4.522268227447057e-06, Validation Loss: 6.511973977722168e-06\n",
      "Epoch 252, Training Loss: 2.1625937733915634e-06, Validation Loss: 3.28644343272763e-06\n",
      "Epoch 253, Training Loss: 1.754633558448404e-06, Validation Loss: 3.2863083995568563e-06\n",
      "Epoch 254, Training Loss: 4.8663077905075625e-06, Validation Loss: 5.254758385362832e-06\n",
      "Epoch 255, Training Loss: 2.636036242620321e-06, Validation Loss: 3.2656328521567327e-06\n",
      "Epoch 256, Training Loss: 3.7163204069656786e-06, Validation Loss: 3.2520345841221337e-06\n",
      "Epoch 257, Training Loss: 2.6784689453052124e-06, Validation Loss: 3.2204225881345058e-06\n",
      "Epoch 258, Training Loss: 2.2288927539193537e-06, Validation Loss: 3.2144161786783596e-06\n",
      "Epoch 259, Training Loss: 4.060513219883433e-06, Validation Loss: 4.040926229023333e-06\n",
      "Epoch 260, Training Loss: 3.329365426907316e-06, Validation Loss: 3.2362667722336947e-06\n",
      "Epoch 261, Training Loss: 3.0600167519878596e-06, Validation Loss: 3.2531302725198952e-06\n",
      "Epoch 262, Training Loss: 3.2200405257754028e-06, Validation Loss: 3.1950744675327045e-06\n",
      "Epoch 263, Training Loss: 3.6841379369434435e-06, Validation Loss: 3.1419238078800105e-06\n",
      "Epoch 264, Training Loss: 5.527369921765057e-06, Validation Loss: 3.8562821121495235e-06\n",
      "Epoch 265, Training Loss: 2.8765771276084706e-06, Validation Loss: 3.1377637248051022e-06\n",
      "Epoch 266, Training Loss: 2.2766885194869246e-06, Validation Loss: 3.2193740450166398e-06\n",
      "Epoch 267, Training Loss: 4.354483280621935e-06, Validation Loss: 3.112629856824607e-06\n",
      "Epoch 268, Training Loss: 2.968557510030223e-06, Validation Loss: 3.3910065230073343e-06\n",
      "Epoch 269, Training Loss: 5.159430202184012e-06, Validation Loss: 3.154672659275672e-06\n",
      "Epoch 270, Training Loss: 2.5344306777697057e-06, Validation Loss: 3.6767722710007378e-06\n",
      "Epoch 271, Training Loss: 2.1870664568268694e-06, Validation Loss: 3.0886147672558504e-06\n",
      "Epoch 272, Training Loss: 2.3157817850005813e-06, Validation Loss: 3.0195168978847922e-06\n",
      "Epoch 273, Training Loss: 1.858605855886708e-06, Validation Loss: 3.0562350809832077e-06\n",
      "Epoch 274, Training Loss: 9.991628758143634e-06, Validation Loss: 1.3318576724961797e-05\n",
      "Epoch 275, Training Loss: 2.694845534279011e-06, Validation Loss: 4.790012983839857e-06\n",
      "Epoch 276, Training Loss: 3.445671154622687e-06, Validation Loss: 3.0573066923914517e-06\n",
      "Epoch 277, Training Loss: 9.438252163818106e-06, Validation Loss: 3.171622523904302e-06\n",
      "Epoch 278, Training Loss: 2.552741989347851e-06, Validation Loss: 3.00206967144413e-06\n",
      "Epoch 279, Training Loss: 4.213286501908442e-06, Validation Loss: 2.949011266849947e-06\n",
      "Epoch 280, Training Loss: 7.834973075659946e-06, Validation Loss: 3.919447107173081e-06\n",
      "Epoch 281, Training Loss: 6.1942751017340925e-06, Validation Loss: 6.593285932259005e-06\n",
      "Epoch 282, Training Loss: 2.3506800062023103e-06, Validation Loss: 3.0474432998315626e-06\n",
      "Epoch 283, Training Loss: 2.7738733479054645e-06, Validation Loss: 2.9400527411410035e-06\n",
      "Epoch 284, Training Loss: 2.7209680411033332e-06, Validation Loss: 2.9443361867448796e-06\n",
      "Epoch 285, Training Loss: 2.0644135929615004e-06, Validation Loss: 2.8701318029608007e-06\n",
      "Epoch 286, Training Loss: 2.4012558696995256e-06, Validation Loss: 3.2227940651563393e-06\n",
      "Epoch 287, Training Loss: 4.508330675889738e-06, Validation Loss: 2.90605991322545e-06\n",
      "Epoch 288, Training Loss: 2.1472815205925144e-06, Validation Loss: 2.9069020430449728e-06\n",
      "Epoch 289, Training Loss: 9.456345651415177e-06, Validation Loss: 1.043380015214277e-05\n",
      "Epoch 290, Training Loss: 3.7252582387736766e-06, Validation Loss: 1.0774774943983736e-05\n",
      "Epoch 291, Training Loss: 2.4649743863847107e-06, Validation Loss: 2.830049446028326e-06\n",
      "Epoch 292, Training Loss: 3.6636429285863414e-06, Validation Loss: 3.894986276133698e-06\n",
      "Epoch 293, Training Loss: 2.5339909370813984e-06, Validation Loss: 2.8219877074395303e-06\n",
      "Epoch 294, Training Loss: 2.7818894068332156e-06, Validation Loss: 2.998349526018165e-06\n",
      "Epoch 295, Training Loss: 2.860351514755166e-06, Validation Loss: 2.810742765635209e-06\n",
      "Epoch 296, Training Loss: 2.268464868393494e-06, Validation Loss: 2.8692772465182068e-06\n",
      "Epoch 297, Training Loss: 2.0874226720479783e-06, Validation Loss: 2.7819643409416737e-06\n",
      "Epoch 298, Training Loss: 3.4148458780691726e-06, Validation Loss: 3.0317881059797464e-06\n",
      "Epoch 299, Training Loss: 2.461524672980886e-06, Validation Loss: 2.7563716553318537e-06\n",
      "Epoch 300, Training Loss: 3.7824333958269563e-06, Validation Loss: 3.766505865517438e-06\n",
      "Epoch 301, Training Loss: 1.6646183667035075e-06, Validation Loss: 2.7326057455877836e-06\n",
      "Epoch 302, Training Loss: 2.054051492450526e-06, Validation Loss: 2.7449352938576084e-06\n",
      "Epoch 303, Training Loss: 2.0483976186369546e-06, Validation Loss: 2.737610077257292e-06\n",
      "Epoch 304, Training Loss: 4.061144409206463e-06, Validation Loss: 4.109853756409005e-06\n",
      "Epoch 305, Training Loss: 3.0708624763065018e-06, Validation Loss: 2.829483663567105e-06\n",
      "Epoch 306, Training Loss: 3.3955539038288407e-06, Validation Loss: 2.8220368165328265e-06\n",
      "Epoch 307, Training Loss: 2.0130150915065315e-06, Validation Loss: 2.911678775837723e-06\n",
      "Epoch 308, Training Loss: 4.009403255622601e-06, Validation Loss: 5.844524753373876e-06\n",
      "Epoch 309, Training Loss: 2.0722588942589937e-06, Validation Loss: 2.668554386427149e-06\n",
      "Epoch 310, Training Loss: 3.4047923236357747e-06, Validation Loss: 2.7048982661121056e-06\n",
      "Epoch 311, Training Loss: 4.607704795489553e-06, Validation Loss: 5.795715489199257e-06\n",
      "Epoch 312, Training Loss: 3.7960376175760757e-06, Validation Loss: 4.097720100698254e-06\n",
      "Epoch 313, Training Loss: 3.6057308534509502e-06, Validation Loss: 3.1282637166534104e-06\n",
      "Epoch 314, Training Loss: 8.320740562339779e-06, Validation Loss: 3.9035902929731055e-06\n",
      "Epoch 315, Training Loss: 2.221019485659781e-06, Validation Loss: 2.6316560386557442e-06\n",
      "Epoch 316, Training Loss: 2.034700401054579e-06, Validation Loss: 3.042838183475373e-06\n",
      "Epoch 317, Training Loss: 1.98837551579345e-06, Validation Loss: 2.6102716661324655e-06\n",
      "Epoch 318, Training Loss: 1.8998588302565622e-06, Validation Loss: 2.667653107590731e-06\n",
      "Epoch 319, Training Loss: 1.926865934365196e-06, Validation Loss: 2.6352523798395504e-06\n",
      "Epoch 320, Training Loss: 2.371817117818864e-06, Validation Loss: 2.607761681612168e-06\n",
      "Epoch 321, Training Loss: 2.438796173009905e-06, Validation Loss: 2.6453005844744955e-06\n",
      "Epoch 322, Training Loss: 2.5569277113390854e-06, Validation Loss: 2.5935711558414384e-06\n",
      "Epoch 323, Training Loss: 3.253420800319873e-06, Validation Loss: 3.997085372201622e-06\n",
      "Epoch 324, Training Loss: 8.063078894338105e-06, Validation Loss: 1.8471703946384287e-05\n",
      "Epoch 325, Training Loss: 2.17464730667416e-06, Validation Loss: 2.5463302508333478e-06\n",
      "Epoch 326, Training Loss: 2.0870734260824975e-06, Validation Loss: 2.5884447324735364e-06\n",
      "Epoch 327, Training Loss: 2.6605928269418655e-06, Validation Loss: 3.2894258562983e-06\n",
      "Epoch 328, Training Loss: 6.341745574900415e-06, Validation Loss: 5.463914622694597e-06\n",
      "Epoch 329, Training Loss: 2.2573640308110043e-06, Validation Loss: 2.5991511491411307e-06\n",
      "Epoch 330, Training Loss: 2.238739170934423e-06, Validation Loss: 3.0414400352285133e-06\n",
      "Epoch 331, Training Loss: 2.4186190330510726e-06, Validation Loss: 2.633023100431627e-06\n",
      "Epoch 332, Training Loss: 2.457939444866497e-06, Validation Loss: 3.0848005550231703e-06\n",
      "Epoch 333, Training Loss: 3.648873644124251e-06, Validation Loss: 3.1468379750254062e-06\n",
      "Epoch 334, Training Loss: 5.002843408874469e-06, Validation Loss: 2.8965960212821545e-06\n",
      "Epoch 335, Training Loss: 4.191446350887418e-06, Validation Loss: 3.120357360455221e-06\n",
      "Epoch 336, Training Loss: 2.4623750505270436e-06, Validation Loss: 2.5491254336305298e-06\n",
      "Epoch 337, Training Loss: 2.3961426904861582e-06, Validation Loss: 2.6259824159178087e-06\n",
      "Epoch 338, Training Loss: 1.9399328721192433e-06, Validation Loss: 2.5330837313228994e-06\n",
      "Epoch 339, Training Loss: 1.8159121282224078e-06, Validation Loss: 2.4474486566481818e-06\n",
      "Epoch 340, Training Loss: 3.3589744816708844e-06, Validation Loss: 2.626530022243274e-06\n",
      "Epoch 341, Training Loss: 2.7253645384917036e-06, Validation Loss: 2.5013176288440803e-06\n",
      "Epoch 342, Training Loss: 9.439215318707284e-06, Validation Loss: 1.6440815908159945e-05\n",
      "Epoch 343, Training Loss: 3.3348055694659706e-06, Validation Loss: 3.526139092283319e-06\n",
      "Epoch 344, Training Loss: 2.7714572752302047e-06, Validation Loss: 2.63715749752241e-06\n",
      "Epoch 345, Training Loss: 4.6345398914127145e-06, Validation Loss: 4.813974642767244e-06\n",
      "Epoch 346, Training Loss: 3.1602303351974115e-06, Validation Loss: 2.484997256925355e-06\n",
      "Epoch 347, Training Loss: 2.2900744625076186e-06, Validation Loss: 2.6535884874583585e-06\n",
      "Epoch 348, Training Loss: 2.034116278082365e-06, Validation Loss: 2.6189941975175387e-06\n",
      "Epoch 349, Training Loss: 2.283839194205939e-06, Validation Loss: 2.4556015670119903e-06\n",
      "Epoch 350, Training Loss: 2.8155085601611063e-05, Validation Loss: 6.923294193395911e-06\n",
      "Epoch 351, Training Loss: 1.8870450730901212e-06, Validation Loss: 2.390997864332945e-06\n",
      "Epoch 352, Training Loss: 1.7129814295913093e-06, Validation Loss: 2.3934218067442073e-06\n",
      "Epoch 353, Training Loss: 3.0336777854245156e-06, Validation Loss: 2.5760915250314256e-06\n",
      "Epoch 354, Training Loss: 1.9587673705245834e-06, Validation Loss: 2.385482688461394e-06\n",
      "Epoch 355, Training Loss: 2.543817572586704e-06, Validation Loss: 2.6181959305420164e-06\n",
      "Epoch 356, Training Loss: 2.1033711163909175e-06, Validation Loss: 2.3575372360039856e-06\n",
      "Epoch 357, Training Loss: 4.698104930866975e-06, Validation Loss: 4.136666739156049e-06\n",
      "Epoch 358, Training Loss: 1.5527771211054642e-06, Validation Loss: 2.34014853330261e-06\n",
      "Epoch 359, Training Loss: 5.938632057223003e-06, Validation Loss: 3.618619229009536e-06\n",
      "Epoch 360, Training Loss: 1.3151700841262937e-05, Validation Loss: 1.0369499209822618e-05\n",
      "Epoch 361, Training Loss: 1.8393905065750005e-06, Validation Loss: 2.3424440284143667e-06\n",
      "Epoch 362, Training Loss: 4.492671905609313e-06, Validation Loss: 2.4200422803828095e-06\n",
      "Epoch 363, Training Loss: 2.299028437846573e-06, Validation Loss: 2.3174228135903135e-06\n",
      "Epoch 364, Training Loss: 2.6818033802555874e-06, Validation Loss: 2.6024616932008242e-06\n",
      "Epoch 365, Training Loss: 2.5387180357938632e-06, Validation Loss: 2.3231225231127866e-06\n",
      "Epoch 366, Training Loss: 1.8584648842079332e-06, Validation Loss: 2.3144923112329033e-06\n",
      "Epoch 367, Training Loss: 8.635851372673642e-06, Validation Loss: 5.802716720035666e-06\n",
      "Epoch 368, Training Loss: 2.127879497493268e-06, Validation Loss: 2.462463605332429e-06\n",
      "Epoch 369, Training Loss: 4.653782070818124e-06, Validation Loss: 3.71456291562426e-06\n",
      "Epoch 370, Training Loss: 2.4834512259985786e-06, Validation Loss: 2.560653514314497e-06\n",
      "Epoch 371, Training Loss: 5.1282031563459896e-06, Validation Loss: 7.056988052119928e-06\n",
      "Epoch 372, Training Loss: 4.1718776628840715e-06, Validation Loss: 2.3951617591754568e-06\n",
      "Epoch 373, Training Loss: 1.8395001006865641e-06, Validation Loss: 2.350869568142411e-06\n",
      "Epoch 374, Training Loss: 4.790615548699861e-06, Validation Loss: 1.663391898326119e-05\n",
      "Epoch 375, Training Loss: 2.5723575163283385e-06, Validation Loss: 2.816556984233647e-06\n",
      "Epoch 376, Training Loss: 2.2422236725105904e-06, Validation Loss: 2.3886192692116543e-06\n",
      "Epoch 377, Training Loss: 3.830521563941147e-06, Validation Loss: 2.264762181500379e-06\n",
      "Epoch 378, Training Loss: 4.210696715745144e-06, Validation Loss: 2.484532704989865e-06\n",
      "Epoch 379, Training Loss: 2.4809010028548073e-06, Validation Loss: 2.3242549464794716e-06\n",
      "Epoch 380, Training Loss: 1.1569580237846822e-05, Validation Loss: 1.598698967844574e-05\n",
      "Epoch 381, Training Loss: 4.464389348868281e-06, Validation Loss: 3.205327095584273e-06\n",
      "Epoch 382, Training Loss: 1.7301721300100326e-06, Validation Loss: 2.471211482365234e-06\n",
      "Epoch 383, Training Loss: 3.962614755437244e-06, Validation Loss: 2.442743223605683e-06\n",
      "Epoch 384, Training Loss: 1.3715990462515038e-05, Validation Loss: 3.298873995011311e-06\n",
      "Epoch 385, Training Loss: 3.747228220163379e-06, Validation Loss: 2.2853106545324897e-06\n",
      "Epoch 386, Training Loss: 1.663834041210066e-06, Validation Loss: 2.2177286177503927e-06\n",
      "Epoch 387, Training Loss: 2.2416736555896932e-06, Validation Loss: 2.3659830141615045e-06\n",
      "Epoch 388, Training Loss: 2.3305369722947944e-06, Validation Loss: 2.2285724760118325e-06\n",
      "Epoch 389, Training Loss: 4.494209861150011e-06, Validation Loss: 2.485483812694844e-06\n",
      "Epoch 390, Training Loss: 2.953533112304285e-06, Validation Loss: 2.1954457205810617e-06\n",
      "Epoch 391, Training Loss: 3.561507355698268e-06, Validation Loss: 2.8553473771498133e-06\n",
      "Epoch 392, Training Loss: 1.8138320001526154e-06, Validation Loss: 2.201759613106646e-06\n",
      "Epoch 393, Training Loss: 2.1891801225137897e-06, Validation Loss: 2.188384578591473e-06\n",
      "Epoch 394, Training Loss: 1.1427042409195565e-05, Validation Loss: 8.431729777949859e-06\n",
      "Epoch 395, Training Loss: 2.716907602007268e-06, Validation Loss: 2.9940384595815626e-06\n",
      "Epoch 396, Training Loss: 1.7649006167630432e-06, Validation Loss: 2.1927364969811767e-06\n",
      "Epoch 397, Training Loss: 1.7089480479626218e-06, Validation Loss: 2.325744636888717e-06\n",
      "Epoch 398, Training Loss: 2.5593685677449685e-06, Validation Loss: 3.002128691567667e-06\n",
      "Epoch 399, Training Loss: 1.9151918877469143e-06, Validation Loss: 2.1537432965221346e-06\n",
      "Epoch 400, Training Loss: 2.6387569960206747e-06, Validation Loss: 2.3875456563357702e-06\n",
      "Epoch 401, Training Loss: 3.289976575615583e-06, Validation Loss: 2.286469308783589e-06\n",
      "Epoch 402, Training Loss: 4.502192950894823e-06, Validation Loss: 4.540779567033514e-06\n",
      "Epoch 403, Training Loss: 2.5311844638054026e-06, Validation Loss: 2.29025502037776e-06\n",
      "Epoch 404, Training Loss: 2.757803258646163e-06, Validation Loss: 2.3454436382487716e-06\n",
      "Epoch 405, Training Loss: 3.169997398799751e-06, Validation Loss: 2.167513146235535e-06\n",
      "Epoch 406, Training Loss: 1.733232056722045e-05, Validation Loss: 1.1152810970042123e-05\n",
      "Epoch 407, Training Loss: 3.517598088365048e-06, Validation Loss: 2.1159926204253313e-06\n",
      "Epoch 408, Training Loss: 2.4997320906550158e-06, Validation Loss: 2.201278018283177e-06\n",
      "Epoch 409, Training Loss: 3.070084858336486e-06, Validation Loss: 2.10312099122759e-06\n",
      "Epoch 410, Training Loss: 2.2242679733608384e-06, Validation Loss: 2.245073469523258e-06\n",
      "Epoch 411, Training Loss: 1.9684480321302544e-06, Validation Loss: 2.3470752330033333e-06\n",
      "Epoch 412, Training Loss: 1.512072776677087e-05, Validation Loss: 1.3389081194508185e-05\n",
      "Epoch 413, Training Loss: 2.066147317236755e-06, Validation Loss: 2.1114679132488104e-06\n",
      "Epoch 414, Training Loss: 3.3703279314067913e-06, Validation Loss: 7.107318208671908e-06\n",
      "Epoch 415, Training Loss: 1.614512598280271e-06, Validation Loss: 2.244077708959374e-06\n",
      "Epoch 416, Training Loss: 1.6690559050402953e-06, Validation Loss: 2.1647080466151517e-06\n",
      "Epoch 417, Training Loss: 1.3254001487439382e-06, Validation Loss: 2.112160724458486e-06\n",
      "Epoch 418, Training Loss: 3.104610641457839e-06, Validation Loss: 2.16838211882086e-06\n",
      "Epoch 419, Training Loss: 1.3351100278669037e-06, Validation Loss: 2.0776837220420154e-06\n",
      "Epoch 420, Training Loss: 2.2369131329469383e-06, Validation Loss: 2.085900597906802e-06\n",
      "Epoch 421, Training Loss: 3.873579771607183e-06, Validation Loss: 3.252533310176219e-06\n",
      "Epoch 422, Training Loss: 1.7810618828661973e-06, Validation Loss: 2.075898683728109e-06\n",
      "Epoch 423, Training Loss: 7.67811343393987e-06, Validation Loss: 4.68477894787756e-06\n",
      "Epoch 424, Training Loss: 4.7144960262812674e-05, Validation Loss: 2.8594477391530384e-05\n",
      "Epoch 425, Training Loss: 3.670782461995259e-06, Validation Loss: 3.2785423126410555e-06\n",
      "Epoch 426, Training Loss: 3.1811259759706445e-06, Validation Loss: 2.052727044773729e-06\n",
      "Epoch 427, Training Loss: 1.9745264125958784e-06, Validation Loss: 2.0660218062437904e-06\n",
      "Epoch 428, Training Loss: 1.5585219443892129e-06, Validation Loss: 2.059215431388696e-06\n",
      "Epoch 429, Training Loss: 1.3053927432338241e-06, Validation Loss: 2.289431051125413e-06\n",
      "Epoch 430, Training Loss: 3.2342620670533506e-06, Validation Loss: 2.5900152712412216e-06\n",
      "Epoch 431, Training Loss: 2.0151962871750584e-06, Validation Loss: 2.0651527333678475e-06\n",
      "Epoch 432, Training Loss: 2.2351507595885778e-06, Validation Loss: 2.113093362759115e-06\n",
      "Epoch 433, Training Loss: 1.5613293271599105e-06, Validation Loss: 2.041676790954886e-06\n",
      "Epoch 434, Training Loss: 6.112785740697291e-06, Validation Loss: 6.176444556586152e-06\n",
      "Epoch 435, Training Loss: 1.646862983761821e-06, Validation Loss: 2.0779664202946497e-06\n",
      "Epoch 436, Training Loss: 2.5687327251944225e-06, Validation Loss: 2.1161071450698763e-06\n",
      "Epoch 437, Training Loss: 1.422486775481957e-06, Validation Loss: 2.055344590800791e-06\n",
      "Epoch 438, Training Loss: 2.0801990103791468e-06, Validation Loss: 2.4581477466697625e-06\n",
      "Epoch 439, Training Loss: 1.8524173128753318e-06, Validation Loss: 2.122258334351801e-06\n",
      "Epoch 440, Training Loss: 1.960723238880746e-06, Validation Loss: 2.14340615678912e-06\n",
      "Epoch 441, Training Loss: 2.078732450172538e-06, Validation Loss: 2.0147493664750352e-06\n",
      "Epoch 442, Training Loss: 2.256191692140419e-06, Validation Loss: 2.2344276439575095e-06\n",
      "Epoch 443, Training Loss: 7.808148438925855e-06, Validation Loss: 4.563272641114813e-06\n",
      "Epoch 444, Training Loss: 3.6323685890238266e-06, Validation Loss: 2.070885483399078e-06\n",
      "Epoch 445, Training Loss: 1.8448242826707428e-06, Validation Loss: 2.121195161113748e-06\n",
      "Epoch 446, Training Loss: 1.9907822661480168e-06, Validation Loss: 2.4192318963455104e-06\n",
      "Epoch 447, Training Loss: 1.5529265056102304e-06, Validation Loss: 2.0423849086128144e-06\n",
      "Epoch 448, Training Loss: 1.3010183465667069e-05, Validation Loss: 3.34778307550533e-06\n",
      "Epoch 449, Training Loss: 1.6142465710800025e-06, Validation Loss: 2.021118698712888e-06\n",
      "Epoch 450, Training Loss: 1.709831281004881e-06, Validation Loss: 2.1016399960511305e-06\n",
      "Epoch 451, Training Loss: 1.6463545762235299e-06, Validation Loss: 1.985715165947866e-06\n",
      "Epoch 452, Training Loss: 2.511211960154469e-06, Validation Loss: 2.334121013576527e-06\n",
      "Epoch 453, Training Loss: 1.8138720179194934e-06, Validation Loss: 2.6461165054952636e-06\n",
      "Epoch 454, Training Loss: 2.955219315481372e-06, Validation Loss: 2.104116311164463e-06\n",
      "Epoch 455, Training Loss: 1.5183650248218328e-06, Validation Loss: 2.0174307835054173e-06\n",
      "Epoch 456, Training Loss: 3.061481493205065e-06, Validation Loss: 2.0296589676493737e-06\n",
      "Epoch 457, Training Loss: 2.3704990326223196e-06, Validation Loss: 2.0788796637655875e-06\n",
      "Epoch 458, Training Loss: 5.206995410844684e-06, Validation Loss: 3.732130664595307e-06\n",
      "Epoch 459, Training Loss: 2.289575604663696e-06, Validation Loss: 2.0508145334637573e-06\n",
      "Epoch 460, Training Loss: 1.4959244936108007e-06, Validation Loss: 2.0003480251185107e-06\n",
      "Epoch 461, Training Loss: 5.463491106638685e-06, Validation Loss: 2.7937081159698383e-06\n",
      "Epoch 462, Training Loss: 2.1116306925250683e-06, Validation Loss: 2.2980304875438528e-06\n",
      "Epoch 463, Training Loss: 1.4382975450644153e-06, Validation Loss: 1.9440979986898506e-06\n",
      "Epoch 464, Training Loss: 2.411718469375046e-06, Validation Loss: 2.30039530157564e-06\n",
      "Epoch 465, Training Loss: 2.0760980987688527e-06, Validation Loss: 1.9312744506970856e-06\n",
      "Epoch 466, Training Loss: 2.5255549189751036e-06, Validation Loss: 1.9893597425742056e-06\n",
      "Epoch 467, Training Loss: 3.2968309824354947e-06, Validation Loss: 2.92916766669411e-06\n",
      "Epoch 468, Training Loss: 2.6036730105261086e-06, Validation Loss: 1.983917487757553e-06\n",
      "Epoch 469, Training Loss: 1.8171680267187185e-06, Validation Loss: 1.948324935418299e-06\n",
      "Epoch 470, Training Loss: 3.479897259239806e-06, Validation Loss: 3.044295020198817e-06\n",
      "Epoch 471, Training Loss: 2.134642727469327e-06, Validation Loss: 1.9561045575848683e-06\n",
      "Epoch 472, Training Loss: 2.3033203433442395e-06, Validation Loss: 2.0443532361774475e-06\n",
      "Epoch 473, Training Loss: 1.4842812561255414e-06, Validation Loss: 1.983441854251865e-06\n",
      "Epoch 474, Training Loss: 1.22937831292802e-06, Validation Loss: 1.933222948597976e-06\n",
      "Epoch 475, Training Loss: 1.8867112885345705e-06, Validation Loss: 2.0066311331311518e-06\n",
      "Epoch 476, Training Loss: 3.2400730560766533e-06, Validation Loss: 1.918323553894833e-06\n",
      "Epoch 477, Training Loss: 1.9769540813285857e-06, Validation Loss: 1.942793875974866e-06\n",
      "Epoch 478, Training Loss: 1.590976353327278e-06, Validation Loss: 1.9081476832858464e-06\n",
      "Epoch 479, Training Loss: 4.1427147152717225e-06, Validation Loss: 5.0924865694944214e-06\n",
      "Epoch 480, Training Loss: 2.1615246623696294e-06, Validation Loss: 1.9099536674944526e-06\n",
      "Epoch 481, Training Loss: 8.229502782342024e-06, Validation Loss: 1.9204714635779268e-06\n",
      "Epoch 482, Training Loss: 1.8048348238153267e-06, Validation Loss: 2.029455162631218e-06\n",
      "Epoch 483, Training Loss: 2.895316811191151e-06, Validation Loss: 1.969731939587604e-06\n",
      "Epoch 484, Training Loss: 3.0124235763651086e-06, Validation Loss: 2.8061001898280504e-06\n",
      "Epoch 485, Training Loss: 1.4279530660132878e-06, Validation Loss: 1.9485382169947044e-06\n",
      "Epoch 486, Training Loss: 1.9377364424144616e-06, Validation Loss: 2.8424588980420697e-06\n",
      "Epoch 487, Training Loss: 2.7394978587835794e-06, Validation Loss: 4.211259792894099e-06\n",
      "Epoch 488, Training Loss: 2.7225651137996465e-06, Validation Loss: 2.141032359324793e-06\n",
      "Epoch 489, Training Loss: 2.4754920104896883e-06, Validation Loss: 1.96614497141289e-06\n",
      "Epoch 490, Training Loss: 2.626987679832382e-06, Validation Loss: 2.103920172404594e-06\n",
      "Epoch 491, Training Loss: 2.4905755253712414e-06, Validation Loss: 1.8787332531472572e-06\n",
      "Epoch 492, Training Loss: 1.5398416053358233e-06, Validation Loss: 1.9002093365528777e-06\n",
      "Epoch 493, Training Loss: 2.32326556215412e-06, Validation Loss: 2.069086270436456e-06\n",
      "Epoch 494, Training Loss: 2.0911420506308787e-06, Validation Loss: 1.8677732771773048e-06\n",
      "Epoch 495, Training Loss: 1.4501840723823989e-06, Validation Loss: 2.0993795823803115e-06\n",
      "Epoch 496, Training Loss: 1.4351556956171407e-06, Validation Loss: 1.865774504712002e-06\n",
      "Epoch 497, Training Loss: 2.885128878915566e-06, Validation Loss: 2.2406931898881426e-06\n",
      "Epoch 498, Training Loss: 1.4012991869094549e-06, Validation Loss: 1.8866400488862782e-06\n",
      "Epoch 499, Training Loss: 2.2937042558623943e-06, Validation Loss: 2.2526231651293753e-06\n",
      "Epoch 500, Training Loss: 1.7508949667899287e-06, Validation Loss: 1.882852650092541e-06\n",
      "Epoch 501, Training Loss: 2.9409654871415114e-06, Validation Loss: 6.505744941197084e-06\n",
      "Epoch 502, Training Loss: 2.9261636882438324e-06, Validation Loss: 2.6501788454062907e-06\n",
      "Epoch 503, Training Loss: 2.4969044716272037e-06, Validation Loss: 2.219903062810355e-06\n",
      "Epoch 504, Training Loss: 2.265339389850851e-06, Validation Loss: 1.8802421635112174e-06\n",
      "Epoch 505, Training Loss: 1.6029307516873814e-06, Validation Loss: 2.24990199065325e-06\n",
      "Epoch 506, Training Loss: 1.603994746801618e-06, Validation Loss: 2.035416778749182e-06\n",
      "Epoch 507, Training Loss: 1.3797150586469797e-06, Validation Loss: 1.960214779929936e-06\n",
      "Epoch 508, Training Loss: 1.1362928944436135e-06, Validation Loss: 1.8989689461722363e-06\n",
      "Epoch 509, Training Loss: 3.0137885005387943e-06, Validation Loss: 3.7101017615623856e-06\n",
      "Epoch 510, Training Loss: 1.8613139900480746e-06, Validation Loss: 1.8475090570834376e-06\n",
      "Epoch 511, Training Loss: 2.8296462915022857e-06, Validation Loss: 1.9371525493503434e-06\n",
      "Epoch 512, Training Loss: 1.5292660009436077e-06, Validation Loss: 1.8297633194199577e-06\n",
      "Epoch 513, Training Loss: 2.6000152502092533e-06, Validation Loss: 1.8402360737920945e-06\n",
      "Epoch 514, Training Loss: 1.4843965345789911e-06, Validation Loss: 2.0196755659601695e-06\n",
      "Epoch 515, Training Loss: 1.3802653029415524e-06, Validation Loss: 2.0011867146439825e-06\n",
      "Epoch 516, Training Loss: 2.9211821583885467e-06, Validation Loss: 2.2936398924574844e-06\n",
      "Epoch 517, Training Loss: 2.111188905473682e-06, Validation Loss: 1.8963384532418938e-06\n",
      "Epoch 518, Training Loss: 2.411342393315863e-06, Validation Loss: 4.185240819297062e-06\n",
      "Epoch 519, Training Loss: 1.6653438024150091e-06, Validation Loss: 2.080688304046301e-06\n",
      "Epoch 520, Training Loss: 1.6114120171550894e-06, Validation Loss: 2.2098176094433563e-06\n",
      "Epoch 521, Training Loss: 1.9840988443320384e-06, Validation Loss: 1.8287604715317157e-06\n",
      "Epoch 522, Training Loss: 1.913446567414212e-06, Validation Loss: 2.0770618025409793e-06\n",
      "Epoch 523, Training Loss: 2.6448949483892648e-06, Validation Loss: 1.9921900495354036e-06\n",
      "Epoch 524, Training Loss: 3.2198126973526087e-06, Validation Loss: 6.695150677031307e-06\n",
      "Epoch 525, Training Loss: 1.3368041891226312e-06, Validation Loss: 1.8150627267899905e-06\n",
      "Epoch 526, Training Loss: 2.513717390684178e-06, Validation Loss: 2.483472324682596e-06\n",
      "Epoch 527, Training Loss: 1.3532288676287862e-06, Validation Loss: 1.8691343041372286e-06\n",
      "Epoch 528, Training Loss: 1.8910720882558962e-06, Validation Loss: 1.8391422242970362e-06\n",
      "Epoch 529, Training Loss: 1.9858362065861e-06, Validation Loss: 1.841684989729077e-06\n",
      "Epoch 530, Training Loss: 1.7185350316140102e-06, Validation Loss: 2.006446438363533e-06\n",
      "Epoch 531, Training Loss: 1.1472706091808504e-06, Validation Loss: 2.032540814574659e-06\n",
      "Epoch 532, Training Loss: 1.826701918616891e-06, Validation Loss: 2.2511113039764744e-06\n",
      "Epoch 533, Training Loss: 2.131368319169269e-06, Validation Loss: 2.255574849806661e-06\n",
      "Epoch 534, Training Loss: 1.4072383009988698e-06, Validation Loss: 2.162998317865916e-06\n",
      "Epoch 535, Training Loss: 1.6694934856786858e-06, Validation Loss: 1.8419994138506356e-06\n",
      "Epoch 536, Training Loss: 2.322556156286737e-06, Validation Loss: 3.7924863680678857e-06\n",
      "Epoch 537, Training Loss: 6.977461453061551e-06, Validation Loss: 5.872644496080892e-06\n",
      "Epoch 538, Training Loss: 5.447325747809373e-06, Validation Loss: 6.789701844707938e-06\n",
      "Epoch 539, Training Loss: 4.572591024043504e-06, Validation Loss: 2.446601040895649e-06\n",
      "Epoch 540, Training Loss: 1.3097508144710446e-06, Validation Loss: 1.7918901392384295e-06\n",
      "Epoch 541, Training Loss: 1.6917795164772542e-06, Validation Loss: 1.8153313930455463e-06\n",
      "Epoch 542, Training Loss: 2.052203626590199e-06, Validation Loss: 1.953811418035425e-06\n",
      "Epoch 543, Training Loss: 1.4822193179497845e-06, Validation Loss: 1.7951655030148084e-06\n",
      "Epoch 544, Training Loss: 2.631037659739377e-06, Validation Loss: 2.011198475768298e-06\n",
      "Epoch 545, Training Loss: 1.6129224604810588e-06, Validation Loss: 1.7951342713577743e-06\n",
      "Epoch 546, Training Loss: 2.773133928712923e-06, Validation Loss: 2.727457484933479e-06\n",
      "Epoch 547, Training Loss: 2.575888629507972e-06, Validation Loss: 2.070692425045627e-06\n",
      "Epoch 548, Training Loss: 1.7183878071591607e-06, Validation Loss: 2.300400582220634e-06\n",
      "Epoch 549, Training Loss: 2.1999560431140708e-06, Validation Loss: 1.798049982478301e-06\n",
      "Epoch 550, Training Loss: 1.347614102087391e-06, Validation Loss: 1.92878382878314e-06\n",
      "Epoch 551, Training Loss: 2.9365323825913947e-06, Validation Loss: 1.8718072037062815e-06\n",
      "Epoch 552, Training Loss: 1.119428134188638e-06, Validation Loss: 1.8179702401481614e-06\n",
      "Epoch 553, Training Loss: 1.3384591284193448e-06, Validation Loss: 1.8057053076390614e-06\n",
      "Epoch 554, Training Loss: 1.8665919014893007e-06, Validation Loss: 1.766770606299227e-06\n",
      "Epoch 555, Training Loss: 1.5255534435709706e-06, Validation Loss: 1.8979215848181864e-06\n",
      "Epoch 556, Training Loss: 1.4242050383472815e-06, Validation Loss: 1.7620793540604639e-06\n",
      "Epoch 557, Training Loss: 2.2700130557495868e-06, Validation Loss: 3.0328725306911335e-06\n",
      "Epoch 558, Training Loss: 2.2705244191456586e-06, Validation Loss: 2.915093873409761e-06\n",
      "Epoch 559, Training Loss: 1.876032683867379e-06, Validation Loss: 1.81530026421355e-06\n",
      "Epoch 560, Training Loss: 1.737899310683133e-06, Validation Loss: 2.3145227626506532e-06\n",
      "Epoch 561, Training Loss: 1.4310386177385226e-05, Validation Loss: 5.913923467082345e-06\n",
      "Epoch 562, Training Loss: 1.6164024145837175e-06, Validation Loss: 1.924153468518076e-06\n",
      "Epoch 563, Training Loss: 1.2432376479409868e-06, Validation Loss: 1.8256950417250497e-06\n",
      "Epoch 564, Training Loss: 1.9109363620373188e-06, Validation Loss: 1.745287215074713e-06\n",
      "Epoch 565, Training Loss: 4.7943321987986565e-06, Validation Loss: 4.470008162240395e-06\n",
      "Epoch 566, Training Loss: 1.4331338888950995e-06, Validation Loss: 1.943649483477619e-06\n",
      "Epoch 567, Training Loss: 1.9134893136651954e-06, Validation Loss: 1.791286294134291e-06\n",
      "Epoch 568, Training Loss: 2.6204759251413634e-06, Validation Loss: 2.3325180834736513e-06\n",
      "Epoch 569, Training Loss: 2.579853571660351e-06, Validation Loss: 2.2656874989479547e-06\n",
      "Epoch 570, Training Loss: 1.8619775801198557e-06, Validation Loss: 1.7428248403572812e-06\n",
      "Epoch 571, Training Loss: 2.3740929009363754e-06, Validation Loss: 1.7330749517948174e-06\n",
      "Epoch 572, Training Loss: 1.663606212787272e-06, Validation Loss: 1.9678790881611766e-06\n",
      "Epoch 573, Training Loss: 1.4817085229879012e-06, Validation Loss: 1.7600243120413885e-06\n",
      "Epoch 574, Training Loss: 8.425532541878056e-06, Validation Loss: 1.7999056022154326e-06\n",
      "Epoch 575, Training Loss: 4.767725840792991e-06, Validation Loss: 3.5505014285222765e-06\n",
      "Epoch 576, Training Loss: 1.4987020904300152e-06, Validation Loss: 1.7361075652078613e-06\n",
      "Epoch 577, Training Loss: 4.554804945655633e-06, Validation Loss: 4.375005859123364e-06\n",
      "Epoch 578, Training Loss: 1.6037068917285069e-06, Validation Loss: 1.8324429120300365e-06\n",
      "Epoch 579, Training Loss: 1.170034011011012e-06, Validation Loss: 1.7360336155405233e-06\n",
      "Epoch 580, Training Loss: 2.108275111822877e-06, Validation Loss: 2.144225650979957e-06\n",
      "Epoch 581, Training Loss: 3.237004193579196e-06, Validation Loss: 2.5835904095002216e-06\n",
      "Epoch 582, Training Loss: 1.3039300483796978e-06, Validation Loss: 1.786719773011268e-06\n",
      "Epoch 583, Training Loss: 1.2301005654080654e-06, Validation Loss: 1.8677537382477364e-06\n",
      "Epoch 584, Training Loss: 1.3235423921287293e-06, Validation Loss: 1.8257659146065692e-06\n",
      "Epoch 585, Training Loss: 1.3055209819867741e-06, Validation Loss: 2.1440444906419874e-06\n",
      "Epoch 586, Training Loss: 1.223178287546034e-06, Validation Loss: 1.7402560366567422e-06\n",
      "Epoch 587, Training Loss: 4.8111623982549645e-06, Validation Loss: 5.614340627787688e-06\n",
      "Epoch 588, Training Loss: 3.0637954751000507e-06, Validation Loss: 3.3183670711298924e-06\n",
      "Epoch 589, Training Loss: 2.4683345145604108e-06, Validation Loss: 2.6119572987030044e-06\n",
      "Epoch 590, Training Loss: 1.267440666197217e-06, Validation Loss: 1.9677602187967187e-06\n",
      "Epoch 591, Training Loss: 2.0052120817126706e-06, Validation Loss: 1.7757710527623701e-06\n",
      "Epoch 592, Training Loss: 2.3690306534263073e-06, Validation Loss: 2.0370147786811366e-06\n",
      "Epoch 593, Training Loss: 1.2640030035981908e-06, Validation Loss: 1.7079319244206257e-06\n",
      "Epoch 594, Training Loss: 2.720196334848879e-06, Validation Loss: 3.006083703138565e-06\n",
      "Epoch 595, Training Loss: 5.481080279423622e-06, Validation Loss: 2.155314123127514e-06\n",
      "Epoch 596, Training Loss: 2.2795693439547904e-06, Validation Loss: 1.706723279822208e-06\n",
      "Epoch 597, Training Loss: 1.4173760973790195e-06, Validation Loss: 1.7031494207536513e-06\n",
      "Epoch 598, Training Loss: 2.906951976910932e-06, Validation Loss: 1.7501117245799856e-06\n",
      "Epoch 599, Training Loss: 2.2356671252055094e-06, Validation Loss: 2.7652209808505055e-06\n",
      "Epoch 600, Training Loss: 1.5399130006699124e-06, Validation Loss: 1.8488850059839024e-06\n",
      "Epoch 601, Training Loss: 1.8670134522835724e-06, Validation Loss: 2.1874073558612776e-06\n",
      "Epoch 602, Training Loss: 4.543540853774175e-06, Validation Loss: 2.6420036784906085e-06\n",
      "Epoch 603, Training Loss: 1.2517493814812042e-06, Validation Loss: 1.806765761444509e-06\n",
      "Epoch 604, Training Loss: 2.6249322218063753e-06, Validation Loss: 1.968048120937629e-06\n",
      "Epoch 605, Training Loss: 1.7585610976311727e-06, Validation Loss: 2.453223147127736e-06\n",
      "Epoch 606, Training Loss: 1.1243675999139668e-06, Validation Loss: 1.952769054919575e-06\n",
      "Epoch 607, Training Loss: 1.6751645262047532e-06, Validation Loss: 2.251248187273811e-06\n",
      "Epoch 608, Training Loss: 1.6177358475033543e-06, Validation Loss: 1.722998624511996e-06\n",
      "Epoch 609, Training Loss: 1.8844215219360194e-06, Validation Loss: 2.074751678757276e-06\n",
      "Epoch 610, Training Loss: 2.4608493731648196e-06, Validation Loss: 3.3193972715644063e-06\n",
      "Epoch 611, Training Loss: 1.1981368288616068e-06, Validation Loss: 1.7996451282913326e-06\n",
      "Epoch 612, Training Loss: 2.4823473268043017e-06, Validation Loss: 6.087502552658708e-06\n",
      "Epoch 613, Training Loss: 7.139365152397659e-06, Validation Loss: 4.084581346172286e-06\n",
      "Epoch 614, Training Loss: 2.4694177227502223e-06, Validation Loss: 1.7874872992831064e-06\n",
      "Epoch 615, Training Loss: 2.152127990484587e-06, Validation Loss: 1.8901221688318135e-06\n",
      "Epoch 616, Training Loss: 2.7838427740789484e-06, Validation Loss: 2.2987142660808294e-06\n",
      "Epoch 617, Training Loss: 1.7255491684409208e-06, Validation Loss: 1.7348663511407363e-06\n",
      "Epoch 618, Training Loss: 1.3575788671005284e-06, Validation Loss: 1.8026226209669612e-06\n",
      "Epoch 619, Training Loss: 1.9942340259149205e-06, Validation Loss: 2.143045617810406e-06\n",
      "Epoch 620, Training Loss: 4.067166173626902e-06, Validation Loss: 2.2061506397685256e-06\n",
      "Epoch 621, Training Loss: 1.4219189097275375e-06, Validation Loss: 1.6894664248877905e-06\n",
      "Epoch 622, Training Loss: 3.6296357848186744e-06, Validation Loss: 2.518345290751591e-06\n",
      "Epoch 623, Training Loss: 2.2241401893552393e-06, Validation Loss: 2.161229475943725e-06\n",
      "Epoch 624, Training Loss: 1.9416270333749708e-06, Validation Loss: 2.3982956338893314e-06\n",
      "Epoch 625, Training Loss: 1.8157297745347023e-06, Validation Loss: 1.9371545272840836e-06\n",
      "Epoch 626, Training Loss: 2.4017667783482466e-06, Validation Loss: 2.2252769078396183e-06\n",
      "Epoch 627, Training Loss: 2.3731001874693902e-06, Validation Loss: 1.7826292666431055e-06\n",
      "Epoch 628, Training Loss: 1.5016946690593613e-06, Validation Loss: 1.7040398392385265e-06\n",
      "Epoch 629, Training Loss: 7.688794539717492e-06, Validation Loss: 1.2107067154510952e-05\n",
      "Epoch 630, Training Loss: 1.221006527885038e-06, Validation Loss: 1.919457676771124e-06\n",
      "Epoch 631, Training Loss: 1.438762410543859e-06, Validation Loss: 1.6921123173158046e-06\n",
      "Epoch 632, Training Loss: 2.7860569389304146e-06, Validation Loss: 2.3487172507409655e-06\n",
      "Epoch 633, Training Loss: 1.8726861981122056e-06, Validation Loss: 1.788030706436776e-06\n",
      "Epoch 634, Training Loss: 3.4601930565258954e-06, Validation Loss: 4.485638015513319e-06\n",
      "Epoch 635, Training Loss: 1.4717843441758305e-06, Validation Loss: 1.6906711306355473e-06\n",
      "Epoch 636, Training Loss: 1.7763618416211102e-06, Validation Loss: 1.776278074697069e-06\n",
      "Epoch 637, Training Loss: 6.075170858821366e-06, Validation Loss: 5.189172909221542e-06\n",
      "Epoch 638, Training Loss: 5.205934940022416e-06, Validation Loss: 4.93516664814677e-06\n",
      "Epoch 639, Training Loss: 1.341078814220964e-06, Validation Loss: 1.8183416166686387e-06\n",
      "Epoch 640, Training Loss: 3.679155042846105e-06, Validation Loss: 4.066492339429686e-06\n",
      "Epoch 641, Training Loss: 9.817209729590104e-07, Validation Loss: 1.696359266910352e-06\n",
      "Epoch 642, Training Loss: 1.6711255739210173e-06, Validation Loss: 1.744552167756584e-06\n",
      "Epoch 643, Training Loss: 2.4114833649946377e-06, Validation Loss: 2.691037017974137e-06\n",
      "Epoch 644, Training Loss: 1.7865104382508434e-06, Validation Loss: 1.768422274021846e-06\n",
      "Epoch 645, Training Loss: 2.2810729660704965e-06, Validation Loss: 2.9154165101381354e-06\n",
      "Epoch 646, Training Loss: 1.8401701709080953e-06, Validation Loss: 1.8531732000045832e-06\n",
      "Epoch 647, Training Loss: 1.5638197510270402e-06, Validation Loss: 1.8684096330964318e-06\n",
      "Epoch 648, Training Loss: 2.576789938757429e-06, Validation Loss: 2.131139336861278e-06\n",
      "Epoch 649, Training Loss: 3.4490449252189137e-06, Validation Loss: 2.0712519083870945e-06\n",
      "Epoch 650, Training Loss: 2.1657306206179783e-06, Validation Loss: 3.2522047617322835e-06\n",
      "Epoch 651, Training Loss: 2.511029379093088e-06, Validation Loss: 4.279843188049627e-06\n",
      "Epoch 652, Training Loss: 1.3931734201833024e-06, Validation Loss: 1.9601938083289765e-06\n",
      "Epoch 653, Training Loss: 9.104165656026453e-06, Validation Loss: 3.6603260549938544e-06\n",
      "Epoch 654, Training Loss: 1.4207819276634837e-06, Validation Loss: 1.707340878137231e-06\n",
      "Epoch 655, Training Loss: 3.0381713713723e-06, Validation Loss: 3.1986589085286692e-06\n",
      "Epoch 656, Training Loss: 3.242104412493063e-06, Validation Loss: 2.849681971725473e-06\n",
      "Epoch 657, Training Loss: 1.2018559800708317e-06, Validation Loss: 1.8279029863858404e-06\n",
      "Epoch 658, Training Loss: 5.072914973425213e-06, Validation Loss: 2.682940737413794e-06\n",
      "Epoch 659, Training Loss: 3.1040963222039863e-06, Validation Loss: 2.9281724144624557e-06\n",
      "Epoch 660, Training Loss: 2.064604814222548e-06, Validation Loss: 1.7470911174383612e-06\n",
      "Epoch 661, Training Loss: 2.626243713166332e-06, Validation Loss: 2.9672003672499013e-06\n",
      "Epoch 662, Training Loss: 1.2267640840946115e-06, Validation Loss: 1.6405640783475765e-06\n",
      "Epoch 663, Training Loss: 1.490303816353844e-06, Validation Loss: 1.6615796207183067e-06\n",
      "Epoch 664, Training Loss: 3.7294262256182265e-06, Validation Loss: 2.1739459189873765e-06\n",
      "Epoch 665, Training Loss: 4.610635642166017e-06, Validation Loss: 2.602732500317117e-06\n",
      "Epoch 666, Training Loss: 3.4559895993879763e-06, Validation Loss: 3.886492227051766e-06\n",
      "Epoch 667, Training Loss: 1.2368834632070502e-06, Validation Loss: 1.881422722029818e-06\n",
      "Epoch 668, Training Loss: 2.2654826352663804e-06, Validation Loss: 1.99716705726138e-06\n",
      "Epoch 669, Training Loss: 1.5726839137641946e-06, Validation Loss: 2.9465129457302554e-06\n",
      "Epoch 670, Training Loss: 1.6341489299520617e-06, Validation Loss: 1.6381117975006824e-06\n",
      "Epoch 671, Training Loss: 2.602158929221332e-06, Validation Loss: 2.030835858138188e-06\n",
      "Epoch 672, Training Loss: 2.339022557862336e-06, Validation Loss: 1.897718557504895e-06\n",
      "Epoch 673, Training Loss: 1.140020003731479e-06, Validation Loss: 1.711347035026824e-06\n",
      "Epoch 674, Training Loss: 1.9090715795755386e-05, Validation Loss: 1.7710528951214262e-05\n",
      "Epoch 675, Training Loss: 8.908430118026445e-07, Validation Loss: 1.6699430476063562e-06\n",
      "Epoch 676, Training Loss: 2.454302830301458e-06, Validation Loss: 1.7797211696442263e-06\n",
      "Epoch 677, Training Loss: 1.675498651820817e-06, Validation Loss: 2.1438043138010806e-06\n",
      "Epoch 678, Training Loss: 4.988183263776591e-06, Validation Loss: 3.182807204443431e-06\n",
      "Epoch 679, Training Loss: 7.988703032424382e-07, Validation Loss: 1.6611892896330892e-06\n",
      "Epoch 680, Training Loss: 2.869821173590026e-06, Validation Loss: 2.8332402611931393e-06\n",
      "Epoch 681, Training Loss: 3.972089416492963e-06, Validation Loss: 3.974498934434114e-06\n",
      "Epoch 682, Training Loss: 1.7935675487024128e-06, Validation Loss: 2.1047883060969157e-06\n",
      "Epoch 683, Training Loss: 2.737758677540114e-06, Validation Loss: 1.7467074837388896e-06\n",
      "Epoch 684, Training Loss: 2.035505303865648e-06, Validation Loss: 1.75085299136467e-06\n",
      "Epoch 685, Training Loss: 1.2091149983461946e-06, Validation Loss: 1.651722547553531e-06\n",
      "Epoch 686, Training Loss: 1.2327636795816943e-06, Validation Loss: 1.7087612365922857e-06\n",
      "Epoch 687, Training Loss: 1.382152163387218e-06, Validation Loss: 1.7556766952864587e-06\n",
      "Epoch 688, Training Loss: 1.3352054111237521e-06, Validation Loss: 1.7617367866536092e-06\n",
      "Epoch 689, Training Loss: 2.098288632623735e-06, Validation Loss: 1.7406943540872179e-06\n",
      "Epoch 690, Training Loss: 1.8487141915102256e-06, Validation Loss: 1.6280762483107236e-06\n",
      "Epoch 691, Training Loss: 1.4212072301234002e-06, Validation Loss: 1.7518900825249657e-06\n",
      "Epoch 692, Training Loss: 1.3353950407690718e-06, Validation Loss: 1.6504864736521231e-06\n",
      "Epoch 693, Training Loss: 1.3684883697351324e-06, Validation Loss: 1.9350566666020266e-06\n",
      "Epoch 694, Training Loss: 1.1934057511098217e-06, Validation Loss: 1.674172073783023e-06\n",
      "Epoch 695, Training Loss: 1.5088107829797082e-06, Validation Loss: 1.689903907407923e-06\n",
      "Epoch 696, Training Loss: 2.4005883005884243e-06, Validation Loss: 1.7489485156141773e-06\n",
      "Epoch 697, Training Loss: 1.2682651231443742e-06, Validation Loss: 1.921468578608244e-06\n",
      "Epoch 698, Training Loss: 3.3881460694829e-06, Validation Loss: 3.0485956607558683e-06\n",
      "Epoch 699, Training Loss: 2.2597439510718687e-06, Validation Loss: 1.664167361604706e-06\n",
      "Epoch 700, Training Loss: 2.3736979528621305e-06, Validation Loss: 1.677090153862082e-06\n",
      "Epoch 701, Training Loss: 1.857101779023651e-06, Validation Loss: 2.1068781084097144e-06\n",
      "Epoch 702, Training Loss: 1.4372117220773362e-06, Validation Loss: 1.6362147667898206e-06\n",
      "Epoch 703, Training Loss: 2.11391943594208e-06, Validation Loss: 3.6857817298612204e-06\n",
      "Epoch 704, Training Loss: 1.6409587715315865e-06, Validation Loss: 1.631701061530299e-06\n",
      "Epoch 705, Training Loss: 1.5455660786756198e-06, Validation Loss: 1.7696668972458628e-06\n",
      "Epoch 706, Training Loss: 2.749808800217579e-06, Validation Loss: 1.6228444021244343e-06\n",
      "Epoch 707, Training Loss: 2.67424070443667e-06, Validation Loss: 2.855653620525836e-06\n",
      "Epoch 708, Training Loss: 1.326973688264843e-06, Validation Loss: 1.7562766902446343e-06\n",
      "Epoch 709, Training Loss: 2.9852935767848976e-06, Validation Loss: 5.2940995490088005e-06\n",
      "Epoch 710, Training Loss: 1.4232991816243157e-06, Validation Loss: 1.7065626801471066e-06\n",
      "Epoch 711, Training Loss: 3.1570368719258113e-06, Validation Loss: 1.7313003114818428e-06\n",
      "Epoch 712, Training Loss: 1.5342096730819321e-06, Validation Loss: 2.1815687423846486e-06\n",
      "Epoch 713, Training Loss: 3.074253527302062e-06, Validation Loss: 1.7424934052090233e-06\n",
      "Epoch 714, Training Loss: 1.4070542420085985e-06, Validation Loss: 1.959033654787447e-06\n",
      "Epoch 715, Training Loss: 1.7401869172317674e-06, Validation Loss: 1.6473175866507703e-06\n",
      "Epoch 716, Training Loss: 4.339846782386303e-06, Validation Loss: 1.7633211890604763e-06\n",
      "Epoch 717, Training Loss: 1.1710469607351115e-06, Validation Loss: 1.6351436423522668e-06\n",
      "Epoch 718, Training Loss: 2.05755213755765e-06, Validation Loss: 1.632262726283223e-06\n",
      "Epoch 719, Training Loss: 1.976439762074733e-06, Validation Loss: 1.810240766549127e-06\n",
      "Epoch 720, Training Loss: 8.846086529956665e-07, Validation Loss: 1.6259835705174627e-06\n",
      "Epoch 721, Training Loss: 1.2633001915673958e-06, Validation Loss: 1.627345021811698e-06\n",
      "Epoch 722, Training Loss: 9.915399914461887e-07, Validation Loss: 1.684126421606049e-06\n",
      "Epoch 723, Training Loss: 1.2497609986894531e-06, Validation Loss: 1.7069935688306512e-06\n",
      "Epoch 724, Training Loss: 1.6842191143950913e-06, Validation Loss: 1.941205295395683e-06\n",
      "Epoch 725, Training Loss: 2.6129273464903235e-06, Validation Loss: 2.5718424772992202e-06\n",
      "Epoch 726, Training Loss: 2.4753317120485008e-06, Validation Loss: 1.8672549090065534e-06\n",
      "Epoch 727, Training Loss: 1.991099907172611e-06, Validation Loss: 1.7343405701536933e-06\n",
      "Epoch 728, Training Loss: 1.7397209148839465e-06, Validation Loss: 2.1474481818760143e-06\n",
      "Epoch 729, Training Loss: 1.9415206224948633e-06, Validation Loss: 2.3260544443833082e-06\n",
      "Epoch 730, Training Loss: 1.5059544011819526e-06, Validation Loss: 1.6260676622093175e-06\n",
      "Epoch 731, Training Loss: 2.21435266212211e-06, Validation Loss: 1.8991741824135432e-06\n",
      "Epoch 732, Training Loss: 1.1861733355544857e-06, Validation Loss: 1.6199804478419305e-06\n",
      "Epoch 733, Training Loss: 2.4133300939865876e-06, Validation Loss: 2.5720304943293323e-06\n",
      "Epoch 734, Training Loss: 1.4557344911736436e-06, Validation Loss: 1.6175667857621025e-06\n",
      "Epoch 735, Training Loss: 2.6548682399152312e-06, Validation Loss: 1.6105502593392398e-06\n",
      "Epoch 736, Training Loss: 1.6371629953937372e-06, Validation Loss: 1.7359587719470631e-06\n",
      "Epoch 737, Training Loss: 1.1087705615864252e-06, Validation Loss: 1.6036181931853046e-06\n",
      "Epoch 738, Training Loss: 1.7371921785525046e-06, Validation Loss: 1.6216098206343164e-06\n",
      "Epoch 739, Training Loss: 1.0773509302453022e-06, Validation Loss: 1.6216878152358934e-06\n",
      "Epoch 740, Training Loss: 4.1995426727226e-06, Validation Loss: 3.1039343372873525e-06\n",
      "Epoch 741, Training Loss: 1.924591742863413e-06, Validation Loss: 1.6527624135416113e-06\n",
      "Epoch 742, Training Loss: 1.909054844873026e-06, Validation Loss: 1.7060356786558283e-06\n",
      "Epoch 743, Training Loss: 1.430944735147932e-06, Validation Loss: 1.9845532966040306e-06\n",
      "Epoch 744, Training Loss: 3.279878455941798e-06, Validation Loss: 2.1655170399800187e-06\n",
      "Epoch 745, Training Loss: 2.0100299025216373e-06, Validation Loss: 1.6147710041176923e-06\n",
      "Epoch 746, Training Loss: 3.92902757084812e-06, Validation Loss: 4.2953885336237654e-06\n",
      "Epoch 747, Training Loss: 1.8803882539941696e-06, Validation Loss: 1.6423850127374034e-06\n",
      "Epoch 748, Training Loss: 1.2842486967201694e-06, Validation Loss: 1.6492287735452884e-06\n",
      "Epoch 749, Training Loss: 2.953326202259632e-06, Validation Loss: 6.4737585223162e-06\n",
      "Epoch 750, Training Loss: 6.129242592578521e-06, Validation Loss: 6.557818425303565e-06\n",
      "Epoch 751, Training Loss: 2.5162487418128876e-06, Validation Loss: 2.677372105556674e-06\n",
      "Epoch 752, Training Loss: 2.0981001398467924e-06, Validation Loss: 1.794002746986051e-06\n",
      "Epoch 753, Training Loss: 8.924982466851361e-06, Validation Loss: 5.632703375056642e-06\n",
      "Epoch 754, Training Loss: 1.302659939028672e-06, Validation Loss: 1.7507516026187218e-06\n",
      "Epoch 755, Training Loss: 2.4276132535305806e-06, Validation Loss: 1.6731968260900877e-06\n",
      "Epoch 756, Training Loss: 1.7819431832322152e-06, Validation Loss: 1.8064234043945102e-06\n",
      "Epoch 757, Training Loss: 1.3327038459465257e-06, Validation Loss: 1.7465310099054662e-06\n",
      "Epoch 758, Training Loss: 3.5989323805551976e-06, Validation Loss: 4.37231607778303e-06\n",
      "Epoch 759, Training Loss: 1.2475213679863373e-06, Validation Loss: 1.6972443978710932e-06\n",
      "Epoch 760, Training Loss: 1.477582713960146e-06, Validation Loss: 1.8512982039028197e-06\n",
      "Epoch 761, Training Loss: 2.8543922780954745e-06, Validation Loss: 1.7327694166081406e-06\n",
      "Epoch 762, Training Loss: 2.905323299273732e-06, Validation Loss: 2.8884839042177286e-06\n",
      "Epoch 763, Training Loss: 1.5124081755857333e-06, Validation Loss: 1.6026713277372604e-06\n",
      "Epoch 764, Training Loss: 1.2362468169158092e-06, Validation Loss: 1.595080719351963e-06\n",
      "Epoch 765, Training Loss: 2.7593475806497736e-06, Validation Loss: 1.679618482513972e-06\n",
      "Epoch 766, Training Loss: 1.161771592705918e-06, Validation Loss: 1.8125168172289728e-06\n",
      "Epoch 767, Training Loss: 1.7157426555058919e-06, Validation Loss: 1.5888543732946844e-06\n",
      "Epoch 768, Training Loss: 3.909580300387461e-06, Validation Loss: 3.3916099423289212e-06\n",
      "Epoch 769, Training Loss: 1.1303936844342388e-06, Validation Loss: 1.6312981796354333e-06\n",
      "Epoch 770, Training Loss: 1.393376578562311e-06, Validation Loss: 1.5924245248689154e-06\n",
      "Epoch 771, Training Loss: 1.567141566738428e-06, Validation Loss: 1.660860317578962e-06\n",
      "Epoch 772, Training Loss: 1.4510831078951014e-06, Validation Loss: 1.6254679453132163e-06\n",
      "Epoch 773, Training Loss: 1.99241526388505e-06, Validation Loss: 2.64034049726022e-06\n",
      "Epoch 774, Training Loss: 1.5210110859698034e-06, Validation Loss: 1.7325797272229221e-06\n",
      "Epoch 775, Training Loss: 4.617594186129281e-06, Validation Loss: 4.785914761611189e-06\n",
      "Epoch 776, Training Loss: 1.5370274013548624e-06, Validation Loss: 1.7236345057704722e-06\n",
      "Epoch 777, Training Loss: 1.829641860240372e-06, Validation Loss: 2.0566363832006624e-06\n",
      "Epoch 778, Training Loss: 6.9919019551889505e-06, Validation Loss: 3.159292084966895e-06\n",
      "Epoch 779, Training Loss: 8.602866614637605e-07, Validation Loss: 1.5915232474807379e-06\n",
      "Epoch 780, Training Loss: 7.70289079810027e-06, Validation Loss: 4.034455888202524e-06\n",
      "Epoch 781, Training Loss: 4.982491645932896e-06, Validation Loss: 4.914594308119939e-06\n",
      "Epoch 782, Training Loss: 1.2808117162421695e-06, Validation Loss: 1.746642794842046e-06\n",
      "Epoch 783, Training Loss: 3.0004825930518564e-06, Validation Loss: 1.7917790668359154e-06\n",
      "Epoch 784, Training Loss: 4.079180143889971e-06, Validation Loss: 2.5482939107205544e-06\n",
      "Epoch 785, Training Loss: 1.8319168475500192e-06, Validation Loss: 1.7403838977811198e-06\n",
      "Epoch 786, Training Loss: 1.3279563972901087e-06, Validation Loss: 1.7247803662696683e-06\n",
      "Epoch 787, Training Loss: 4.450989308679709e-06, Validation Loss: 4.002038617930275e-06\n",
      "Epoch 788, Training Loss: 1.2749923143928754e-06, Validation Loss: 1.597346088537903e-06\n",
      "Epoch 789, Training Loss: 5.766799858974991e-06, Validation Loss: 3.0463924764598622e-06\n",
      "Epoch 790, Training Loss: 1.4718369811816956e-06, Validation Loss: 1.5819102498188706e-06\n",
      "Epoch 791, Training Loss: 3.2387624742113985e-06, Validation Loss: 2.0251482982628915e-06\n",
      "Epoch 792, Training Loss: 3.0043581773497863e-06, Validation Loss: 2.2556462820708696e-06\n",
      "Epoch 793, Training Loss: 2.209817466791719e-06, Validation Loss: 1.594088995394938e-06\n",
      "Epoch 794, Training Loss: 3.975686922785826e-06, Validation Loss: 2.6092327603274864e-06\n",
      "Epoch 795, Training Loss: 1.734472220960015e-06, Validation Loss: 1.5944346109848716e-06\n",
      "Epoch 796, Training Loss: 1.61433251832932e-06, Validation Loss: 1.795899468497679e-06\n",
      "Epoch 797, Training Loss: 1.9467613583401544e-06, Validation Loss: 1.6441406843652983e-06\n",
      "Epoch 798, Training Loss: 2.9866102977393894e-06, Validation Loss: 2.064122405488216e-06\n",
      "Epoch 799, Training Loss: 1.9837002582789864e-06, Validation Loss: 3.24025933449468e-06\n",
      "Epoch 800, Training Loss: 2.092970589728793e-06, Validation Loss: 1.581140404552779e-06\n",
      "Epoch 801, Training Loss: 1.2198412377983914e-06, Validation Loss: 1.7856910045240257e-06\n",
      "Epoch 802, Training Loss: 1.8480117205399438e-06, Validation Loss: 1.6089560436583293e-06\n",
      "Epoch 803, Training Loss: 2.3669376787438523e-06, Validation Loss: 2.8510868999077573e-06\n",
      "Epoch 804, Training Loss: 1.4786713791181683e-06, Validation Loss: 1.7883332122674935e-06\n",
      "Epoch 805, Training Loss: 6.522297098854324e-06, Validation Loss: 3.3341757617638724e-06\n",
      "Epoch 806, Training Loss: 1.768637389432115e-06, Validation Loss: 1.639448385049036e-06\n",
      "Epoch 807, Training Loss: 1.6337615988959442e-06, Validation Loss: 2.543196982932021e-06\n",
      "Epoch 808, Training Loss: 2.1472046682902146e-06, Validation Loss: 1.7891028518835092e-06\n",
      "Epoch 809, Training Loss: 1.8626305973157287e-06, Validation Loss: 1.5965132053685347e-06\n",
      "Epoch 810, Training Loss: 1.3731266790273366e-06, Validation Loss: 2.0888262359882094e-06\n",
      "Epoch 811, Training Loss: 4.616647402144736e-06, Validation Loss: 2.846710939286295e-06\n",
      "Epoch 812, Training Loss: 1.524786739537376e-06, Validation Loss: 1.5950870474365162e-06\n",
      "Epoch 813, Training Loss: 2.747196049313061e-06, Validation Loss: 2.537069310170745e-06\n",
      "Epoch 814, Training Loss: 2.970594096041168e-06, Validation Loss: 4.235042559427311e-06\n",
      "Epoch 815, Training Loss: 1.6168460206245072e-06, Validation Loss: 1.6390879923425585e-06\n",
      "Epoch 816, Training Loss: 1.716740257506899e-06, Validation Loss: 1.7773417403367122e-06\n",
      "Epoch 817, Training Loss: 2.1283492515067337e-06, Validation Loss: 2.159333557119101e-06\n",
      "Epoch 818, Training Loss: 2.191429302911274e-06, Validation Loss: 3.115583024499623e-06\n",
      "Epoch 819, Training Loss: 2.0038783077325206e-06, Validation Loss: 1.5963846642954908e-06\n",
      "Epoch 820, Training Loss: 2.693205487958039e-06, Validation Loss: 2.467396433773971e-06\n",
      "Epoch 821, Training Loss: 1.2036283578709117e-06, Validation Loss: 1.646557011930793e-06\n",
      "Epoch 822, Training Loss: 2.95474887934688e-06, Validation Loss: 2.705182414415011e-06\n",
      "Epoch 823, Training Loss: 2.0595680325641297e-06, Validation Loss: 1.8512293505919396e-06\n",
      "Epoch 824, Training Loss: 1.2476878055167617e-06, Validation Loss: 1.8756561498023051e-06\n",
      "Epoch 825, Training Loss: 1.4546176316798665e-06, Validation Loss: 1.5717577476479957e-06\n",
      "Epoch 826, Training Loss: 1.5174404097706429e-06, Validation Loss: 1.6301041231582432e-06\n",
      "Epoch 827, Training Loss: 1.5370258097391343e-06, Validation Loss: 2.229176725536983e-06\n",
      "Epoch 828, Training Loss: 7.255861419253051e-06, Validation Loss: 7.119117807557993e-06\n",
      "Epoch 829, Training Loss: 2.365600721532246e-06, Validation Loss: 2.6956533089499427e-06\n",
      "Epoch 830, Training Loss: 3.0150777092785574e-06, Validation Loss: 3.51447886477084e-06\n",
      "Epoch 831, Training Loss: 1.3102542197884759e-06, Validation Loss: 1.7424013764380077e-06\n",
      "Epoch 832, Training Loss: 2.323164608242223e-06, Validation Loss: 1.7849976788271005e-06\n",
      "Epoch 833, Training Loss: 1.233213424711721e-06, Validation Loss: 1.6126574789100492e-06\n",
      "Epoch 834, Training Loss: 1.9655230971693527e-06, Validation Loss: 1.5726746406836605e-06\n",
      "Epoch 835, Training Loss: 2.17702245208784e-06, Validation Loss: 3.63183850712613e-06\n",
      "Epoch 836, Training Loss: 1.2909025599583401e-06, Validation Loss: 1.6559938913021496e-06\n",
      "Epoch 837, Training Loss: 1.9409994820307475e-06, Validation Loss: 1.5921293123746918e-06\n",
      "Epoch 838, Training Loss: 1.2600630725501105e-06, Validation Loss: 1.6243647952210643e-06\n",
      "Epoch 839, Training Loss: 1.4152014955470804e-06, Validation Loss: 1.5837212362483369e-06\n",
      "Epoch 840, Training Loss: 1.9696167328220326e-06, Validation Loss: 2.3912370983696305e-06\n",
      "Epoch 841, Training Loss: 2.99531347991433e-06, Validation Loss: 3.1539802702279484e-06\n",
      "Epoch 842, Training Loss: 2.5822114366746973e-06, Validation Loss: 2.1448600257068034e-06\n",
      "Epoch 843, Training Loss: 4.98917279401212e-06, Validation Loss: 2.9231172553629257e-06\n",
      "Epoch 844, Training Loss: 1.9776771296164952e-06, Validation Loss: 2.0722781699709475e-06\n",
      "Epoch 845, Training Loss: 2.3633606360817794e-06, Validation Loss: 1.915753158216506e-06\n",
      "Epoch 846, Training Loss: 1.1881937780344742e-06, Validation Loss: 1.5903481024831487e-06\n",
      "Epoch 847, Training Loss: 2.2004021502652904e-06, Validation Loss: 2.3001554024347475e-06\n",
      "Epoch 848, Training Loss: 3.687887101477827e-06, Validation Loss: 2.9937692469774778e-06\n",
      "Epoch 849, Training Loss: 2.4125183699652553e-06, Validation Loss: 1.616092474575169e-06\n",
      "Epoch 850, Training Loss: 7.262962299137143e-06, Validation Loss: 9.482215214439128e-06\n",
      "Epoch 851, Training Loss: 1.3699243481823942e-06, Validation Loss: 1.5643552471698684e-06\n",
      "Epoch 852, Training Loss: 3.0838764359941706e-06, Validation Loss: 2.230547946658062e-06\n",
      "Epoch 853, Training Loss: 1.2502050594775937e-06, Validation Loss: 1.6701911260445445e-06\n",
      "Epoch 854, Training Loss: 2.038802904280601e-06, Validation Loss: 2.6245122177323927e-06\n",
      "Epoch 855, Training Loss: 1.3486219359037932e-06, Validation Loss: 1.6251298113309728e-06\n",
      "Epoch 856, Training Loss: 1.0257767826260533e-06, Validation Loss: 1.610755029609336e-06\n",
      "Epoch 857, Training Loss: 1.6431367839686573e-06, Validation Loss: 1.6561646000543084e-06\n",
      "Epoch 858, Training Loss: 2.242352820758242e-06, Validation Loss: 1.5941206665928033e-06\n",
      "Epoch 859, Training Loss: 1.0990587497872184e-06, Validation Loss: 1.780754677134714e-06\n",
      "Epoch 860, Training Loss: 3.393698534637224e-06, Validation Loss: 1.5892069665047982e-06\n",
      "Epoch 861, Training Loss: 1.872599455055024e-06, Validation Loss: 2.6411541090623544e-06\n",
      "Epoch 862, Training Loss: 2.027197069764952e-06, Validation Loss: 1.6647023953969235e-06\n",
      "Epoch 863, Training Loss: 1.7782374470698414e-06, Validation Loss: 1.5529313249907938e-06\n",
      "Epoch 864, Training Loss: 4.3236027522652876e-06, Validation Loss: 1.6094092522542466e-06\n",
      "Epoch 865, Training Loss: 2.9675593395950273e-06, Validation Loss: 2.00993295154502e-06\n",
      "Epoch 866, Training Loss: 2.446696726110531e-06, Validation Loss: 1.7243193632462272e-06\n",
      "Epoch 867, Training Loss: 1.6371393485314911e-06, Validation Loss: 1.958713608960381e-06\n",
      "Epoch 868, Training Loss: 2.7810253868665313e-06, Validation Loss: 1.8874159926908876e-06\n",
      "Epoch 869, Training Loss: 1.3633724620376597e-06, Validation Loss: 1.6153837584320723e-06\n",
      "Epoch 870, Training Loss: 1.662749014030851e-06, Validation Loss: 1.5794563364434693e-06\n",
      "Epoch 871, Training Loss: 1.3463071582009434e-06, Validation Loss: 1.6390457153213258e-06\n",
      "Epoch 872, Training Loss: 5.284305188979488e-06, Validation Loss: 6.3210875497557e-06\n",
      "Epoch 873, Training Loss: 2.8603305963770254e-06, Validation Loss: 1.9163473815763373e-06\n",
      "Epoch 874, Training Loss: 2.5007302610902116e-06, Validation Loss: 2.348094648756843e-06\n",
      "Epoch 875, Training Loss: 1.2049597444274696e-06, Validation Loss: 1.5673108634566153e-06\n",
      "Epoch 876, Training Loss: 2.0194624994473998e-06, Validation Loss: 1.6641356947515607e-06\n",
      "Epoch 877, Training Loss: 1.2448138022591593e-06, Validation Loss: 1.5498078323472113e-06\n",
      "Epoch 878, Training Loss: 4.276247182133375e-06, Validation Loss: 1.4759782432195891e-05\n",
      "Epoch 879, Training Loss: 3.1339163797383662e-06, Validation Loss: 1.576336379315716e-06\n",
      "Epoch 880, Training Loss: 2.0196484911139123e-06, Validation Loss: 1.677549767299271e-06\n",
      "Epoch 881, Training Loss: 2.4309620130225085e-06, Validation Loss: 1.5979030866995801e-06\n",
      "Epoch 882, Training Loss: 1.4270417523221113e-06, Validation Loss: 1.7754427114167504e-06\n",
      "Epoch 883, Training Loss: 8.851804409459874e-07, Validation Loss: 1.5534815877504261e-06\n",
      "Epoch 884, Training Loss: 1.2442151273717172e-06, Validation Loss: 1.5776017634217765e-06\n",
      "Epoch 885, Training Loss: 1.6244639482465573e-06, Validation Loss: 1.6447606480182537e-06\n",
      "Epoch 886, Training Loss: 8.455807574136998e-07, Validation Loss: 1.604561690027176e-06\n",
      "Epoch 887, Training Loss: 1.6473138657602249e-06, Validation Loss: 1.8792371001778797e-06\n",
      "Epoch 888, Training Loss: 1.669051926000975e-06, Validation Loss: 2.2743192481345668e-06\n",
      "Epoch 889, Training Loss: 1.4274463637775625e-06, Validation Loss: 1.5679932350284599e-06\n",
      "Epoch 890, Training Loss: 2.8017748263664544e-06, Validation Loss: 2.287141945286349e-06\n",
      "Epoch 891, Training Loss: 2.1301425476849545e-06, Validation Loss: 1.5872603661603296e-06\n",
      "Epoch 892, Training Loss: 7.540911610703915e-07, Validation Loss: 1.5910130882962018e-06\n",
      "Epoch 893, Training Loss: 2.52285212809511e-06, Validation Loss: 2.0712819542976524e-06\n",
      "Epoch 894, Training Loss: 2.005529722737265e-06, Validation Loss: 3.871596572066288e-06\n",
      "Epoch 895, Training Loss: 1.2319019333517645e-06, Validation Loss: 1.621695516251939e-06\n",
      "Epoch 896, Training Loss: 1.3214747696110862e-06, Validation Loss: 1.6368582024300456e-06\n",
      "Epoch 897, Training Loss: 1.8223322513222229e-06, Validation Loss: 1.6045590871778881e-06\n",
      "Epoch 898, Training Loss: 1.0223152457911056e-05, Validation Loss: 1.1061609552453559e-05\n",
      "Epoch 899, Training Loss: 1.360884880341473e-06, Validation Loss: 1.6046777581334698e-06\n",
      "Epoch 900, Training Loss: 2.0451407181099057e-06, Validation Loss: 2.2869279526241174e-06\n",
      "Epoch 901, Training Loss: 4.608639756042976e-06, Validation Loss: 4.34346106219451e-06\n",
      "Epoch 902, Training Loss: 1.8033159676633659e-06, Validation Loss: 1.6274197767004599e-06\n",
      "Epoch 903, Training Loss: 1.8825910501618637e-06, Validation Loss: 1.692043936131155e-06\n",
      "Epoch 904, Training Loss: 3.4089734981535003e-06, Validation Loss: 3.7285135954769162e-06\n",
      "Epoch 905, Training Loss: 1.7642771581449779e-06, Validation Loss: 2.1221408944001946e-06\n",
      "Epoch 906, Training Loss: 2.8499371182988398e-06, Validation Loss: 1.5523998249041056e-06\n",
      "Epoch 907, Training Loss: 1.5847272152313963e-06, Validation Loss: 1.7579615921781073e-06\n",
      "Epoch 908, Training Loss: 1.4867636082271929e-06, Validation Loss: 1.7903949925850936e-06\n",
      "Epoch 909, Training Loss: 1.9566350601962768e-06, Validation Loss: 1.5748379578297457e-06\n",
      "Epoch 910, Training Loss: 3.348318841744913e-06, Validation Loss: 3.7386289723781953e-06\n",
      "Epoch 911, Training Loss: 1.3196721511121723e-06, Validation Loss: 1.5554658369032399e-06\n",
      "Epoch 912, Training Loss: 2.058585323538864e-06, Validation Loss: 1.5644693199635423e-06\n",
      "Epoch 913, Training Loss: 1.1977790563832968e-06, Validation Loss: 1.5479801614691594e-06\n",
      "Epoch 914, Training Loss: 1.9928709207306383e-06, Validation Loss: 1.69197882760678e-06\n",
      "Epoch 915, Training Loss: 2.4408277567999903e-06, Validation Loss: 2.948771704786908e-06\n",
      "Epoch 916, Training Loss: 1.237085257344006e-06, Validation Loss: 1.5966561383262902e-06\n",
      "Epoch 917, Training Loss: 2.163186081816093e-06, Validation Loss: 1.5412196110991067e-06\n",
      "Epoch 918, Training Loss: 3.381217993592145e-06, Validation Loss: 2.163750473242822e-06\n",
      "Epoch 919, Training Loss: 1.2996588338864967e-06, Validation Loss: 1.572490884013483e-06\n",
      "Epoch 920, Training Loss: 9.744799172040075e-07, Validation Loss: 1.5722703478410028e-06\n",
      "Epoch 921, Training Loss: 1.8541899180490873e-06, Validation Loss: 1.908633470401971e-06\n",
      "Epoch 922, Training Loss: 2.690109567993204e-06, Validation Loss: 2.479454882383029e-06\n",
      "Epoch 923, Training Loss: 1.3893882169213612e-06, Validation Loss: 1.626785016379721e-06\n",
      "Epoch 924, Training Loss: 3.148179985146271e-06, Validation Loss: 2.633646030442103e-06\n",
      "Epoch 925, Training Loss: 1.511584969193791e-06, Validation Loss: 1.740767750888947e-06\n",
      "Epoch 926, Training Loss: 2.1590940377791412e-06, Validation Loss: 1.6136974274478079e-06\n",
      "Epoch 927, Training Loss: 1.5693984778408776e-06, Validation Loss: 2.131007946376091e-06\n",
      "Epoch 928, Training Loss: 1.9864955902448855e-06, Validation Loss: 1.7776170221524554e-06\n",
      "Epoch 929, Training Loss: 1.7272507193411002e-06, Validation Loss: 1.8912788621659924e-06\n",
      "Epoch 930, Training Loss: 1.517964733466215e-06, Validation Loss: 1.5504159490351442e-06\n",
      "Epoch 931, Training Loss: 1.34823039843468e-06, Validation Loss: 1.5491883922330054e-06\n",
      "Epoch 932, Training Loss: 6.524117452499922e-06, Validation Loss: 5.4094807797045394e-06\n",
      "Epoch 933, Training Loss: 1.7095323983085109e-06, Validation Loss: 1.5601823243614374e-06\n",
      "Epoch 934, Training Loss: 1.3011671171625494e-06, Validation Loss: 1.6395011585515183e-06\n",
      "Epoch 935, Training Loss: 1.2276775578357046e-06, Validation Loss: 1.6922150232360063e-06\n",
      "Epoch 936, Training Loss: 2.2689996512781363e-06, Validation Loss: 2.1020104804557857e-06\n",
      "Epoch 937, Training Loss: 1.3184726412873715e-06, Validation Loss: 1.5912969990877521e-06\n",
      "Epoch 938, Training Loss: 2.243767085019499e-06, Validation Loss: 1.550524589480687e-06\n",
      "Epoch 939, Training Loss: 3.0350201996043324e-06, Validation Loss: 1.9298021192541924e-06\n",
      "Epoch 940, Training Loss: 1.8633261333889095e-06, Validation Loss: 2.638198418553309e-06\n",
      "Epoch 941, Training Loss: 1.7372738057019887e-06, Validation Loss: 1.588232270229232e-06\n",
      "Epoch 942, Training Loss: 1.6365355577363516e-06, Validation Loss: 1.6824065164365973e-06\n",
      "Epoch 943, Training Loss: 1.776869112291024e-06, Validation Loss: 1.6276348417843897e-06\n",
      "Epoch 944, Training Loss: 1.3914021792515996e-06, Validation Loss: 1.585336427512867e-06\n",
      "Epoch 945, Training Loss: 1.297944777434168e-06, Validation Loss: 1.5568963362966315e-06\n",
      "Epoch 946, Training Loss: 1.4000072496855864e-06, Validation Loss: 1.5500078614417025e-06\n",
      "Epoch 947, Training Loss: 1.2136740679125069e-06, Validation Loss: 1.677432801646255e-06\n",
      "Epoch 948, Training Loss: 2.064988393613021e-06, Validation Loss: 2.973606540902316e-06\n",
      "Epoch 949, Training Loss: 2.3361844796454534e-06, Validation Loss: 3.006805143880107e-06\n",
      "Epoch 950, Training Loss: 2.273892505399999e-06, Validation Loss: 1.9863503716023957e-06\n",
      "Epoch 951, Training Loss: 1.570090148561576e-06, Validation Loss: 1.9553017876692166e-06\n",
      "Epoch 952, Training Loss: 2.2621302377956454e-06, Validation Loss: 1.5472991077956884e-06\n",
      "Epoch 953, Training Loss: 1.7978254618356004e-06, Validation Loss: 2.5600021864905504e-06\n",
      "Epoch 954, Training Loss: 2.5028882646438433e-06, Validation Loss: 1.6280137875310753e-06\n",
      "Epoch 955, Training Loss: 1.7955302382688387e-06, Validation Loss: 1.569876735557393e-06\n",
      "Epoch 956, Training Loss: 1.2856826288043521e-06, Validation Loss: 1.581026836108675e-06\n",
      "Epoch 957, Training Loss: 1.3899701798436581e-06, Validation Loss: 1.5924294608327966e-06\n",
      "Epoch 958, Training Loss: 1.5904083738860209e-06, Validation Loss: 1.8024641230501175e-06\n",
      "Epoch 959, Training Loss: 1.9854533093166538e-06, Validation Loss: 2.6489237311319245e-06\n",
      "Epoch 960, Training Loss: 1.730566850710602e-06, Validation Loss: 1.910587651196507e-06\n",
      "Epoch 961, Training Loss: 1.3128076261637034e-06, Validation Loss: 1.7188621318179748e-06\n",
      "Epoch 962, Training Loss: 3.3335722946503665e-06, Validation Loss: 3.7950777979835515e-06\n",
      "Epoch 963, Training Loss: 2.1677785753126955e-06, Validation Loss: 2.0159737773378966e-06\n",
      "Epoch 964, Training Loss: 1.8640657799551263e-06, Validation Loss: 1.5980752762771095e-06\n",
      "Epoch 965, Training Loss: 1.5971297671057982e-06, Validation Loss: 2.520206090118979e-06\n",
      "Epoch 966, Training Loss: 1.4612260201829486e-06, Validation Loss: 1.6456663457968826e-06\n",
      "Epoch 967, Training Loss: 1.319921807407809e-06, Validation Loss: 1.5556168615397733e-06\n",
      "Epoch 968, Training Loss: 1.5078770729815005e-06, Validation Loss: 1.5786993135320207e-06\n",
      "Epoch 969, Training Loss: 1.3707798416362493e-06, Validation Loss: 1.8132188906573827e-06\n",
      "Epoch 970, Training Loss: 1.8121522771252785e-06, Validation Loss: 1.7467433776424502e-06\n",
      "Epoch 971, Training Loss: 1.5316390999942087e-06, Validation Loss: 1.576163679233597e-06\n",
      "Epoch 972, Training Loss: 1.1295323929516599e-06, Validation Loss: 1.5751244805220834e-06\n",
      "Epoch 973, Training Loss: 1.834654312915518e-06, Validation Loss: 1.8083536240587267e-06\n",
      "Epoch 974, Training Loss: 2.901409743572003e-06, Validation Loss: 3.0461529506028223e-06\n",
      "Epoch 975, Training Loss: 1.7322688563581323e-06, Validation Loss: 1.5391767603641073e-06\n",
      "Epoch 976, Training Loss: 1.4824570371274604e-06, Validation Loss: 1.551033646554605e-06\n",
      "Epoch 977, Training Loss: 1.7394183942087693e-06, Validation Loss: 1.709629093670773e-06\n",
      "Epoch 978, Training Loss: 1.6433158407380688e-06, Validation Loss: 1.5617490278276193e-06\n",
      "Epoch 979, Training Loss: 5.029398380429484e-06, Validation Loss: 6.046327143849339e-06\n",
      "Epoch 980, Training Loss: 1.4417748843698064e-06, Validation Loss: 1.6182047437966971e-06\n",
      "Epoch 981, Training Loss: 7.84275744081242e-06, Validation Loss: 8.267529106297223e-06\n",
      "Epoch 982, Training Loss: 2.8147176180937095e-06, Validation Loss: 5.9301595420212e-06\n",
      "Epoch 983, Training Loss: 1.2306849157539546e-06, Validation Loss: 1.7259070180380092e-06\n",
      "Epoch 984, Training Loss: 1.204321279146825e-06, Validation Loss: 1.5891141006510791e-06\n",
      "Epoch 985, Training Loss: 1.952158754647826e-06, Validation Loss: 1.6770800697671641e-06\n",
      "Epoch 986, Training Loss: 4.79265145258978e-06, Validation Loss: 4.158714815437946e-06\n",
      "Epoch 987, Training Loss: 1.9221147340431344e-06, Validation Loss: 1.7206095756325346e-06\n",
      "Epoch 988, Training Loss: 1.385664631925465e-06, Validation Loss: 1.686216048681811e-06\n",
      "Epoch 989, Training Loss: 1.511377149654436e-06, Validation Loss: 1.5741287065619797e-06\n",
      "Epoch 990, Training Loss: 1.4221362789612613e-06, Validation Loss: 1.60678045448879e-06\n",
      "Epoch 991, Training Loss: 1.46479465001903e-06, Validation Loss: 1.7835799377037333e-06\n",
      "Epoch 992, Training Loss: 2.8655899768637028e-06, Validation Loss: 1.8760394659751632e-06\n",
      "Epoch 993, Training Loss: 1.1171415508215432e-06, Validation Loss: 1.5362721072798791e-06\n",
      "Epoch 994, Training Loss: 2.654083345987601e-06, Validation Loss: 1.73197657437749e-06\n",
      "Epoch 995, Training Loss: 3.899038347299211e-06, Validation Loss: 1.6810096205552524e-06\n",
      "Epoch 996, Training Loss: 1.2289256119402125e-06, Validation Loss: 1.5587503951931345e-06\n",
      "Epoch 997, Training Loss: 1.3012958106628503e-06, Validation Loss: 1.560265922565522e-06\n",
      "Epoch 998, Training Loss: 4.117846401641145e-06, Validation Loss: 3.165579926551881e-06\n",
      "Epoch 999, Training Loss: 1.815247173908574e-06, Validation Loss: 1.5466689492574572e-06\n",
      "Epoch 1000, Training Loss: 1.3653846053784946e-06, Validation Loss: 1.7068303438585424e-06\n",
      "Epoch 1001, Training Loss: 1.6418969153164653e-06, Validation Loss: 1.93803741311081e-06\n",
      "Epoch 1002, Training Loss: 1.3205898312662612e-06, Validation Loss: 1.5708311810934449e-06\n",
      "Epoch 1003, Training Loss: 1.2198461263324134e-06, Validation Loss: 1.7611074916284607e-06\n",
      "Epoch 1004, Training Loss: 2.465220859448891e-06, Validation Loss: 2.356287951981682e-06\n",
      "Epoch 1005, Training Loss: 1.595436287971097e-06, Validation Loss: 1.5472171830328902e-06\n",
      "Epoch 1006, Training Loss: 2.8316749194345903e-06, Validation Loss: 1.5255562926210536e-06\n",
      "Epoch 1007, Training Loss: 1.6557163462493918e-06, Validation Loss: 1.7159194928475479e-06\n",
      "Epoch 1008, Training Loss: 1.2205598522996297e-06, Validation Loss: 2.370924043986002e-06\n",
      "Epoch 1009, Training Loss: 3.188702521583764e-06, Validation Loss: 2.4020942261298043e-06\n",
      "Epoch 1010, Training Loss: 1.9514973246259615e-06, Validation Loss: 1.951824888990837e-06\n",
      "Epoch 1011, Training Loss: 1.6738381418690551e-06, Validation Loss: 1.6845802138426746e-06\n",
      "Epoch 1012, Training Loss: 1.8503751562093385e-06, Validation Loss: 1.9213221510474386e-06\n",
      "Epoch 1013, Training Loss: 2.258383574371692e-06, Validation Loss: 1.7625769866933708e-06\n",
      "Epoch 1014, Training Loss: 2.740537411227706e-06, Validation Loss: 2.665111774468022e-06\n",
      "Epoch 1015, Training Loss: 1.5405512385768816e-06, Validation Loss: 1.8450866317035452e-06\n",
      "Epoch 1016, Training Loss: 1.8829875898518367e-06, Validation Loss: 1.8125246652413756e-06\n",
      "Epoch 1017, Training Loss: 1.3258539866001229e-06, Validation Loss: 1.7208125602227465e-06\n",
      "Epoch 1018, Training Loss: 2.6469713247934124e-06, Validation Loss: 1.6118788399308538e-06\n",
      "Epoch 1019, Training Loss: 1.4982288121245801e-06, Validation Loss: 1.7443131889721932e-06\n",
      "Epoch 1020, Training Loss: 1.2645510878428468e-06, Validation Loss: 1.5276593952076855e-06\n",
      "Epoch 1021, Training Loss: 1.3068527096038451e-06, Validation Loss: 1.6604752899485184e-06\n",
      "Epoch 1022, Training Loss: 2.019795601881924e-06, Validation Loss: 2.2544529077147058e-06\n",
      "Epoch 1023, Training Loss: 1.7833817764767446e-06, Validation Loss: 1.5854602697713252e-06\n",
      "Epoch 1024, Training Loss: 1.5353350590885384e-06, Validation Loss: 1.6571173029956153e-06\n",
      "Epoch 1025, Training Loss: 1.2345474260655465e-06, Validation Loss: 1.551306187214143e-06\n",
      "Epoch 1026, Training Loss: 1.91037793229043e-06, Validation Loss: 1.8009300726885012e-06\n",
      "Epoch 1027, Training Loss: 1.2088790981579223e-06, Validation Loss: 1.823948694228148e-06\n",
      "Epoch 1028, Training Loss: 1.50755874983588e-06, Validation Loss: 1.7700782799472982e-06\n",
      "Epoch 1029, Training Loss: 1.1041688594559673e-06, Validation Loss: 1.558086421337761e-06\n",
      "Epoch 1030, Training Loss: 4.101162630831823e-06, Validation Loss: 2.481027254622517e-06\n",
      "Epoch 1031, Training Loss: 2.919432517956011e-06, Validation Loss: 1.611484594977115e-06\n",
      "Epoch 1032, Training Loss: 1.2453978115445352e-06, Validation Loss: 1.5774595991173857e-06\n",
      "Epoch 1033, Training Loss: 4.1131115722237155e-06, Validation Loss: 1.817363003628071e-06\n",
      "Epoch 1034, Training Loss: 3.2117027330968995e-06, Validation Loss: 3.938492044983884e-06\n",
      "Epoch 1035, Training Loss: 2.0257928099454148e-06, Validation Loss: 1.801537948244479e-06\n",
      "Epoch 1036, Training Loss: 1.4335618061522837e-06, Validation Loss: 1.5639755085117988e-06\n",
      "Epoch 1037, Training Loss: 2.165016439903411e-06, Validation Loss: 1.5274837990076845e-06\n",
      "Epoch 1038, Training Loss: 9.218336344929412e-06, Validation Loss: 7.313118782751804e-06\n",
      "Epoch 1039, Training Loss: 6.547877546836389e-06, Validation Loss: 2.5402761899374363e-06\n",
      "Epoch 1040, Training Loss: 3.1377510367747163e-06, Validation Loss: 3.0486128441231238e-06\n",
      "Epoch 1041, Training Loss: 1.3393134850048227e-06, Validation Loss: 1.591720286683809e-06\n",
      "Epoch 1042, Training Loss: 6.253752871998586e-06, Validation Loss: 1.64997118154307e-06\n",
      "Epoch 1043, Training Loss: 1.521085550848511e-06, Validation Loss: 1.53411047841954e-06\n",
      "Epoch 1044, Training Loss: 1.5815487586223753e-06, Validation Loss: 1.602695909438408e-06\n",
      "Epoch 1045, Training Loss: 1.1232572205699398e-06, Validation Loss: 2.140362693886334e-06\n",
      "Epoch 1046, Training Loss: 1.5604946383973584e-06, Validation Loss: 1.55571981981579e-06\n",
      "Epoch 1047, Training Loss: 4.89622925670119e-06, Validation Loss: 4.137295564951447e-06\n",
      "Epoch 1048, Training Loss: 5.025710834161146e-06, Validation Loss: 3.6051065823702614e-06\n",
      "Epoch 1049, Training Loss: 1.5405578324134694e-06, Validation Loss: 1.840613676296207e-06\n",
      "Epoch 1050, Training Loss: 1.8932232705992647e-06, Validation Loss: 2.2239328071843556e-06\n",
      "Epoch 1051, Training Loss: 2.0030468022014247e-06, Validation Loss: 1.8546740291895053e-06\n",
      "Epoch 1052, Training Loss: 1.915924258355517e-06, Validation Loss: 1.8655633592895535e-06\n",
      "Epoch 1053, Training Loss: 1.7173190371977398e-06, Validation Loss: 1.6160849426411399e-06\n",
      "Epoch 1054, Training Loss: 1.7731211983118556e-06, Validation Loss: 1.767129113397262e-06\n",
      "Epoch 1055, Training Loss: 2.5885299237415893e-06, Validation Loss: 2.7667958614416235e-06\n",
      "Epoch 1056, Training Loss: 1.4860324881738052e-06, Validation Loss: 1.6020675767687203e-06\n",
      "Epoch 1057, Training Loss: 1.2496548151830211e-06, Validation Loss: 1.5702378730212754e-06\n",
      "Epoch 1058, Training Loss: 1.1360671123838983e-06, Validation Loss: 1.5767876331497324e-06\n",
      "Epoch 1059, Training Loss: 1.6929698176681995e-06, Validation Loss: 3.1485944048069258e-06\n",
      "Epoch 1060, Training Loss: 1.278299578189035e-06, Validation Loss: 1.5705677951208235e-06\n",
      "Epoch 1061, Training Loss: 9.909525033435784e-06, Validation Loss: 1.2751361010156817e-05\n",
      "Epoch 1062, Training Loss: 3.005418420798378e-06, Validation Loss: 1.6178427692501115e-06\n",
      "Epoch 1063, Training Loss: 1.7781719634513138e-06, Validation Loss: 1.5609046478052414e-06\n",
      "Epoch 1064, Training Loss: 2.263657279399922e-06, Validation Loss: 2.11045102069139e-06\n",
      "Epoch 1065, Training Loss: 1.4695424397359602e-06, Validation Loss: 1.5482448041520157e-06\n",
      "Epoch 1066, Training Loss: 6.9361121859401464e-06, Validation Loss: 7.22293878853941e-06\n",
      "Epoch 1067, Training Loss: 1.3192988035370945e-06, Validation Loss: 2.505271641853518e-06\n",
      "Epoch 1068, Training Loss: 1.5637583601346705e-06, Validation Loss: 1.7278396555388573e-06\n",
      "Epoch 1069, Training Loss: 9.777108971320558e-07, Validation Loss: 1.5120959169449911e-06\n",
      "Epoch 1070, Training Loss: 1.3957596820546314e-06, Validation Loss: 1.7443110339911165e-06\n",
      "Epoch 1071, Training Loss: 1.0523007176743704e-06, Validation Loss: 1.557920514587286e-06\n",
      "Epoch 1072, Training Loss: 1.607372496437165e-06, Validation Loss: 1.6139312664285415e-06\n",
      "Epoch 1073, Training Loss: 1.5201461565084173e-06, Validation Loss: 2.7919680657823906e-06\n",
      "Epoch 1074, Training Loss: 2.7784312806033995e-06, Validation Loss: 2.9672395855883153e-06\n",
      "Epoch 1075, Training Loss: 1.60142440108757e-06, Validation Loss: 1.664576922420258e-06\n",
      "Epoch 1076, Training Loss: 5.146656803844962e-06, Validation Loss: 8.503180058263531e-06\n",
      "Epoch 1077, Training Loss: 3.8050307011872064e-06, Validation Loss: 2.3366076046765738e-06\n",
      "Epoch 1078, Training Loss: 1.3500282420864096e-06, Validation Loss: 2.0595380478417106e-06\n",
      "Epoch 1079, Training Loss: 1.2004252312181052e-06, Validation Loss: 1.6474610764567946e-06\n",
      "Epoch 1080, Training Loss: 5.368151960283285e-06, Validation Loss: 6.929633892833657e-06\n",
      "Epoch 1081, Training Loss: 2.0901513835269725e-06, Validation Loss: 1.5340496291689195e-06\n",
      "Epoch 1082, Training Loss: 1.8597887674332014e-06, Validation Loss: 1.7580079898041176e-06\n",
      "Epoch 1083, Training Loss: 1.7193368648804608e-06, Validation Loss: 1.5579194827163066e-06\n",
      "Epoch 1084, Training Loss: 1.3556312978835194e-06, Validation Loss: 1.5731898813801946e-06\n",
      "Epoch 1085, Training Loss: 1.7670879515208071e-06, Validation Loss: 1.5053243912693485e-06\n",
      "Epoch 1086, Training Loss: 8.253461601270828e-06, Validation Loss: 8.73504728055207e-06\n",
      "Epoch 1087, Training Loss: 2.1897028545936337e-06, Validation Loss: 1.740131353694981e-06\n",
      "Epoch 1088, Training Loss: 1.199176494992571e-06, Validation Loss: 1.5630863804087375e-06\n",
      "Epoch 1089, Training Loss: 3.0583657917304663e-06, Validation Loss: 1.5932685851923198e-06\n",
      "Epoch 1090, Training Loss: 1.894268621072115e-06, Validation Loss: 2.015176381116657e-06\n",
      "Epoch 1091, Training Loss: 1.662971953919623e-06, Validation Loss: 1.607408127847231e-06\n",
      "Epoch 1092, Training Loss: 2.6311836336390115e-06, Validation Loss: 2.0025105728871113e-06\n",
      "Epoch 1093, Training Loss: 1.6438316379208118e-06, Validation Loss: 1.911669113533034e-06\n",
      "Epoch 1094, Training Loss: 1.1350157365086488e-06, Validation Loss: 1.5101539239949605e-06\n",
      "Epoch 1095, Training Loss: 1.3904054867452942e-06, Validation Loss: 1.544473906242448e-06\n",
      "Epoch 1096, Training Loss: 2.4907662918849383e-06, Validation Loss: 1.5758606361058503e-06\n",
      "Epoch 1097, Training Loss: 1.0457840744493296e-06, Validation Loss: 1.5632490691699971e-06\n",
      "Epoch 1098, Training Loss: 2.085954292851966e-06, Validation Loss: 1.6329620875202272e-06\n",
      "Epoch 1099, Training Loss: 1.3561977993958862e-06, Validation Loss: 1.5640538640871884e-06\n",
      "Epoch 1100, Training Loss: 9.869970654108329e-07, Validation Loss: 1.5127772664214765e-06\n",
      "Epoch 1101, Training Loss: 2.7212954591959715e-06, Validation Loss: 1.5892286944490828e-06\n",
      "Epoch 1102, Training Loss: 2.1972887225274462e-06, Validation Loss: 1.6248319192307427e-06\n",
      "Epoch 1103, Training Loss: 1.7995548660110217e-06, Validation Loss: 1.5232229604297554e-06\n",
      "Epoch 1104, Training Loss: 1.9262838577560615e-06, Validation Loss: 1.7018196927939202e-06\n",
      "Epoch 1105, Training Loss: 9.616737770556938e-07, Validation Loss: 1.6484291046734745e-06\n",
      "Epoch 1106, Training Loss: 2.936194960057037e-06, Validation Loss: 2.040574033697826e-06\n",
      "Epoch 1107, Training Loss: 2.3946381588757504e-06, Validation Loss: 1.6524803315150843e-06\n",
      "Epoch 1108, Training Loss: 1.0619933163980022e-06, Validation Loss: 1.6584163492674413e-06\n",
      "Epoch 1109, Training Loss: 1.7221631196662202e-06, Validation Loss: 2.0350724155247634e-06\n",
      "Epoch 1110, Training Loss: 2.0837553620367544e-06, Validation Loss: 2.2327022412691284e-06\n",
      "Epoch 1111, Training Loss: 3.8011628475942416e-06, Validation Loss: 5.497214304132922e-06\n",
      "Epoch 1112, Training Loss: 1.9860394786519464e-06, Validation Loss: 2.041607973125885e-06\n",
      "Epoch 1113, Training Loss: 2.167061211366672e-06, Validation Loss: 2.0776997276281147e-06\n",
      "Epoch 1114, Training Loss: 1.7511910073153558e-06, Validation Loss: 1.62842470715778e-06\n",
      "Epoch 1115, Training Loss: 1.4770351981496788e-06, Validation Loss: 1.6399087881390691e-06\n",
      "Epoch 1116, Training Loss: 1.8828335441867239e-06, Validation Loss: 2.95356630379206e-06\n",
      "Epoch 1117, Training Loss: 1.44137152346957e-06, Validation Loss: 1.5571779161459485e-06\n",
      "Epoch 1118, Training Loss: 1.867511855380144e-06, Validation Loss: 1.8631304920994703e-06\n",
      "Epoch 1119, Training Loss: 1.7304289485764457e-06, Validation Loss: 1.5185908901553462e-06\n",
      "Epoch 1120, Training Loss: 2.0119177861488424e-06, Validation Loss: 2.112007089369405e-06\n",
      "Epoch 1121, Training Loss: 3.5594925975601654e-06, Validation Loss: 1.7295008943173847e-06\n",
      "Epoch 1122, Training Loss: 1.4710408322571311e-06, Validation Loss: 2.0811718177877307e-06\n",
      "Epoch 1123, Training Loss: 2.5301840196334524e-06, Validation Loss: 3.769464968803975e-06\n",
      "Epoch 1124, Training Loss: 4.738794814329594e-06, Validation Loss: 4.961908312316444e-06\n",
      "Epoch 1125, Training Loss: 3.36858574883081e-05, Validation Loss: 1.4363463170139437e-05\n",
      "Epoch 1126, Training Loss: 1.3326580301509239e-06, Validation Loss: 1.5707038956444558e-06\n",
      "Epoch 1127, Training Loss: 1.5907144188531674e-06, Validation Loss: 1.8173273533908853e-06\n",
      "Epoch 1128, Training Loss: 2.7042133297072724e-06, Validation Loss: 1.6400678302320817e-06\n",
      "Epoch 1129, Training Loss: 1.406395767844515e-06, Validation Loss: 1.8938196420610328e-06\n",
      "Epoch 1130, Training Loss: 3.311681211926043e-06, Validation Loss: 1.5863631036554117e-06\n",
      "Epoch 1131, Training Loss: 4.769506631419063e-06, Validation Loss: 4.794231450536835e-06\n",
      "Epoch 1132, Training Loss: 3.0432388484769035e-06, Validation Loss: 1.6429224998481317e-06\n",
      "Epoch 1133, Training Loss: 7.92430728324689e-06, Validation Loss: 3.0514771977578e-06\n",
      "Epoch 1134, Training Loss: 1.2734420806737035e-06, Validation Loss: 1.753294195710206e-06\n",
      "Epoch 1135, Training Loss: 1.285740268031077e-06, Validation Loss: 1.6048276339536463e-06\n",
      "Epoch 1136, Training Loss: 1.3998173926665913e-06, Validation Loss: 1.5904373184100807e-06\n",
      "Epoch 1137, Training Loss: 1.1620884379226482e-06, Validation Loss: 1.5743996099862306e-06\n",
      "Epoch 1138, Training Loss: 2.197269168391358e-06, Validation Loss: 1.5762570418336259e-06\n",
      "Epoch 1139, Training Loss: 4.6656973609060515e-06, Validation Loss: 6.028192311897577e-06\n",
      "Epoch 1140, Training Loss: 1.8587323893370922e-06, Validation Loss: 2.579994635302364e-06\n",
      "Epoch 1141, Training Loss: 1.1216768598387716e-06, Validation Loss: 1.6310319511298087e-06\n",
      "Epoch 1142, Training Loss: 1.3113221939420328e-06, Validation Loss: 1.6431398984087149e-06\n",
      "Epoch 1143, Training Loss: 1.5426755908265477e-06, Validation Loss: 1.5337317795977696e-06\n",
      "Epoch 1144, Training Loss: 8.810487088339869e-06, Validation Loss: 4.307152162573692e-06\n",
      "Epoch 1145, Training Loss: 2.8532638225442497e-06, Validation Loss: 1.5284322819587462e-06\n",
      "Epoch 1146, Training Loss: 3.058064521610504e-06, Validation Loss: 6.28807501230466e-06\n",
      "Epoch 1147, Training Loss: 1.8419220850773854e-06, Validation Loss: 1.7016284697225727e-06\n",
      "Epoch 1148, Training Loss: 1.4905808711773716e-06, Validation Loss: 1.549878054244864e-06\n",
      "Epoch 1149, Training Loss: 1.9656727090477943e-06, Validation Loss: 1.6406196009715838e-06\n",
      "Epoch 1150, Training Loss: 1.4150455172057264e-06, Validation Loss: 1.5293421611682314e-06\n",
      "Epoch 1151, Training Loss: 1.076116859621834e-06, Validation Loss: 1.6681757263361349e-06\n",
      "Epoch 1152, Training Loss: 3.2248990464722738e-06, Validation Loss: 3.3369261302233926e-06\n",
      "Epoch 1153, Training Loss: 1.0145972737518605e-06, Validation Loss: 1.617207243896608e-06\n",
      "Epoch 1154, Training Loss: 1.6916081904128077e-06, Validation Loss: 1.5529777570125034e-06\n",
      "Epoch 1155, Training Loss: 1.3253096540211118e-06, Validation Loss: 1.5905936620699676e-06\n",
      "Epoch 1156, Training Loss: 1.5532511952187633e-06, Validation Loss: 1.5054243524127452e-06\n",
      "Epoch 1157, Training Loss: 1.5009777598606888e-06, Validation Loss: 1.8404896964549517e-06\n",
      "Epoch 1158, Training Loss: 1.3046727644905332e-06, Validation Loss: 1.7610687287615176e-06\n",
      "Epoch 1159, Training Loss: 1.050002538249828e-06, Validation Loss: 1.544025166553021e-06\n",
      "Epoch 1160, Training Loss: 1.0549781563895522e-06, Validation Loss: 1.5756832498812452e-06\n",
      "Epoch 1161, Training Loss: 8.851748134475201e-07, Validation Loss: 1.769608009998343e-06\n",
      "Epoch 1162, Training Loss: 1.8611142422741977e-06, Validation Loss: 1.581144596121315e-06\n",
      "Epoch 1163, Training Loss: 1.3913142993260408e-06, Validation Loss: 1.5633524970378243e-06\n",
      "Epoch 1164, Training Loss: 1.5923719729471486e-06, Validation Loss: 1.6830592809145954e-06\n",
      "Epoch 1165, Training Loss: 1.3793214748147875e-06, Validation Loss: 1.5276037784480802e-06\n",
      "Epoch 1166, Training Loss: 2.7367477741790935e-06, Validation Loss: 2.1828598533876135e-06\n",
      "Epoch 1167, Training Loss: 1.2384490446493146e-06, Validation Loss: 1.653932831121875e-06\n",
      "Epoch 1168, Training Loss: 1.4578945410903543e-06, Validation Loss: 1.618133346652308e-06\n",
      "Epoch 1169, Training Loss: 1.4760483963982551e-06, Validation Loss: 1.6445941073007312e-06\n",
      "Epoch 1170, Training Loss: 2.9757256925222464e-06, Validation Loss: 1.89874471228105e-06\n",
      "Epoch 1171, Training Loss: 1.361423983325949e-06, Validation Loss: 1.5154245602026923e-06\n",
      "Epoch 1172, Training Loss: 1.8114855038220412e-06, Validation Loss: 1.5721409696852556e-06\n",
      "Epoch 1173, Training Loss: 2.752555246843258e-06, Validation Loss: 2.221826166909534e-06\n",
      "Epoch 1174, Training Loss: 2.735089537964086e-06, Validation Loss: 3.0511840066445157e-06\n",
      "Epoch 1175, Training Loss: 6.4961800489982124e-06, Validation Loss: 8.632991538995915e-06\n",
      "Epoch 1176, Training Loss: 2.0481252249737736e-06, Validation Loss: 5.607704415698284e-06\n",
      "Epoch 1177, Training Loss: 1.4728148016729392e-06, Validation Loss: 1.5264942852458852e-06\n",
      "Epoch 1178, Training Loss: 2.8826980269514024e-06, Validation Loss: 2.2215182261962657e-06\n",
      "Epoch 1179, Training Loss: 2.483821845089551e-06, Validation Loss: 1.5450241595885203e-06\n",
      "Epoch 1180, Training Loss: 1.7096087958634598e-06, Validation Loss: 1.566890526649499e-06\n",
      "Epoch 1181, Training Loss: 9.260871820515604e-07, Validation Loss: 1.504650901934276e-06\n",
      "Epoch 1182, Training Loss: 1.5639416233170778e-06, Validation Loss: 2.1171684649228265e-06\n",
      "Epoch 1183, Training Loss: 1.14931845018873e-06, Validation Loss: 1.6707096459184328e-06\n",
      "Epoch 1184, Training Loss: 2.4586274776083883e-06, Validation Loss: 1.5445339607713844e-06\n",
      "Epoch 1185, Training Loss: 3.3888634334289236e-06, Validation Loss: 2.7416021246689682e-06\n",
      "Epoch 1186, Training Loss: 4.149505002715159e-06, Validation Loss: 4.235883517620698e-06\n",
      "Epoch 1187, Training Loss: 1.930006874317769e-06, Validation Loss: 1.9379120842340216e-06\n",
      "Epoch 1188, Training Loss: 1.7585880414117128e-06, Validation Loss: 1.6794364872656593e-06\n",
      "Epoch 1189, Training Loss: 1.3867502275388688e-06, Validation Loss: 1.5172559925220087e-06\n",
      "Epoch 1190, Training Loss: 2.2165984319144627e-06, Validation Loss: 1.4969816220762449e-06\n",
      "Epoch 1191, Training Loss: 2.062315161310835e-06, Validation Loss: 1.5538096999120704e-06\n",
      "Epoch 1192, Training Loss: 3.961921720474493e-06, Validation Loss: 6.675548641854388e-06\n",
      "Epoch 1193, Training Loss: 1.1969485740337404e-06, Validation Loss: 1.5779948128534196e-06\n",
      "Epoch 1194, Training Loss: 3.832754373434e-06, Validation Loss: 1.8110884681098782e-06\n",
      "Epoch 1195, Training Loss: 8.814701573101047e-07, Validation Loss: 1.5161452259548194e-06\n",
      "Epoch 1196, Training Loss: 1.3562556659962866e-06, Validation Loss: 1.5210378304363174e-06\n",
      "Epoch 1197, Training Loss: 1.262305318050494e-06, Validation Loss: 1.5195354380573766e-06\n",
      "Epoch 1198, Training Loss: 2.1605958409054438e-06, Validation Loss: 1.5510826644087826e-06\n",
      "Epoch 1199, Training Loss: 2.13154589800979e-06, Validation Loss: 1.6860649204961197e-06\n",
      "Epoch 1200, Training Loss: 1.4502202247967944e-06, Validation Loss: 1.8107387996203859e-06\n",
      "Epoch 1201, Training Loss: 2.1833382106706267e-06, Validation Loss: 1.572091201280713e-06\n",
      "Epoch 1202, Training Loss: 2.489647386028082e-06, Validation Loss: 3.78290025191224e-06\n",
      "Epoch 1203, Training Loss: 1.6845755226313486e-06, Validation Loss: 1.7897436672955667e-06\n",
      "Epoch 1204, Training Loss: 4.504493517742958e-06, Validation Loss: 3.235805329669422e-06\n",
      "Epoch 1205, Training Loss: 1.3729959391639568e-06, Validation Loss: 1.7208332867090912e-06\n",
      "Epoch 1206, Training Loss: 2.19326875594561e-06, Validation Loss: 2.8305194708626666e-06\n",
      "Epoch 1207, Training Loss: 1.5451394119736506e-06, Validation Loss: 1.7733265753944996e-06\n",
      "Epoch 1208, Training Loss: 1.3104175877742819e-06, Validation Loss: 1.5246122896193121e-06\n",
      "Epoch 1209, Training Loss: 1.2791615517926402e-06, Validation Loss: 1.5986890016363899e-06\n",
      "Epoch 1210, Training Loss: 2.053400066870381e-06, Validation Loss: 1.7302892129704861e-06\n",
      "Epoch 1211, Training Loss: 1.1627323601715034e-06, Validation Loss: 1.5335666711895784e-06\n",
      "Epoch 1212, Training Loss: 1.5126495327422163e-06, Validation Loss: 1.5884483157716222e-06\n",
      "Epoch 1213, Training Loss: 1.0836699857463827e-06, Validation Loss: 1.587321579281243e-06\n",
      "Epoch 1214, Training Loss: 1.7942019212568994e-06, Validation Loss: 2.1407051501407705e-06\n",
      "Epoch 1215, Training Loss: 1.2132026085964753e-06, Validation Loss: 1.5577835313613913e-06\n",
      "Epoch 1216, Training Loss: 1.738792434480274e-06, Validation Loss: 1.5734914755438504e-06\n",
      "Epoch 1217, Training Loss: 1.2968844202987384e-06, Validation Loss: 1.558469794354038e-06\n",
      "Epoch 1218, Training Loss: 7.589152119180653e-06, Validation Loss: 7.213950171300349e-06\n",
      "Epoch 1219, Training Loss: 1.8839649555957294e-06, Validation Loss: 1.7126441127784695e-06\n",
      "Epoch 1220, Training Loss: 5.388743375078775e-06, Validation Loss: 3.3273281645683045e-06\n",
      "Epoch 1221, Training Loss: 2.808260887832148e-06, Validation Loss: 1.7460751481339222e-06\n",
      "Epoch 1222, Training Loss: 2.6328011699661147e-06, Validation Loss: 2.0575545995656005e-06\n",
      "Epoch 1223, Training Loss: 3.0779287953919265e-06, Validation Loss: 3.6534150929548363e-06\n",
      "Epoch 1224, Training Loss: 4.7256289690267295e-06, Validation Loss: 3.811010594093604e-06\n",
      "Epoch 1225, Training Loss: 2.014341362155392e-06, Validation Loss: 2.086089960351847e-06\n",
      "Epoch 1226, Training Loss: 2.005562237172853e-06, Validation Loss: 1.5435284640522353e-06\n",
      "Epoch 1227, Training Loss: 2.7403782496548956e-06, Validation Loss: 2.298777694646838e-06\n",
      "Epoch 1228, Training Loss: 2.4718210624996573e-06, Validation Loss: 2.3013086417165956e-06\n",
      "Epoch 1229, Training Loss: 1.8844816622731742e-06, Validation Loss: 2.2330829212831793e-06\n",
      "Epoch 1230, Training Loss: 2.8151530386821833e-06, Validation Loss: 2.037819901262621e-06\n",
      "Epoch 1231, Training Loss: 1.9114154383714776e-06, Validation Loss: 2.2332458967959527e-06\n",
      "Epoch 1232, Training Loss: 1.3858383454135037e-06, Validation Loss: 1.5417420039287375e-06\n",
      "Epoch 1233, Training Loss: 1.2453417639335385e-06, Validation Loss: 1.5724088690977468e-06\n",
      "Epoch 1234, Training Loss: 2.204303655162221e-06, Validation Loss: 1.7535705235172681e-06\n",
      "Epoch 1235, Training Loss: 1.164025434263749e-05, Validation Loss: 8.651545902895429e-06\n",
      "Epoch 1236, Training Loss: 2.2286392322712345e-06, Validation Loss: 1.5475705839988476e-06\n",
      "Epoch 1237, Training Loss: 1.544969336464419e-06, Validation Loss: 1.5046464833541245e-06\n",
      "Epoch 1238, Training Loss: 2.2249446374189574e-06, Validation Loss: 2.168101889817376e-06\n",
      "Epoch 1239, Training Loss: 1.414090888829378e-06, Validation Loss: 1.621080177553331e-06\n",
      "Epoch 1240, Training Loss: 1.456049972148321e-06, Validation Loss: 1.5906314415819704e-06\n",
      "Epoch 1241, Training Loss: 1.772704081304255e-06, Validation Loss: 1.6176580610862733e-06\n",
      "Epoch 1242, Training Loss: 4.182973952993052e-06, Validation Loss: 6.885714185138487e-06\n",
      "Epoch 1243, Training Loss: 1.114813017011329e-06, Validation Loss: 1.580416590145925e-06\n",
      "Epoch 1244, Training Loss: 2.18239847527002e-06, Validation Loss: 1.6043919324066182e-06\n",
      "Epoch 1245, Training Loss: 1.8265543531015282e-06, Validation Loss: 1.5512478785384916e-06\n",
      "Epoch 1246, Training Loss: 3.000791139129433e-06, Validation Loss: 1.7136627262070355e-06\n",
      "Epoch 1247, Training Loss: 2.130934944943874e-06, Validation Loss: 1.7561257995702983e-06\n",
      "Epoch 1248, Training Loss: 9.361347110825591e-07, Validation Loss: 1.5147123163280564e-06\n",
      "Epoch 1249, Training Loss: 9.95189566310728e-07, Validation Loss: 1.5270331881922151e-06\n",
      "Epoch 1250, Training Loss: 1.3344900935408077e-06, Validation Loss: 1.5980915316846037e-06\n",
      "Epoch 1251, Training Loss: 1.6886913272173842e-06, Validation Loss: 1.9877422518666613e-06\n",
      "Epoch 1252, Training Loss: 1.1317202961436124e-06, Validation Loss: 1.7407386550238101e-06\n",
      "Epoch 1253, Training Loss: 1.6552565966776456e-06, Validation Loss: 1.693132267831924e-06\n",
      "Epoch 1254, Training Loss: 1.3952467270428315e-06, Validation Loss: 1.5601743474556772e-06\n",
      "Epoch 1255, Training Loss: 2.2684671421302482e-06, Validation Loss: 1.7680228385760511e-06\n",
      "Epoch 1256, Training Loss: 1.823033812797803e-06, Validation Loss: 2.613268914611598e-06\n",
      "Epoch 1257, Training Loss: 1.0468952496012207e-06, Validation Loss: 1.5143798856958484e-06\n",
      "Epoch 1258, Training Loss: 1.3892124570702435e-06, Validation Loss: 1.5650984133212746e-06\n",
      "Epoch 1259, Training Loss: 2.3046259229886346e-06, Validation Loss: 1.7156318550104325e-06\n",
      "Epoch 1260, Training Loss: 1.6019764643715462e-06, Validation Loss: 1.5172795363781868e-06\n",
      "Epoch 1261, Training Loss: 3.965175437770085e-06, Validation Loss: 4.221254330100439e-06\n",
      "Epoch 1262, Training Loss: 2.2159279069455806e-06, Validation Loss: 1.533834247644556e-06\n",
      "Epoch 1263, Training Loss: 1.8795593632603413e-06, Validation Loss: 1.5820424639905411e-06\n",
      "Epoch 1264, Training Loss: 5.873525424249237e-06, Validation Loss: 1.2820823841793522e-05\n",
      "Epoch 1265, Training Loss: 1.5866644389461726e-06, Validation Loss: 1.811211674228929e-06\n",
      "Epoch 1266, Training Loss: 2.400689254500321e-06, Validation Loss: 2.5403032184400146e-06\n",
      "Epoch 1267, Training Loss: 1.5380862805614015e-06, Validation Loss: 1.5256823691517268e-06\n",
      "Epoch 1268, Training Loss: 1.3767028121947078e-06, Validation Loss: 2.886470181571205e-06\n",
      "Epoch 1269, Training Loss: 1.108381866288255e-06, Validation Loss: 1.5774428237917418e-06\n",
      "Epoch 1270, Training Loss: 9.902735200739698e-07, Validation Loss: 1.4921055007379767e-06\n",
      "Epoch 1271, Training Loss: 1.310411789745558e-06, Validation Loss: 1.6504638858153557e-06\n",
      "Epoch 1272, Training Loss: 2.7620617402135395e-06, Validation Loss: 3.0173571730979548e-06\n",
      "Epoch 1273, Training Loss: 1.2493577514760545e-06, Validation Loss: 1.7309005885604136e-06\n",
      "Epoch 1274, Training Loss: 1.2911281146443798e-06, Validation Loss: 1.6561142577846772e-06\n",
      "Epoch 1275, Training Loss: 1.5163748230406782e-06, Validation Loss: 1.6855502757503337e-06\n",
      "Epoch 1276, Training Loss: 1.838222715377924e-06, Validation Loss: 1.5114790264572542e-06\n",
      "Epoch 1277, Training Loss: 3.8575562939513475e-06, Validation Loss: 1.6732550638019804e-06\n",
      "Epoch 1278, Training Loss: 2.1092137103551067e-06, Validation Loss: 2.2082768146281953e-06\n",
      "Epoch 1279, Training Loss: 3.812472414210788e-06, Validation Loss: 2.4020356187546588e-06\n",
      "Epoch 1280, Training Loss: 1.7948181039173505e-06, Validation Loss: 1.9989503710835406e-06\n",
      "Epoch 1281, Training Loss: 2.267751824547304e-06, Validation Loss: 3.7125049000064646e-06\n",
      "Epoch 1282, Training Loss: 2.4911987566156313e-06, Validation Loss: 1.5365904220981199e-06\n",
      "Epoch 1283, Training Loss: 4.718997843156103e-06, Validation Loss: 5.163897236100169e-06\n",
      "Epoch 1284, Training Loss: 1.7216337937497883e-06, Validation Loss: 1.951930150692527e-06\n",
      "Epoch 1285, Training Loss: 1.623960542929126e-06, Validation Loss: 1.7865137673924768e-06\n",
      "Epoch 1286, Training Loss: 1.1835139730465016e-06, Validation Loss: 1.5584864422345643e-06\n",
      "Epoch 1287, Training Loss: 1.3648577805724926e-06, Validation Loss: 1.526642952328776e-06\n",
      "Epoch 1288, Training Loss: 1.690962449174549e-06, Validation Loss: 1.5907966082818202e-06\n",
      "Epoch 1289, Training Loss: 1.4113854831521166e-06, Validation Loss: 1.4930200106947691e-06\n",
      "Epoch 1290, Training Loss: 2.001484972424805e-06, Validation Loss: 1.5362021214453417e-06\n",
      "Epoch 1291, Training Loss: 1.2363211681076791e-06, Validation Loss: 1.5131810281221245e-06\n",
      "Epoch 1292, Training Loss: 1.070740950126492e-06, Validation Loss: 1.4897992531685558e-06\n",
      "Epoch 1293, Training Loss: 1.842007236518839e-06, Validation Loss: 1.6471405892785974e-06\n",
      "Epoch 1294, Training Loss: 1.0010985533881467e-06, Validation Loss: 1.5183268551901877e-06\n",
      "Epoch 1295, Training Loss: 2.3709985725872684e-06, Validation Loss: 2.1212772951472218e-06\n",
      "Epoch 1296, Training Loss: 2.136578359568375e-06, Validation Loss: 1.5479058776204481e-06\n",
      "Epoch 1297, Training Loss: 2.536208285164321e-06, Validation Loss: 1.5951145784783643e-06\n",
      "Epoch 1298, Training Loss: 8.60262389323907e-07, Validation Loss: 1.5770139974019158e-06\n",
      "Epoch 1299, Training Loss: 5.259632416709792e-06, Validation Loss: 5.124019424236475e-06\n",
      "Epoch 1300, Training Loss: 2.2173878733156016e-06, Validation Loss: 1.5760067113822828e-06\n",
      "Epoch 1301, Training Loss: 1.5830595430088579e-06, Validation Loss: 2.4313284206316453e-06\n",
      "Epoch 1302, Training Loss: 9.826588893702137e-07, Validation Loss: 1.858074144719044e-06\n",
      "Epoch 1303, Training Loss: 1.3892530432713102e-06, Validation Loss: 1.6073810946378726e-06\n",
      "Epoch 1304, Training Loss: 1.7599949160285178e-06, Validation Loss: 1.529339490794755e-06\n",
      "Epoch 1305, Training Loss: 1.4573297448805533e-06, Validation Loss: 1.5812640958322201e-06\n",
      "Epoch 1306, Training Loss: 1.8958997998197447e-06, Validation Loss: 1.6978007848755027e-06\n",
      "Epoch 1307, Training Loss: 2.5098297555814497e-06, Validation Loss: 2.0160049344105723e-06\n",
      "Epoch 1308, Training Loss: 1.5123818002393818e-06, Validation Loss: 1.5105950848635892e-06\n",
      "Epoch 1309, Training Loss: 1.1541513913471135e-06, Validation Loss: 1.5253068426996127e-06\n",
      "Epoch 1310, Training Loss: 1.856092694652034e-06, Validation Loss: 1.5522186366875167e-06\n",
      "Epoch 1311, Training Loss: 2.3031961973174475e-06, Validation Loss: 1.9909537218299405e-06\n",
      "Epoch 1312, Training Loss: 2.2914896362635773e-06, Validation Loss: 1.5145039631122718e-06\n",
      "Epoch 1313, Training Loss: 9.472909709984378e-07, Validation Loss: 1.5342528270124672e-06\n",
      "Epoch 1314, Training Loss: 1.969222466868814e-06, Validation Loss: 1.8879877954856682e-06\n",
      "Epoch 1315, Training Loss: 1.219907971972134e-06, Validation Loss: 1.5325258030194384e-06\n",
      "Epoch 1316, Training Loss: 1.5476398402824998e-06, Validation Loss: 1.5295423519225094e-06\n",
      "Epoch 1317, Training Loss: 2.9756836283922894e-06, Validation Loss: 2.314853592796663e-06\n",
      "Epoch 1318, Training Loss: 2.0853340174653567e-06, Validation Loss: 1.9311112326041167e-06\n",
      "Epoch 1319, Training Loss: 4.429491127666552e-06, Validation Loss: 2.855700913526208e-06\n",
      "Epoch 1320, Training Loss: 1.2234602309035836e-06, Validation Loss: 1.5297817995745906e-06\n",
      "Epoch 1321, Training Loss: 1.8152061329601565e-06, Validation Loss: 1.5520591633810527e-06\n",
      "Epoch 1322, Training Loss: 1.5888603002167656e-06, Validation Loss: 1.6123875628233986e-06\n",
      "Epoch 1323, Training Loss: 1.857475012911891e-06, Validation Loss: 2.70253787753961e-06\n",
      "Epoch 1324, Training Loss: 1.8216458101960598e-06, Validation Loss: 3.039956547269004e-06\n",
      "Epoch 1325, Training Loss: 1.9789270027104067e-06, Validation Loss: 1.6142890714922507e-06\n",
      "Epoch 1326, Training Loss: 4.0364498090639245e-06, Validation Loss: 3.8142760696495213e-06\n",
      "Epoch 1327, Training Loss: 1.4903702094670734e-06, Validation Loss: 1.6124473592035602e-06\n",
      "Epoch 1328, Training Loss: 1.9383467133593513e-06, Validation Loss: 2.100711529405738e-06\n",
      "Epoch 1329, Training Loss: 7.71342729422031e-06, Validation Loss: 4.133759805817986e-06\n",
      "Epoch 1330, Training Loss: 3.341390538480482e-06, Validation Loss: 1.5747574045502e-06\n",
      "Epoch 1331, Training Loss: 1.0583958101051394e-06, Validation Loss: 1.484814538633574e-06\n",
      "Epoch 1332, Training Loss: 1.3023844758208725e-06, Validation Loss: 2.118968015687422e-06\n",
      "Epoch 1333, Training Loss: 2.617554855532944e-06, Validation Loss: 1.9038120308033766e-06\n",
      "Epoch 1334, Training Loss: 9.834798220254015e-07, Validation Loss: 1.5239620345791237e-06\n",
      "Epoch 1335, Training Loss: 1.8967468804476084e-06, Validation Loss: 1.5303845787741104e-06\n",
      "Epoch 1336, Training Loss: 1.0276233979311655e-06, Validation Loss: 1.5227936188285598e-06\n",
      "Epoch 1337, Training Loss: 4.121626261621714e-06, Validation Loss: 1.779749518941658e-06\n",
      "Epoch 1338, Training Loss: 1.3944621741757146e-06, Validation Loss: 1.5048246719036734e-06\n",
      "Epoch 1339, Training Loss: 2.133578846041928e-06, Validation Loss: 2.0682620205876143e-06\n",
      "Epoch 1340, Training Loss: 1.5450954151674523e-06, Validation Loss: 1.7235239786303073e-06\n",
      "Epoch 1341, Training Loss: 1.232158183483989e-06, Validation Loss: 1.517358020846934e-06\n",
      "Epoch 1342, Training Loss: 2.6169097964157118e-06, Validation Loss: 5.225880609234928e-06\n",
      "Epoch 1343, Training Loss: 2.1051541807537433e-06, Validation Loss: 2.099945941241041e-06\n",
      "Epoch 1344, Training Loss: 1.3718581612920389e-06, Validation Loss: 1.6191298769557368e-06\n",
      "Epoch 1345, Training Loss: 1.075806494554854e-06, Validation Loss: 1.5172021065901987e-06\n",
      "Epoch 1346, Training Loss: 2.8529661904030945e-06, Validation Loss: 1.9143661139875634e-06\n",
      "Epoch 1347, Training Loss: 3.5594473501987522e-06, Validation Loss: 3.032782651470295e-06\n",
      "Epoch 1348, Training Loss: 1.5196744698187103e-06, Validation Loss: 1.5159834206199421e-06\n",
      "Epoch 1349, Training Loss: 1.3544877219828777e-06, Validation Loss: 1.5101532518305839e-06\n",
      "Epoch 1350, Training Loss: 2.270844106533332e-06, Validation Loss: 1.6918555541445703e-06\n",
      "Epoch 1351, Training Loss: 1.1576862561923917e-06, Validation Loss: 1.579390976287399e-06\n",
      "Epoch 1352, Training Loss: 1.1140426749989274e-06, Validation Loss: 1.5240876302941266e-06\n",
      "Epoch 1353, Training Loss: 1.5866888816162827e-06, Validation Loss: 1.529962161570276e-06\n",
      "Epoch 1354, Training Loss: 1.3405054914983339e-06, Validation Loss: 1.7128939978960214e-06\n",
      "Epoch 1355, Training Loss: 6.843011760793161e-06, Validation Loss: 2.828021838381081e-06\n",
      "Epoch 1356, Training Loss: 1.6851108739501797e-06, Validation Loss: 1.5989003798634136e-06\n",
      "Epoch 1357, Training Loss: 8.353274552064249e-07, Validation Loss: 1.5650561475239016e-06\n",
      "Epoch 1358, Training Loss: 2.5845199616014725e-06, Validation Loss: 1.7106596536308596e-06\n",
      "Epoch 1359, Training Loss: 5.0446205932530575e-06, Validation Loss: 5.323996083751327e-06\n",
      "Epoch 1360, Training Loss: 3.832920810964424e-06, Validation Loss: 1.845142971496956e-06\n",
      "Epoch 1361, Training Loss: 2.339746970392298e-06, Validation Loss: 1.9317873640719866e-06\n",
      "Epoch 1362, Training Loss: 1.2688578863162547e-06, Validation Loss: 1.5331478296902325e-06\n",
      "Epoch 1363, Training Loss: 1.6391072676924523e-06, Validation Loss: 1.8019173639450351e-06\n",
      "Epoch 1364, Training Loss: 2.385412244620966e-06, Validation Loss: 2.203745528097486e-06\n",
      "Epoch 1365, Training Loss: 2.420646524115e-06, Validation Loss: 2.873720150584087e-06\n",
      "Epoch 1366, Training Loss: 1.2400980722304666e-06, Validation Loss: 1.5272302317400064e-06\n",
      "Epoch 1367, Training Loss: 8.928676606956287e-07, Validation Loss: 1.507509198124243e-06\n",
      "Epoch 1368, Training Loss: 1.8977082163473824e-06, Validation Loss: 1.6905243312391293e-06\n",
      "Epoch 1369, Training Loss: 2.421868430246832e-06, Validation Loss: 1.5264680288362415e-06\n",
      "Epoch 1370, Training Loss: 3.285174898337573e-06, Validation Loss: 3.2634057530740857e-06\n",
      "Epoch 1371, Training Loss: 1.6245847973550553e-06, Validation Loss: 1.5119491454271925e-06\n",
      "Epoch 1372, Training Loss: 1.0630236602082732e-06, Validation Loss: 1.4996818260375904e-06\n",
      "Epoch 1373, Training Loss: 1.283165829590871e-06, Validation Loss: 1.6382779696411322e-06\n",
      "Epoch 1374, Training Loss: 1.641026756260544e-06, Validation Loss: 1.5182198825740015e-06\n",
      "Epoch 1375, Training Loss: 1.5260742429745733e-06, Validation Loss: 2.3092791462560598e-06\n",
      "Epoch 1376, Training Loss: 1.6817855339468224e-06, Validation Loss: 3.2751467177925862e-06\n",
      "Epoch 1377, Training Loss: 1.1859256119350903e-06, Validation Loss: 1.7156321352448668e-06\n",
      "Epoch 1378, Training Loss: 1.9952296952396864e-06, Validation Loss: 2.2133086824083923e-06\n",
      "Epoch 1379, Training Loss: 1.5755945241835434e-06, Validation Loss: 1.5374607042544386e-06\n",
      "Epoch 1380, Training Loss: 3.1543593195237918e-06, Validation Loss: 2.300826666006015e-06\n",
      "Epoch 1381, Training Loss: 1.5746744566058624e-06, Validation Loss: 1.5832269081369838e-06\n",
      "Epoch 1382, Training Loss: 3.237445980630582e-06, Validation Loss: 2.5126814921494982e-06\n",
      "Epoch 1383, Training Loss: 8.956941428550635e-07, Validation Loss: 1.6194944246626908e-06\n",
      "Epoch 1384, Training Loss: 1.626392418074829e-06, Validation Loss: 1.9732574097001903e-06\n",
      "Epoch 1385, Training Loss: 1.7495343627160764e-06, Validation Loss: 1.5141362421305113e-06\n",
      "Epoch 1386, Training Loss: 1.717356099106837e-06, Validation Loss: 1.539016203412085e-06\n",
      "Epoch 1387, Training Loss: 1.7409070096618962e-06, Validation Loss: 2.159071450304434e-06\n",
      "Epoch 1388, Training Loss: 1.0278480431225034e-06, Validation Loss: 1.6085905173032147e-06\n",
      "Epoch 1389, Training Loss: 4.169509338680655e-06, Validation Loss: 7.867572363628228e-06\n",
      "Epoch 1390, Training Loss: 1.4491608908429043e-06, Validation Loss: 1.5000894822365507e-06\n",
      "Epoch 1391, Training Loss: 8.59149963616801e-07, Validation Loss: 1.603178464807037e-06\n",
      "Epoch 1392, Training Loss: 1.5059947145346086e-05, Validation Loss: 1.1670058061128518e-05\n",
      "Epoch 1393, Training Loss: 1.6819852817206993e-06, Validation Loss: 1.57813400935205e-06\n",
      "Epoch 1394, Training Loss: 1.1897986951225903e-06, Validation Loss: 1.489165195606263e-06\n",
      "Epoch 1395, Training Loss: 1.4006818673806265e-06, Validation Loss: 1.5029445037749136e-06\n",
      "Epoch 1396, Training Loss: 1.7792237940739142e-06, Validation Loss: 1.6149306070416337e-06\n",
      "Epoch 1397, Training Loss: 1.7599759303266183e-06, Validation Loss: 1.5526144305339046e-06\n",
      "Epoch 1398, Training Loss: 1.7207178188982653e-06, Validation Loss: 1.628996394817483e-06\n",
      "Epoch 1399, Training Loss: 2.205527835030807e-06, Validation Loss: 2.9553286380479414e-06\n",
      "Epoch 1400, Training Loss: 1.264520733457175e-06, Validation Loss: 1.7959959335897859e-06\n",
      "Epoch 1401, Training Loss: 1.4417400961974636e-06, Validation Loss: 1.4923223257955313e-06\n",
      "Epoch 1402, Training Loss: 1.6574631445109844e-06, Validation Loss: 1.5137970945613182e-06\n",
      "Epoch 1403, Training Loss: 2.2709214135829825e-06, Validation Loss: 1.5444527869210111e-06\n",
      "Epoch 1404, Training Loss: 9.192780794364808e-07, Validation Loss: 1.673747037437201e-06\n",
      "Epoch 1405, Training Loss: 1.3640078577736858e-06, Validation Loss: 1.691978865985139e-06\n",
      "Epoch 1406, Training Loss: 1.970307494048029e-06, Validation Loss: 1.5044334305996342e-06\n",
      "Epoch 1407, Training Loss: 1.566457967783208e-06, Validation Loss: 1.6250157942945377e-06\n",
      "Epoch 1408, Training Loss: 2.810279511322733e-06, Validation Loss: 1.5778934349692713e-06\n",
      "Epoch 1409, Training Loss: 1.0917323152170866e-06, Validation Loss: 1.5677793911728855e-06\n",
      "Epoch 1410, Training Loss: 7.981866474437993e-07, Validation Loss: 1.4752108225606508e-06\n",
      "Epoch 1411, Training Loss: 2.1064743123133667e-06, Validation Loss: 1.680618933927122e-06\n",
      "Epoch 1412, Training Loss: 8.80735842656577e-06, Validation Loss: 1.5672862842898877e-06\n",
      "Epoch 1413, Training Loss: 1.938731884365552e-06, Validation Loss: 1.7718620224484906e-06\n",
      "Epoch 1414, Training Loss: 1.425268237653654e-06, Validation Loss: 1.6222880212750447e-06\n",
      "Epoch 1415, Training Loss: 3.2507991818420123e-06, Validation Loss: 3.292506691395597e-06\n",
      "Epoch 1416, Training Loss: 1.6260249822153128e-06, Validation Loss: 1.8506375707749994e-06\n",
      "Epoch 1417, Training Loss: 1.6661513200233458e-06, Validation Loss: 1.5840057361114954e-06\n",
      "Epoch 1418, Training Loss: 1.4355258599607623e-06, Validation Loss: 1.5089389323038399e-06\n",
      "Epoch 1419, Training Loss: 2.163593308068812e-06, Validation Loss: 2.4185031774739943e-06\n",
      "Epoch 1420, Training Loss: 1.7726388250594027e-06, Validation Loss: 1.4898246350222871e-06\n",
      "Epoch 1421, Training Loss: 2.0234917883499293e-06, Validation Loss: 1.6048233131296929e-06\n",
      "Epoch 1422, Training Loss: 1.039675339598034e-06, Validation Loss: 1.5098640444633998e-06\n",
      "Epoch 1423, Training Loss: 1.7217556660398259e-06, Validation Loss: 1.5412000739798383e-06\n",
      "Epoch 1424, Training Loss: 1.1031100257241633e-05, Validation Loss: 6.600509231867844e-06\n",
      "Epoch 1425, Training Loss: 1.3967937775305472e-06, Validation Loss: 1.5292848838204692e-06\n",
      "Epoch 1426, Training Loss: 1.6016781501093647e-06, Validation Loss: 1.5764701764136745e-06\n",
      "Epoch 1427, Training Loss: 2.07159791898448e-06, Validation Loss: 1.6224699589558187e-06\n",
      "Epoch 1428, Training Loss: 1.8852542780223303e-06, Validation Loss: 1.6241749353055892e-06\n",
      "Epoch 1429, Training Loss: 2.315511210326804e-06, Validation Loss: 2.1508476109398067e-06\n",
      "Epoch 1430, Training Loss: 1.54261010720802e-06, Validation Loss: 1.6920342481298692e-06\n",
      "Epoch 1431, Training Loss: 1.5814201788089122e-06, Validation Loss: 1.5081258196014054e-06\n",
      "Epoch 1432, Training Loss: 9.98991140477301e-07, Validation Loss: 1.5586717749518902e-06\n",
      "Epoch 1433, Training Loss: 4.771445219375892e-06, Validation Loss: 2.5905011948539633e-06\n",
      "Epoch 1434, Training Loss: 1.424066795152612e-06, Validation Loss: 1.517088377210428e-06\n",
      "Epoch 1435, Training Loss: 1.9550795968825696e-06, Validation Loss: 1.5323671769333573e-06\n",
      "Epoch 1436, Training Loss: 6.174382178869564e-06, Validation Loss: 7.22178017918375e-06\n",
      "Epoch 1437, Training Loss: 4.423630798555678e-06, Validation Loss: 1.9021085551922758e-06\n",
      "Epoch 1438, Training Loss: 4.032351171190385e-06, Validation Loss: 3.2866302202020067e-06\n",
      "Epoch 1439, Training Loss: 2.2278568394540343e-06, Validation Loss: 1.71124095289719e-06\n",
      "Epoch 1440, Training Loss: 1.3471920965457684e-06, Validation Loss: 1.5386817095810085e-06\n",
      "Epoch 1441, Training Loss: 2.6473981051822193e-06, Validation Loss: 2.6073600318074572e-06\n",
      "Epoch 1442, Training Loss: 1.5454600088560255e-06, Validation Loss: 1.5645939381165647e-06\n",
      "Epoch 1443, Training Loss: 4.332322987465886e-06, Validation Loss: 2.9876706324270994e-06\n",
      "Epoch 1444, Training Loss: 8.436190341853944e-07, Validation Loss: 1.4931770574751618e-06\n",
      "Epoch 1445, Training Loss: 1.8883922621171223e-06, Validation Loss: 1.6005326382739417e-06\n",
      "Epoch 1446, Training Loss: 1.9432313820288982e-06, Validation Loss: 1.544935749607132e-06\n",
      "Epoch 1447, Training Loss: 4.271526904631173e-06, Validation Loss: 1.732284985406689e-06\n",
      "Epoch 1448, Training Loss: 2.0770125956914853e-06, Validation Loss: 1.618201659407619e-06\n",
      "Epoch 1449, Training Loss: 3.5062416827713605e-06, Validation Loss: 2.1612364959249245e-06\n",
      "Epoch 1450, Training Loss: 1.5180542050075019e-06, Validation Loss: 2.1024964192750473e-06\n",
      "Epoch 1451, Training Loss: 4.940432063449407e-06, Validation Loss: 5.536052181217137e-06\n",
      "Epoch 1452, Training Loss: 1.5569710285490146e-06, Validation Loss: 1.6258848841873006e-06\n",
      "Epoch 1453, Training Loss: 1.77159859049425e-06, Validation Loss: 1.8181256728651065e-06\n",
      "Epoch 1454, Training Loss: 1.478611466154689e-06, Validation Loss: 1.7090604212466896e-06\n",
      "Epoch 1455, Training Loss: 2.759259132290026e-06, Validation Loss: 2.912169717601964e-06\n",
      "Epoch 1456, Training Loss: 1.273370799026452e-06, Validation Loss: 1.8872386484652417e-06\n",
      "Epoch 1457, Training Loss: 2.001469965762226e-06, Validation Loss: 1.5143285830620865e-06\n",
      "Epoch 1458, Training Loss: 2.6841694307222497e-06, Validation Loss: 1.6010207838488588e-06\n",
      "Epoch 1459, Training Loss: 1.2552740145110874e-06, Validation Loss: 1.6034464102011472e-06\n",
      "Epoch 1460, Training Loss: 2.937023964477703e-06, Validation Loss: 1.925648135079862e-06\n",
      "Epoch 1461, Training Loss: 2.1452715373015963e-06, Validation Loss: 1.760954793550641e-06\n",
      "Epoch 1462, Training Loss: 1.4156543102217256e-06, Validation Loss: 1.502246401782564e-06\n",
      "Epoch 1463, Training Loss: 1.5588500446028775e-06, Validation Loss: 1.7364058356607836e-06\n",
      "Epoch 1464, Training Loss: 2.5772169465199113e-06, Validation Loss: 2.1152787800471573e-06\n",
      "Epoch 1465, Training Loss: 2.015536665567197e-06, Validation Loss: 1.6961484346697973e-06\n",
      "Epoch 1466, Training Loss: 3.508246663841419e-06, Validation Loss: 1.518591405004656e-06\n",
      "Epoch 1467, Training Loss: 8.167817213688977e-07, Validation Loss: 1.489755087280341e-06\n",
      "Epoch 1468, Training Loss: 1.59064643412421e-06, Validation Loss: 1.5013131802032867e-06\n",
      "Epoch 1469, Training Loss: 1.218127863467089e-06, Validation Loss: 1.4981777460970235e-06\n",
      "Epoch 1470, Training Loss: 1.6940916793828364e-06, Validation Loss: 1.5265241445145169e-06\n",
      "Epoch 1471, Training Loss: 1.3402620879787719e-06, Validation Loss: 1.5865944860590944e-06\n",
      "Epoch 1472, Training Loss: 2.5071758500416763e-06, Validation Loss: 1.6462685641654737e-06\n",
      "Epoch 1473, Training Loss: 1.3770622899755836e-06, Validation Loss: 1.6095884502270553e-06\n",
      "Epoch 1474, Training Loss: 1.1858730886160629e-06, Validation Loss: 1.5234096668026938e-06\n",
      "Epoch 1475, Training Loss: 1.4246547834773082e-06, Validation Loss: 1.6880051018789768e-06\n",
      "Epoch 1476, Training Loss: 1.3601975297206081e-06, Validation Loss: 1.547571415650651e-06\n",
      "Epoch 1477, Training Loss: 1.2317464097577613e-06, Validation Loss: 1.5238996693833136e-06\n",
      "Epoch 1478, Training Loss: 1.1534100394783309e-06, Validation Loss: 2.0766637581296765e-06\n",
      "Epoch 1479, Training Loss: 2.3340601273957873e-06, Validation Loss: 2.2867822126152383e-06\n",
      "Epoch 1480, Training Loss: 1.8781308881443692e-06, Validation Loss: 2.0978339323406473e-06\n",
      "Epoch 1481, Training Loss: 2.469010723871179e-06, Validation Loss: 2.7231052696545813e-06\n",
      "Epoch 1482, Training Loss: 2.502988763808389e-06, Validation Loss: 1.5589775320813425e-06\n",
      "Epoch 1483, Training Loss: 9.9860199043178e-07, Validation Loss: 1.5074567117353349e-06\n",
      "Epoch 1484, Training Loss: 9.93152525552432e-07, Validation Loss: 1.547596669697205e-06\n",
      "Epoch 1485, Training Loss: 7.506804649892729e-06, Validation Loss: 8.505113499175503e-06\n",
      "Epoch 1486, Training Loss: 1.716817450869712e-06, Validation Loss: 1.5252741566475877e-06\n",
      "Epoch 1487, Training Loss: 1.735217097120767e-06, Validation Loss: 1.6975197571676747e-06\n",
      "Epoch 1488, Training Loss: 1.0678265880414983e-06, Validation Loss: 1.490992260033827e-06\n",
      "Epoch 1489, Training Loss: 1.2660009360843105e-06, Validation Loss: 1.5893761375881677e-06\n",
      "Epoch 1490, Training Loss: 1.0680610103008803e-06, Validation Loss: 1.5365903920471405e-06\n",
      "Epoch 1491, Training Loss: 2.3019249510980444e-06, Validation Loss: 1.5020572209106055e-06\n",
      "Epoch 1492, Training Loss: 4.616709247784456e-06, Validation Loss: 3.383790174950356e-06\n",
      "Epoch 1493, Training Loss: 1.9122762751067057e-06, Validation Loss: 1.7909946721890154e-06\n",
      "Epoch 1494, Training Loss: 1.8992986952071078e-06, Validation Loss: 1.8388340084181732e-06\n",
      "Epoch 1495, Training Loss: 1.0239099310638267e-06, Validation Loss: 1.4772286520536718e-06\n",
      "Epoch 1496, Training Loss: 1.2331271364018903e-06, Validation Loss: 1.6187591488847065e-06\n",
      "Epoch 1497, Training Loss: 2.3279458218894433e-06, Validation Loss: 2.5453881707322277e-06\n",
      "Epoch 1498, Training Loss: 1.6056191043389845e-06, Validation Loss: 1.6094824930081187e-06\n",
      "Epoch 1499, Training Loss: 1.3689653997062123e-06, Validation Loss: 1.5301255914683408e-06\n",
      "Epoch 1500, Training Loss: 6.091536306485068e-06, Validation Loss: 2.5350291013875128e-06\n",
      "Epoch 1501, Training Loss: 4.110801000933861e-06, Validation Loss: 2.7050055698321554e-06\n",
      "Epoch 1502, Training Loss: 3.5558464333007578e-06, Validation Loss: 2.8478574015291993e-06\n",
      "Epoch 1503, Training Loss: 6.350123840093147e-06, Validation Loss: 1.6036691520431033e-06\n",
      "Epoch 1504, Training Loss: 1.666499201746774e-06, Validation Loss: 1.6341960915246565e-06\n",
      "Epoch 1505, Training Loss: 1.1627996627794346e-06, Validation Loss: 1.4979549007059096e-06\n",
      "Epoch 1506, Training Loss: 1.696834942777059e-06, Validation Loss: 1.5955728432420804e-06\n",
      "Epoch 1507, Training Loss: 1.2795562724932097e-06, Validation Loss: 1.5224636938325319e-06\n",
      "Epoch 1508, Training Loss: 1.2958329307366512e-06, Validation Loss: 1.5276025615644444e-06\n",
      "Epoch 1509, Training Loss: 1.0976240218951716e-06, Validation Loss: 1.509053291849028e-06\n",
      "Epoch 1510, Training Loss: 1.4665517937828554e-06, Validation Loss: 1.5050557418495826e-06\n",
      "Epoch 1511, Training Loss: 2.0775344182766275e-06, Validation Loss: 1.792654004721681e-06\n",
      "Epoch 1512, Training Loss: 4.8274159780703485e-06, Validation Loss: 3.578858843339169e-06\n",
      "Epoch 1513, Training Loss: 1.9469059679977363e-06, Validation Loss: 1.4870813386806054e-06\n",
      "Epoch 1514, Training Loss: 1.512741960141284e-06, Validation Loss: 2.2373146603753715e-06\n",
      "Epoch 1515, Training Loss: 1.6598640968368272e-06, Validation Loss: 1.7106355574521626e-06\n",
      "Epoch 1516, Training Loss: 8.028918045965838e-07, Validation Loss: 1.5305656927684006e-06\n",
      "Epoch 1517, Training Loss: 1.525259222034947e-06, Validation Loss: 2.486721624846277e-06\n",
      "Epoch 1518, Training Loss: 1.988720214285422e-06, Validation Loss: 1.7739005092743975e-06\n",
      "Epoch 1519, Training Loss: 2.2088140667619882e-06, Validation Loss: 2.4517361384968566e-06\n",
      "Epoch 1520, Training Loss: 1.368710741189716e-06, Validation Loss: 2.0505710300156376e-06\n",
      "Epoch 1521, Training Loss: 1.2228726973262383e-06, Validation Loss: 1.5845030226150492e-06\n",
      "Epoch 1522, Training Loss: 2.4636758553242544e-06, Validation Loss: 1.6101837756975045e-06\n",
      "Epoch 1523, Training Loss: 2.3420977868227055e-06, Validation Loss: 1.6009113360095323e-06\n",
      "Epoch 1524, Training Loss: 2.4230512281064875e-06, Validation Loss: 3.019066880485651e-06\n",
      "Epoch 1525, Training Loss: 2.6502946184336906e-06, Validation Loss: 1.9835129776788996e-06\n",
      "Epoch 1526, Training Loss: 2.167369530070573e-06, Validation Loss: 1.943207389037299e-06\n",
      "Epoch 1527, Training Loss: 3.076709162996849e-06, Validation Loss: 3.164741594017562e-06\n",
      "Epoch 1528, Training Loss: 1.7892559753818205e-06, Validation Loss: 2.1780489443039085e-06\n",
      "Epoch 1529, Training Loss: 8.732117748877499e-07, Validation Loss: 1.5029769019892443e-06\n",
      "Epoch 1530, Training Loss: 3.1651934477849863e-06, Validation Loss: 2.2931162226422592e-06\n",
      "Epoch 1531, Training Loss: 1.9242252164985985e-06, Validation Loss: 3.109729408705134e-06\n",
      "Epoch 1532, Training Loss: 4.7822559281485155e-06, Validation Loss: 1.5846926823113483e-06\n",
      "Epoch 1533, Training Loss: 9.648382729210425e-06, Validation Loss: 7.1338374086996394e-06\n",
      "Epoch 1534, Training Loss: 1.4595611901313532e-06, Validation Loss: 1.66118824436589e-06\n",
      "Epoch 1535, Training Loss: 2.39139080804307e-06, Validation Loss: 2.9426569104995737e-06\n",
      "Epoch 1536, Training Loss: 2.458004018990323e-06, Validation Loss: 2.238246420318458e-06\n",
      "Epoch 1537, Training Loss: 2.0097936612728518e-06, Validation Loss: 1.5803947923240618e-06\n",
      "Epoch 1538, Training Loss: 2.4854400635376805e-06, Validation Loss: 2.6619575874644346e-06\n",
      "Epoch 1539, Training Loss: 1.3802957710140618e-06, Validation Loss: 1.5187633281060305e-06\n",
      "Epoch 1540, Training Loss: 8.619032996648457e-06, Validation Loss: 6.376158647297787e-06\n",
      "Epoch 1541, Training Loss: 1.131205067395058e-06, Validation Loss: 1.5823790041846965e-06\n",
      "Epoch 1542, Training Loss: 1.0888284123211633e-06, Validation Loss: 1.5510917068571013e-06\n",
      "Epoch 1543, Training Loss: 1.0678662647478632e-06, Validation Loss: 1.7013807750679765e-06\n",
      "Epoch 1544, Training Loss: 1.8612626035974245e-06, Validation Loss: 1.5977340064932687e-06\n",
      "Epoch 1545, Training Loss: 1.719184069770563e-06, Validation Loss: 2.0428101339539526e-06\n",
      "Epoch 1546, Training Loss: 3.262221753175254e-06, Validation Loss: 1.905948392220942e-06\n",
      "Epoch 1547, Training Loss: 1.4289207683759741e-06, Validation Loss: 1.5569188365148806e-06\n",
      "Epoch 1548, Training Loss: 1.3157549574316363e-06, Validation Loss: 1.7781307849200991e-06\n",
      "Epoch 1549, Training Loss: 1.7083466445910744e-06, Validation Loss: 1.6021862060874028e-06\n",
      "Epoch 1550, Training Loss: 1.77423021341383e-06, Validation Loss: 1.939811984994446e-06\n",
      "Epoch 1551, Training Loss: 1.0466269486641977e-06, Validation Loss: 1.562875532599603e-06\n",
      "Epoch 1552, Training Loss: 3.6435803849599324e-06, Validation Loss: 2.591790116118092e-06\n",
      "Epoch 1553, Training Loss: 1.1038493994419696e-06, Validation Loss: 1.6409074705270947e-06\n",
      "Epoch 1554, Training Loss: 1.9055090660913265e-06, Validation Loss: 1.5642107349064245e-06\n",
      "Epoch 1555, Training Loss: 1.5879435295573785e-06, Validation Loss: 1.5865435072879962e-06\n",
      "Epoch 1556, Training Loss: 1.2390619303914718e-06, Validation Loss: 1.8277038381833115e-06\n",
      "Epoch 1557, Training Loss: 3.807881057582563e-06, Validation Loss: 2.2797980688381263e-06\n",
      "Epoch 1558, Training Loss: 2.2190615709405392e-06, Validation Loss: 2.0465665603256107e-06\n",
      "Epoch 1559, Training Loss: 2.642315848788712e-06, Validation Loss: 1.5252945337460393e-06\n",
      "Epoch 1560, Training Loss: 6.039638719812501e-06, Validation Loss: 2.785436098730217e-06\n",
      "Epoch 1561, Training Loss: 2.027810069193947e-06, Validation Loss: 1.7028104145688854e-06\n",
      "Epoch 1562, Training Loss: 1.0157293672818923e-06, Validation Loss: 1.647086975434872e-06\n",
      "Epoch 1563, Training Loss: 1.9615877135947812e-06, Validation Loss: 2.100625360575705e-06\n",
      "Epoch 1564, Training Loss: 1.3264683502711705e-06, Validation Loss: 1.6979672662151863e-06\n",
      "Epoch 1565, Training Loss: 7.3551723289710935e-06, Validation Loss: 3.773837736780421e-06\n",
      "Epoch 1566, Training Loss: 1.35829782266228e-06, Validation Loss: 1.5701945572499235e-06\n",
      "Epoch 1567, Training Loss: 2.2513365820486797e-06, Validation Loss: 3.4640339076846013e-06\n",
      "Epoch 1568, Training Loss: 1.6335379768861458e-06, Validation Loss: 1.5729483721585145e-06\n",
      "Epoch 1569, Training Loss: 1.1055834647777374e-06, Validation Loss: 2.305452723188065e-06\n",
      "Epoch 1570, Training Loss: 5.344768851500703e-06, Validation Loss: 4.026545167009062e-06\n",
      "Epoch 1571, Training Loss: 8.481853001285344e-06, Validation Loss: 5.980424233781771e-06\n",
      "Epoch 1572, Training Loss: 9.807538390305126e-07, Validation Loss: 1.5320553708676063e-06\n",
      "Epoch 1573, Training Loss: 1.3921107893111184e-06, Validation Loss: 2.18013659598035e-06\n",
      "Epoch 1574, Training Loss: 1.0331987141398713e-06, Validation Loss: 1.5325693458024058e-06\n",
      "Epoch 1575, Training Loss: 8.196508133551106e-06, Validation Loss: 8.435988709919866e-06\n",
      "Epoch 1576, Training Loss: 1.6756306422394118e-06, Validation Loss: 1.7387662618875187e-06\n",
      "Epoch 1577, Training Loss: 1.1851942645080271e-06, Validation Loss: 2.4330936658498682e-06\n",
      "Epoch 1578, Training Loss: 4.567897121887654e-06, Validation Loss: 2.9051519283138467e-06\n",
      "Epoch 1579, Training Loss: 2.8424765332601964e-06, Validation Loss: 1.7053469351901713e-06\n",
      "Epoch 1580, Training Loss: 3.5164184737368487e-06, Validation Loss: 2.344424766671431e-06\n",
      "Epoch 1581, Training Loss: 1.488462430643267e-06, Validation Loss: 1.8740980789325303e-06\n",
      "Epoch 1582, Training Loss: 1.019307819660753e-05, Validation Loss: 1.1725257785551867e-05\n",
      "Epoch 1583, Training Loss: 1.368782136523805e-06, Validation Loss: 1.5483747416904517e-06\n",
      "Epoch 1584, Training Loss: 1.5130606243474176e-06, Validation Loss: 2.345555852950563e-06\n",
      "Epoch 1585, Training Loss: 2.098826371366158e-06, Validation Loss: 1.6116125422717707e-06\n",
      "Epoch 1586, Training Loss: 2.1947844288661145e-06, Validation Loss: 1.6008270086166223e-06\n",
      "Epoch 1587, Training Loss: 1.061638954524824e-06, Validation Loss: 1.537396214852651e-06\n",
      "Epoch 1588, Training Loss: 1.7747764786690823e-06, Validation Loss: 4.121867085101767e-06\n",
      "Epoch 1589, Training Loss: 2.255849040011526e-06, Validation Loss: 1.5948493680854395e-06\n",
      "Epoch 1590, Training Loss: 1.7843634623204707e-06, Validation Loss: 1.759198283391803e-06\n",
      "Epoch 1591, Training Loss: 2.0165189198451117e-06, Validation Loss: 1.5250036746611685e-06\n",
      "Epoch 1592, Training Loss: 1.1209028798475629e-06, Validation Loss: 1.4890839460853513e-06\n",
      "Epoch 1593, Training Loss: 1.5749662907182937e-06, Validation Loss: 2.134387058805451e-06\n",
      "Epoch 1594, Training Loss: 3.89419528801227e-06, Validation Loss: 1.5716835633657824e-06\n",
      "Epoch 1595, Training Loss: 2.3120810510590672e-06, Validation Loss: 1.8835015539248364e-06\n",
      "Epoch 1596, Training Loss: 1.045496446749894e-06, Validation Loss: 1.5576955641793742e-06\n",
      "Epoch 1597, Training Loss: 1.6923576140470686e-06, Validation Loss: 1.5269980404942997e-06\n",
      "Epoch 1598, Training Loss: 2.8213578389113536e-06, Validation Loss: 3.9037311705162824e-06\n",
      "Epoch 1599, Training Loss: 1.4991501302574761e-06, Validation Loss: 1.5821939162199258e-06\n",
      "Epoch 1600, Training Loss: 2.660582822500146e-06, Validation Loss: 4.2533594887283046e-06\n",
      "Epoch 1601, Training Loss: 1.6112328466988401e-06, Validation Loss: 3.0170601064945086e-06\n",
      "Epoch 1602, Training Loss: 1.289924966840772e-06, Validation Loss: 1.61511841226665e-06\n",
      "Epoch 1603, Training Loss: 1.4479692254099064e-06, Validation Loss: 1.5395006469235764e-06\n",
      "Epoch 1604, Training Loss: 3.331341531520593e-06, Validation Loss: 4.1517239684821296e-06\n",
      "Epoch 1605, Training Loss: 5.650676030199975e-06, Validation Loss: 1.3571907628290904e-05\n",
      "Epoch 1606, Training Loss: 2.4876453608158045e-06, Validation Loss: 1.5233396997952758e-06\n",
      "Epoch 1607, Training Loss: 1.2359764696157072e-06, Validation Loss: 1.702888073540849e-06\n",
      "Epoch 1608, Training Loss: 1.967669049918186e-06, Validation Loss: 2.4118433735554626e-06\n",
      "Epoch 1609, Training Loss: 1.234571300301468e-06, Validation Loss: 1.6789113590727834e-06\n",
      "Epoch 1610, Training Loss: 4.231035745760892e-06, Validation Loss: 8.062116553359791e-06\n",
      "Epoch 1611, Training Loss: 1.3837923233950278e-06, Validation Loss: 1.5210788878584646e-06\n",
      "Epoch 1612, Training Loss: 2.015789050346939e-06, Validation Loss: 2.037604820972172e-06\n",
      "Epoch 1613, Training Loss: 1.6842038803588366e-06, Validation Loss: 1.6037420387023026e-06\n",
      "Epoch 1614, Training Loss: 1.770861445038463e-06, Validation Loss: 3.3019697361185095e-06\n",
      "Epoch 1615, Training Loss: 1.5477248780371156e-06, Validation Loss: 1.4810364799014696e-06\n",
      "Epoch 1616, Training Loss: 2.470258777975687e-06, Validation Loss: 3.402467666763963e-06\n",
      "Epoch 1617, Training Loss: 1.7714064597385004e-06, Validation Loss: 1.517184770795686e-06\n",
      "Epoch 1618, Training Loss: 1.1756719686673023e-06, Validation Loss: 1.4852425751895257e-06\n",
      "Epoch 1619, Training Loss: 2.1174546418478712e-06, Validation Loss: 1.947937814356977e-06\n",
      "Epoch 1620, Training Loss: 2.258532504129107e-06, Validation Loss: 4.802065516388634e-06\n",
      "Epoch 1621, Training Loss: 1.6638530269119656e-06, Validation Loss: 1.5448090032654722e-06\n",
      "Epoch 1622, Training Loss: 2.4777816634014016e-06, Validation Loss: 1.8969562503278525e-06\n",
      "Epoch 1623, Training Loss: 2.567271394582349e-06, Validation Loss: 1.577499598057144e-06\n",
      "Epoch 1624, Training Loss: 1.9574445104808547e-06, Validation Loss: 1.6471373908406215e-06\n",
      "Epoch 1625, Training Loss: 5.442175279313233e-06, Validation Loss: 4.730796800471882e-06\n",
      "Epoch 1626, Training Loss: 1.1755012110370444e-06, Validation Loss: 1.5848440791455541e-06\n",
      "Epoch 1627, Training Loss: 1.8852938410418574e-06, Validation Loss: 1.6780101580792641e-06\n",
      "Epoch 1628, Training Loss: 2.092935574182775e-06, Validation Loss: 1.5996206482546988e-06\n",
      "Epoch 1629, Training Loss: 4.462626748136245e-06, Validation Loss: 1.5823167421759843e-06\n",
      "Epoch 1630, Training Loss: 1.786364009603858e-06, Validation Loss: 2.1562658510412606e-06\n",
      "Epoch 1631, Training Loss: 9.798263818083797e-07, Validation Loss: 1.5229132090544632e-06\n",
      "Epoch 1632, Training Loss: 5.559726105275331e-06, Validation Loss: 1.8877029096665576e-06\n",
      "Epoch 1633, Training Loss: 1.5535619013462565e-06, Validation Loss: 1.6047227752247278e-06\n",
      "Epoch 1634, Training Loss: 1.8498697045288282e-06, Validation Loss: 1.7479555472569572e-06\n",
      "Epoch 1635, Training Loss: 2.9991410883667413e-06, Validation Loss: 4.428880047155518e-06\n",
      "Epoch 1636, Training Loss: 1.1629451819317183e-06, Validation Loss: 1.6047582603625566e-06\n",
      "Epoch 1637, Training Loss: 1.0737292086560046e-06, Validation Loss: 1.501496934339044e-06\n",
      "Epoch 1638, Training Loss: 1.3029894034843892e-06, Validation Loss: 1.4869106322818364e-06\n",
      "Epoch 1639, Training Loss: 1.8167399957746966e-06, Validation Loss: 1.4788008944946526e-06\n",
      "Epoch 1640, Training Loss: 1.4738841400685487e-06, Validation Loss: 1.8020432942034713e-06\n",
      "Epoch 1641, Training Loss: 1.6589874576311558e-06, Validation Loss: 1.515571819958391e-06\n",
      "Epoch 1642, Training Loss: 3.5660300454765093e-06, Validation Loss: 2.7406439061057316e-06\n",
      "Epoch 1643, Training Loss: 1.0818473583640298e-06, Validation Loss: 1.6557539374901985e-06\n",
      "Epoch 1644, Training Loss: 7.977482141541259e-07, Validation Loss: 1.5608032891102728e-06\n",
      "Epoch 1645, Training Loss: 1.5076101362865302e-06, Validation Loss: 1.7867448775270451e-06\n",
      "Epoch 1646, Training Loss: 2.098541699524503e-06, Validation Loss: 1.8084798388963174e-06\n",
      "Epoch 1647, Training Loss: 1.4258623650675872e-06, Validation Loss: 1.6322273189882924e-06\n",
      "Epoch 1648, Training Loss: 1.2080361102562165e-06, Validation Loss: 1.4862249473179682e-06\n",
      "Epoch 1649, Training Loss: 1.4346103398565901e-06, Validation Loss: 1.5531444581226024e-06\n",
      "Epoch 1650, Training Loss: 1.5509708646277431e-06, Validation Loss: 1.7830132834020494e-06\n",
      "Epoch 1651, Training Loss: 9.772760449777707e-07, Validation Loss: 1.5460927025383258e-06\n",
      "Epoch 1652, Training Loss: 1.5134460227272939e-06, Validation Loss: 1.5246612629401107e-06\n",
      "Epoch 1653, Training Loss: 2.7889307148143416e-06, Validation Loss: 2.6950298807449168e-06\n",
      "Epoch 1654, Training Loss: 2.081537559206481e-06, Validation Loss: 2.0205324316213973e-06\n",
      "Epoch 1655, Training Loss: 2.7238118036621017e-06, Validation Loss: 1.6697176735882529e-06\n",
      "Epoch 1656, Training Loss: 1.4036197626410285e-06, Validation Loss: 2.0604727984003372e-06\n",
      "Epoch 1657, Training Loss: 3.3055111998692155e-06, Validation Loss: 2.531734611792017e-06\n",
      "Epoch 1658, Training Loss: 1.456086806683743e-06, Validation Loss: 1.6337191115178557e-06\n",
      "Epoch 1659, Training Loss: 1.3119899904268095e-06, Validation Loss: 1.6808916440307368e-06\n",
      "Epoch 1660, Training Loss: 1.4291248362496844e-06, Validation Loss: 2.039885944474576e-06\n",
      "Epoch 1661, Training Loss: 1.101170255424222e-06, Validation Loss: 1.5532691171884042e-06\n",
      "Epoch 1662, Training Loss: 3.206187784599024e-06, Validation Loss: 2.069263237395589e-06\n",
      "Epoch 1663, Training Loss: 2.318113274668576e-06, Validation Loss: 2.2194846735239397e-06\n",
      "Epoch 1664, Training Loss: 1.9106842046312522e-06, Validation Loss: 3.0177745066174313e-06\n",
      "Epoch 1665, Training Loss: 8.70625854076934e-07, Validation Loss: 1.480385431406807e-06\n",
      "Epoch 1666, Training Loss: 1.930098733282648e-06, Validation Loss: 1.8681886769343599e-06\n",
      "Epoch 1667, Training Loss: 1.0188725809712196e-06, Validation Loss: 1.5562273537033184e-06\n",
      "Epoch 1668, Training Loss: 9.863025525191915e-07, Validation Loss: 1.491055782373374e-06\n",
      "Epoch 1669, Training Loss: 1.7109676946347463e-06, Validation Loss: 1.805978170894053e-06\n",
      "Epoch 1670, Training Loss: 1.5500703511861502e-06, Validation Loss: 1.6652669888531285e-06\n",
      "Epoch 1671, Training Loss: 2.5262283998017665e-06, Validation Loss: 2.456594794604165e-06\n",
      "Epoch 1672, Training Loss: 1.4014949556440115e-06, Validation Loss: 1.6170601233529764e-06\n",
      "Epoch 1673, Training Loss: 1.6207213775487617e-06, Validation Loss: 1.5052209994264907e-06\n",
      "Epoch 1674, Training Loss: 6.103047780925408e-06, Validation Loss: 1.3829290423203767e-05\n",
      "Epoch 1675, Training Loss: 1.516826159786433e-06, Validation Loss: 1.659425251884942e-06\n",
      "Epoch 1676, Training Loss: 1.5576322311972035e-06, Validation Loss: 1.9056831257087183e-06\n",
      "Epoch 1677, Training Loss: 1.1244032975810114e-06, Validation Loss: 1.5809101988440727e-06\n",
      "Epoch 1678, Training Loss: 1.4947481759008951e-06, Validation Loss: 1.6040020419484119e-06\n",
      "Epoch 1679, Training Loss: 9.84111807156296e-07, Validation Loss: 1.7211109617413863e-06\n",
      "Epoch 1680, Training Loss: 1.1087148550359416e-06, Validation Loss: 1.5614141422476308e-06\n",
      "Epoch 1681, Training Loss: 3.43768033417291e-06, Validation Loss: 2.8631363129476365e-06\n",
      "Epoch 1682, Training Loss: 1.5027579820525716e-06, Validation Loss: 1.5072409668837687e-06\n",
      "Epoch 1683, Training Loss: 1.245663383997453e-06, Validation Loss: 1.656731159858334e-06\n",
      "Epoch 1684, Training Loss: 8.930760486691725e-07, Validation Loss: 1.4918561708394235e-06\n",
      "Epoch 1685, Training Loss: 1.7752121266312315e-06, Validation Loss: 2.8772700894351032e-06\n",
      "Epoch 1686, Training Loss: 1.9536103081918554e-06, Validation Loss: 1.6426597105503782e-06\n",
      "Epoch 1687, Training Loss: 1.0381951369708986e-06, Validation Loss: 1.5292413642093415e-06\n",
      "Epoch 1688, Training Loss: 1.1655208709271392e-06, Validation Loss: 1.4920124042520425e-06\n",
      "Epoch 1689, Training Loss: 1.967187472473597e-06, Validation Loss: 1.6775893954895967e-06\n",
      "Epoch 1690, Training Loss: 1.1234039902774384e-06, Validation Loss: 1.531414343650453e-06\n",
      "Epoch 1691, Training Loss: 1.54292945353518e-06, Validation Loss: 1.482110667004502e-06\n",
      "Epoch 1692, Training Loss: 2.571181539678946e-06, Validation Loss: 1.8495140660321155e-06\n",
      "Epoch 1693, Training Loss: 1.2853510042987182e-06, Validation Loss: 1.5668391329576491e-06\n",
      "Epoch 1694, Training Loss: 1.4853712855256163e-06, Validation Loss: 1.531258563174885e-06\n",
      "Epoch 1695, Training Loss: 1.3953953157397336e-06, Validation Loss: 1.534449146802333e-06\n",
      "Epoch 1696, Training Loss: 1.3008169617023668e-06, Validation Loss: 1.5541688963723319e-06\n",
      "Epoch 1697, Training Loss: 1.3907082347941468e-06, Validation Loss: 1.5632695190425073e-06\n",
      "Epoch 1698, Training Loss: 2.7052471978095127e-06, Validation Loss: 1.696622479456237e-06\n",
      "Epoch 1699, Training Loss: 2.3870043150964193e-06, Validation Loss: 1.6316690782367201e-06\n",
      "Epoch 1700, Training Loss: 1.2653108569793403e-06, Validation Loss: 1.5581104892757789e-06\n",
      "Epoch 1701, Training Loss: 3.528032721078489e-06, Validation Loss: 1.5032968826455115e-06\n",
      "Epoch 1702, Training Loss: 2.3257339307747316e-06, Validation Loss: 2.3228124672450403e-06\n",
      "Epoch 1703, Training Loss: 1.0038478421847685e-06, Validation Loss: 1.5162091480085984e-06\n",
      "Epoch 1704, Training Loss: 2.5108365662163123e-06, Validation Loss: 4.76679729014968e-06\n",
      "Epoch 1705, Training Loss: 2.1867551822651876e-06, Validation Loss: 1.6834441260668028e-06\n",
      "Epoch 1706, Training Loss: 2.7605944978859043e-06, Validation Loss: 2.3423514322953424e-06\n",
      "Epoch 1707, Training Loss: 1.9666804291773587e-05, Validation Loss: 1.2406718397075919e-05\n",
      "Epoch 1708, Training Loss: 2.600891775728087e-06, Validation Loss: 2.2360521325605936e-06\n",
      "Epoch 1709, Training Loss: 1.1619572433119174e-06, Validation Loss: 1.6606844552648664e-06\n",
      "Epoch 1710, Training Loss: 1.0434907835588092e-06, Validation Loss: 1.505995248428274e-06\n",
      "Epoch 1711, Training Loss: 1.022205651679542e-06, Validation Loss: 1.5285495374407863e-06\n",
      "Epoch 1712, Training Loss: 1.7155362002085894e-06, Validation Loss: 1.5838793844152276e-06\n",
      "Epoch 1713, Training Loss: 1.2999064438190544e-06, Validation Loss: 1.6058904143566071e-06\n",
      "Epoch 1714, Training Loss: 1.580160414960119e-06, Validation Loss: 1.6364835760590734e-06\n",
      "Epoch 1715, Training Loss: 2.4066657715593465e-06, Validation Loss: 1.5347860385567413e-06\n",
      "Epoch 1716, Training Loss: 1.3054972214376903e-06, Validation Loss: 2.106924677290101e-06\n",
      "Epoch 1717, Training Loss: 1.8095463474310236e-06, Validation Loss: 1.7303170829004876e-06\n",
      "Epoch 1718, Training Loss: 3.3564788282092195e-06, Validation Loss: 2.5814202161107542e-06\n",
      "Epoch 1719, Training Loss: 3.2777579690446146e-06, Validation Loss: 8.117944929789741e-06\n",
      "Epoch 1720, Training Loss: 2.1418227333924733e-06, Validation Loss: 1.8108459878432977e-06\n",
      "Epoch 1721, Training Loss: 9.805714853428071e-07, Validation Loss: 1.5013283190177634e-06\n",
      "Epoch 1722, Training Loss: 1.7237329075214802e-06, Validation Loss: 1.481186406953135e-06\n",
      "Epoch 1723, Training Loss: 3.4792799397109775e-06, Validation Loss: 4.228558710872293e-06\n",
      "Epoch 1724, Training Loss: 1.6882029285625322e-06, Validation Loss: 1.8149952094804835e-06\n",
      "Epoch 1725, Training Loss: 1.3716916100747767e-06, Validation Loss: 1.541621333677876e-06\n",
      "Epoch 1726, Training Loss: 2.308411467311089e-06, Validation Loss: 3.0082760632890286e-06\n",
      "Epoch 1727, Training Loss: 3.587703304219758e-06, Validation Loss: 3.0165715246373e-06\n",
      "Epoch 1728, Training Loss: 1.419319005435682e-06, Validation Loss: 1.5938311525608053e-06\n",
      "Epoch 1729, Training Loss: 2.4051407763181487e-06, Validation Loss: 1.569637126426555e-06\n",
      "Epoch 1730, Training Loss: 1.157022552433773e-06, Validation Loss: 1.557115663550796e-06\n",
      "Epoch 1731, Training Loss: 1.0079863386636134e-05, Validation Loss: 7.423649957713302e-06\n",
      "Epoch 1732, Training Loss: 1.4198328699421836e-06, Validation Loss: 1.7236190682565017e-06\n",
      "Epoch 1733, Training Loss: 1.857299025687098e-06, Validation Loss: 1.7887917188872845e-06\n",
      "Epoch 1734, Training Loss: 5.092173523735255e-06, Validation Loss: 2.2122478864562514e-06\n",
      "Epoch 1735, Training Loss: 1.5158142332438729e-06, Validation Loss: 1.5165121172761067e-06\n",
      "Epoch 1736, Training Loss: 1.1289604344710824e-06, Validation Loss: 1.5193445419262025e-06\n",
      "Epoch 1737, Training Loss: 1.6681691477060667e-06, Validation Loss: 1.6744547532085377e-06\n",
      "Epoch 1738, Training Loss: 1.414287680745474e-06, Validation Loss: 2.0380622997036436e-06\n",
      "Epoch 1739, Training Loss: 9.037551080837147e-07, Validation Loss: 1.4676763586373982e-06\n",
      "Epoch 1740, Training Loss: 1.6076517113106092e-06, Validation Loss: 1.7268662688342849e-06\n",
      "Epoch 1741, Training Loss: 1.2173787808933412e-06, Validation Loss: 1.5665587479063082e-06\n",
      "Epoch 1742, Training Loss: 1.697882908047177e-06, Validation Loss: 1.6632914135715608e-06\n",
      "Epoch 1743, Training Loss: 1.9844353573716944e-06, Validation Loss: 2.456042374329036e-06\n",
      "Epoch 1744, Training Loss: 1.1269294191151857e-06, Validation Loss: 1.727962818934829e-06\n",
      "Epoch 1745, Training Loss: 1.238684490090236e-06, Validation Loss: 1.6874756278800079e-06\n",
      "Epoch 1746, Training Loss: 1.2671309832512634e-06, Validation Loss: 1.5305493294670287e-06\n",
      "Epoch 1747, Training Loss: 2.1985183593642432e-06, Validation Loss: 2.4625961917017717e-06\n",
      "Epoch 1748, Training Loss: 2.804380528687034e-06, Validation Loss: 2.552573422904669e-06\n",
      "Epoch 1749, Training Loss: 1.6605243899903144e-06, Validation Loss: 1.5355184812152825e-06\n",
      "Epoch 1750, Training Loss: 1.538539777357073e-06, Validation Loss: 1.5731505779576821e-06\n",
      "Epoch 1751, Training Loss: 1.0345842156311846e-06, Validation Loss: 1.5773893573064334e-06\n",
      "Epoch 1752, Training Loss: 1.3914166174799902e-06, Validation Loss: 1.5011443547061801e-06\n",
      "Epoch 1753, Training Loss: 4.986452040611766e-06, Validation Loss: 2.681235069891497e-06\n",
      "Epoch 1754, Training Loss: 3.7559002521447837e-06, Validation Loss: 2.8266132034285914e-06\n",
      "Epoch 1755, Training Loss: 1.5141313269850798e-06, Validation Loss: 1.5558141154444202e-06\n",
      "Epoch 1756, Training Loss: 3.092100996582303e-06, Validation Loss: 2.283120900489854e-06\n",
      "Epoch 1757, Training Loss: 2.2428541797125945e-06, Validation Loss: 1.590292748941158e-06\n",
      "Epoch 1758, Training Loss: 2.258262611576356e-06, Validation Loss: 1.6838925149200966e-06\n",
      "Epoch 1759, Training Loss: 7.063542284413415e-07, Validation Loss: 1.562827833367919e-06\n",
      "Epoch 1760, Training Loss: 1.9553217498469166e-06, Validation Loss: 1.6298347092488024e-06\n",
      "Epoch 1761, Training Loss: 2.8259632927074563e-06, Validation Loss: 2.6663264118834392e-06\n",
      "Epoch 1762, Training Loss: 1.2795155726053054e-06, Validation Loss: 1.5259608497675057e-06\n",
      "Epoch 1763, Training Loss: 1.5210223409667378e-06, Validation Loss: 1.4815006480533673e-06\n",
      "Epoch 1764, Training Loss: 1.3127732927387115e-06, Validation Loss: 1.52879834253338e-06\n",
      "Epoch 1765, Training Loss: 1.7333526329821325e-06, Validation Loss: 1.662619022907536e-06\n",
      "Epoch 1766, Training Loss: 2.12197846849449e-06, Validation Loss: 1.8956167739028167e-06\n",
      "Epoch 1767, Training Loss: 2.220884198322892e-06, Validation Loss: 3.356911371868991e-06\n",
      "Epoch 1768, Training Loss: 2.588229108368978e-06, Validation Loss: 5.0739721509497865e-06\n",
      "Epoch 1769, Training Loss: 2.550822500779759e-06, Validation Loss: 1.5013899209911088e-06\n",
      "Epoch 1770, Training Loss: 2.6241468731313944e-06, Validation Loss: 1.514413343116688e-06\n",
      "Epoch 1771, Training Loss: 1.4352867765410338e-06, Validation Loss: 1.5203876483512177e-06\n",
      "Epoch 1772, Training Loss: 1.5002653981355252e-06, Validation Loss: 1.8381769473742413e-06\n",
      "Epoch 1773, Training Loss: 3.286256969659007e-06, Validation Loss: 1.4922026206155804e-06\n",
      "Epoch 1774, Training Loss: 1.4097417988523375e-06, Validation Loss: 1.6815047962490486e-06\n",
      "Epoch 1775, Training Loss: 3.90220293411403e-06, Validation Loss: 2.9147073048519886e-06\n",
      "Epoch 1776, Training Loss: 1.7681362578514381e-06, Validation Loss: 1.534966688752181e-06\n",
      "Epoch 1777, Training Loss: 2.5271292543038726e-06, Validation Loss: 1.9025520381326679e-06\n",
      "Epoch 1778, Training Loss: 1.486441760789603e-06, Validation Loss: 1.561778482494209e-06\n",
      "Epoch 1779, Training Loss: 1.863246666289342e-06, Validation Loss: 1.5253178446166122e-06\n",
      "Epoch 1780, Training Loss: 2.2968474695517216e-06, Validation Loss: 2.177437995582712e-06\n",
      "Epoch 1781, Training Loss: 1.1868394267366966e-06, Validation Loss: 1.5693125733146392e-06\n",
      "Epoch 1782, Training Loss: 1.5347417274824693e-06, Validation Loss: 1.5057189921280603e-06\n",
      "Epoch 1783, Training Loss: 1.1861918665090343e-06, Validation Loss: 1.5340572324287672e-06\n",
      "Epoch 1784, Training Loss: 1.6876324480108451e-06, Validation Loss: 1.5033224672528389e-06\n",
      "Epoch 1785, Training Loss: 2.8888525775983e-06, Validation Loss: 1.6745152769672248e-06\n",
      "Epoch 1786, Training Loss: 1.3703578360946267e-06, Validation Loss: 1.4850234416374573e-06\n",
      "Epoch 1787, Training Loss: 1.8078235370921902e-06, Validation Loss: 2.0856592805769183e-06\n",
      "Epoch 1788, Training Loss: 2.537502041377593e-06, Validation Loss: 2.1987407257499867e-06\n",
      "Epoch 1789, Training Loss: 1.4713499467688962e-06, Validation Loss: 2.6401699322458783e-06\n",
      "Epoch 1790, Training Loss: 1.314052497036755e-06, Validation Loss: 1.5284687767008648e-06\n",
      "Epoch 1791, Training Loss: 1.764203602760972e-06, Validation Loss: 1.7040277189182094e-06\n",
      "Epoch 1792, Training Loss: 3.191221821907675e-06, Validation Loss: 1.765190884603946e-06\n",
      "Epoch 1793, Training Loss: 1.4972645203670254e-06, Validation Loss: 2.3829986882626554e-06\n",
      "Epoch 1794, Training Loss: 1.541904453006282e-06, Validation Loss: 1.7969138679099896e-06\n",
      "Epoch 1795, Training Loss: 2.1755965917691356e-06, Validation Loss: 1.5030201998386265e-06\n",
      "Epoch 1796, Training Loss: 2.5349072529934347e-06, Validation Loss: 2.5284626278456706e-06\n",
      "Epoch 1797, Training Loss: 6.302308520389488e-06, Validation Loss: 9.01020878810431e-06\n",
      "Epoch 1798, Training Loss: 1.3508013125829166e-06, Validation Loss: 1.608520246889523e-06\n",
      "Epoch 1799, Training Loss: 1.436797674614354e-06, Validation Loss: 1.4904555285423035e-06\n",
      "Epoch 1800, Training Loss: 1.2395744306559209e-06, Validation Loss: 1.4850440115328551e-06\n",
      "Epoch 1801, Training Loss: 2.0256115931260865e-06, Validation Loss: 1.5022533145940056e-06\n",
      "Epoch 1802, Training Loss: 2.796684157146956e-06, Validation Loss: 1.6541532880032168e-06\n",
      "Epoch 1803, Training Loss: 1.2293764939386165e-06, Validation Loss: 1.524538151861808e-06\n",
      "Epoch 1804, Training Loss: 2.3008676635072334e-06, Validation Loss: 1.8251395888354213e-06\n",
      "Epoch 1805, Training Loss: 1.3432418199954554e-06, Validation Loss: 1.6590689482840228e-06\n",
      "Epoch 1806, Training Loss: 1.7239635781152174e-06, Validation Loss: 1.9920648419487127e-06\n",
      "Epoch 1807, Training Loss: 1.3403328011918347e-06, Validation Loss: 1.6698857487160245e-06\n",
      "Epoch 1808, Training Loss: 1.3446478988043964e-06, Validation Loss: 1.5451878570489197e-06\n",
      "Epoch 1809, Training Loss: 2.121056240866892e-06, Validation Loss: 1.956239512912744e-06\n",
      "Epoch 1810, Training Loss: 1.4694005585624836e-06, Validation Loss: 1.5602965350999685e-06\n",
      "Epoch 1811, Training Loss: 7.650516636203974e-06, Validation Loss: 7.527610725713197e-06\n",
      "Epoch 1812, Training Loss: 1.6529834283574019e-06, Validation Loss: 1.5528747053250085e-06\n",
      "Epoch 1813, Training Loss: 1.4517503359456896e-06, Validation Loss: 1.6448463291534764e-06\n",
      "Epoch 1814, Training Loss: 1.3331655281945132e-06, Validation Loss: 1.5441726900694245e-06\n",
      "Epoch 1815, Training Loss: 3.8588377719861455e-06, Validation Loss: 1.7280084482695347e-06\n",
      "Epoch 1816, Training Loss: 4.1293287722510286e-06, Validation Loss: 2.323634713453882e-06\n",
      "Epoch 1817, Training Loss: 1.1168110631842865e-06, Validation Loss: 1.537244930619103e-06\n",
      "Epoch 1818, Training Loss: 1.6672423726049601e-06, Validation Loss: 2.761114447172644e-06\n",
      "Epoch 1819, Training Loss: 1.5927523691061651e-06, Validation Loss: 1.7092740583660082e-06\n",
      "Epoch 1820, Training Loss: 1.365364823868731e-06, Validation Loss: 1.7769696313688697e-06\n",
      "Epoch 1821, Training Loss: 1.639741981307452e-06, Validation Loss: 1.5548243679728956e-06\n",
      "Epoch 1822, Training Loss: 2.49809272645507e-06, Validation Loss: 2.8678197327425763e-06\n",
      "Epoch 1823, Training Loss: 1.503628482169006e-06, Validation Loss: 1.829852377488733e-06\n",
      "Epoch 1824, Training Loss: 4.837759661313612e-06, Validation Loss: 3.9951313793162225e-06\n",
      "Epoch 1825, Training Loss: 1.5758387235109694e-06, Validation Loss: 1.980358269670983e-06\n",
      "Epoch 1826, Training Loss: 1.3787387160846265e-06, Validation Loss: 1.5827045931502513e-06\n",
      "Epoch 1827, Training Loss: 1.3618534921988612e-06, Validation Loss: 1.523950451555836e-06\n",
      "Epoch 1828, Training Loss: 1.5325325648518628e-06, Validation Loss: 1.6456746996070952e-06\n",
      "Epoch 1829, Training Loss: 2.4978887722681975e-06, Validation Loss: 1.7313606636240932e-06\n",
      "Epoch 1830, Training Loss: 1.5424710682054865e-06, Validation Loss: 1.5361283538941801e-06\n",
      "Epoch 1831, Training Loss: 8.986129387267283e-07, Validation Loss: 1.6232406413046135e-06\n",
      "Epoch 1832, Training Loss: 3.911271051038057e-06, Validation Loss: 2.2098941583393824e-06\n",
      "Epoch 1833, Training Loss: 2.580703494459158e-06, Validation Loss: 1.515496974554631e-06\n",
      "Epoch 1834, Training Loss: 2.996213879669085e-06, Validation Loss: 1.542042570566134e-06\n",
      "Epoch 1835, Training Loss: 1.4859105021969299e-06, Validation Loss: 1.5359999420765536e-06\n",
      "Epoch 1836, Training Loss: 1.3203443813836202e-06, Validation Loss: 1.5086353011906649e-06\n",
      "Epoch 1837, Training Loss: 2.1509026737476233e-06, Validation Loss: 1.6404214321413353e-06\n",
      "Epoch 1838, Training Loss: 1.5457860627066111e-06, Validation Loss: 1.7028179580888342e-06\n",
      "Epoch 1839, Training Loss: 1.7789742514651152e-06, Validation Loss: 1.6513232055192142e-06\n",
      "Epoch 1840, Training Loss: 1.790525857359171e-06, Validation Loss: 2.3163648406199365e-06\n",
      "Epoch 1841, Training Loss: 1.2520094969659112e-06, Validation Loss: 1.556548672533319e-06\n",
      "Epoch 1842, Training Loss: 1.1626492550931289e-06, Validation Loss: 1.5435810880239407e-06\n",
      "Epoch 1843, Training Loss: 3.1170197871688288e-06, Validation Loss: 2.4642489651556085e-06\n",
      "Epoch 1844, Training Loss: 1.2201463732708362e-06, Validation Loss: 1.6033360007304798e-06\n",
      "Epoch 1845, Training Loss: 2.1370842659962364e-06, Validation Loss: 1.5611744827904536e-06\n",
      "Epoch 1846, Training Loss: 2.0733741621370427e-06, Validation Loss: 1.524293295214465e-06\n",
      "Epoch 1847, Training Loss: 2.214594360339106e-06, Validation Loss: 1.975808256311269e-06\n",
      "Epoch 1848, Training Loss: 1.7250813471036963e-06, Validation Loss: 1.5901034219779923e-06\n",
      "Epoch 1849, Training Loss: 2.436046088405419e-06, Validation Loss: 5.413317181870054e-06\n",
      "Epoch 1850, Training Loss: 1.4274573914008215e-06, Validation Loss: 1.536289630259408e-06\n",
      "Epoch 1851, Training Loss: 1.3164394658815581e-06, Validation Loss: 1.5833128955749605e-06\n",
      "Epoch 1852, Training Loss: 1.0899105973294354e-06, Validation Loss: 1.7194502903108676e-06\n",
      "Epoch 1853, Training Loss: 1.2266489193279995e-06, Validation Loss: 1.5674835472460344e-06\n",
      "Epoch 1854, Training Loss: 1.5630930647603236e-06, Validation Loss: 1.7837663765143368e-06\n",
      "Epoch 1855, Training Loss: 1.421485421815305e-06, Validation Loss: 1.5351644028350785e-06\n",
      "Epoch 1856, Training Loss: 1.3967812719783979e-06, Validation Loss: 1.4768649586272706e-06\n",
      "Epoch 1857, Training Loss: 1.6385110939154401e-06, Validation Loss: 1.574382728577009e-06\n",
      "Epoch 1858, Training Loss: 1.7707533288557897e-06, Validation Loss: 1.5718594568170374e-06\n",
      "Epoch 1859, Training Loss: 2.059879534499487e-06, Validation Loss: 1.5037210272375714e-06\n",
      "Epoch 1860, Training Loss: 1.7463150925323134e-06, Validation Loss: 1.4796911891550103e-06\n",
      "Epoch 1861, Training Loss: 2.1650657799909823e-06, Validation Loss: 2.091631240024755e-06\n",
      "Epoch 1862, Training Loss: 2.4692444640095346e-06, Validation Loss: 1.5428392618565677e-06\n",
      "Epoch 1863, Training Loss: 3.2415905479865614e-06, Validation Loss: 3.2302648429078986e-06\n",
      "Epoch 1864, Training Loss: 2.575539383542491e-06, Validation Loss: 2.1359469403372493e-06\n",
      "Epoch 1865, Training Loss: 1.5776208783790935e-06, Validation Loss: 1.5885249117354471e-06\n",
      "Epoch 1866, Training Loss: 1.5949575526974513e-06, Validation Loss: 1.6049312725403893e-06\n",
      "Epoch 1867, Training Loss: 2.7719177069229772e-06, Validation Loss: 2.709589153032336e-06\n",
      "Epoch 1868, Training Loss: 2.5995136638812255e-06, Validation Loss: 1.508881597207509e-06\n",
      "Epoch 1869, Training Loss: 1.5419972214658628e-06, Validation Loss: 1.6469187475177834e-06\n",
      "Epoch 1870, Training Loss: 1.086566953745205e-06, Validation Loss: 1.7167295644270733e-06\n",
      "Epoch 1871, Training Loss: 1.7192536461152486e-06, Validation Loss: 1.6415245080111887e-06\n",
      "Epoch 1872, Training Loss: 1.5438276932400186e-06, Validation Loss: 1.7837685285989338e-06\n",
      "Epoch 1873, Training Loss: 3.7910729133727727e-06, Validation Loss: 3.4002697808538905e-06\n",
      "Epoch 1874, Training Loss: 1.161803879767831e-06, Validation Loss: 1.51447461977913e-06\n",
      "Epoch 1875, Training Loss: 1.5036551985758706e-06, Validation Loss: 1.5070584310798869e-06\n",
      "Epoch 1876, Training Loss: 2.087905158987269e-06, Validation Loss: 1.5691749582940572e-06\n",
      "Epoch 1877, Training Loss: 1.6007140857254853e-06, Validation Loss: 2.121576714416501e-06\n",
      "Epoch 1878, Training Loss: 3.0339701879711356e-06, Validation Loss: 1.7032824129922462e-06\n",
      "Epoch 1879, Training Loss: 2.700733602978289e-06, Validation Loss: 1.6762761604487151e-06\n",
      "Epoch 1880, Training Loss: 1.8179225662606768e-06, Validation Loss: 2.3912756201046187e-06\n",
      "Epoch 1881, Training Loss: 1.6726734202165972e-06, Validation Loss: 1.5542775433349544e-06\n",
      "Epoch 1882, Training Loss: 1.6306725001413724e-06, Validation Loss: 1.531158411768962e-06\n",
      "Epoch 1883, Training Loss: 8.970378075900953e-07, Validation Loss: 1.4987592164367004e-06\n",
      "Epoch 1884, Training Loss: 2.0660177142417524e-06, Validation Loss: 1.4978534625387725e-06\n",
      "Epoch 1885, Training Loss: 2.6288978460797807e-06, Validation Loss: 1.6363212880982258e-06\n",
      "Epoch 1886, Training Loss: 1.8286850718141068e-06, Validation Loss: 1.7878713127835103e-06\n",
      "Epoch 1887, Training Loss: 1.3130960496710031e-06, Validation Loss: 1.7944915287003754e-06\n",
      "Epoch 1888, Training Loss: 1.8357380895395181e-06, Validation Loss: 1.6379654260591758e-06\n",
      "Epoch 1889, Training Loss: 1.673981159910909e-06, Validation Loss: 1.6732068055496676e-06\n",
      "Epoch 1890, Training Loss: 1.1522668046382023e-06, Validation Loss: 1.6090760314261045e-06\n",
      "Epoch 1891, Training Loss: 1.1213821835553972e-06, Validation Loss: 1.7681089563556252e-06\n",
      "Epoch 1892, Training Loss: 1.0259419696012628e-06, Validation Loss: 1.517397985028933e-06\n",
      "Epoch 1893, Training Loss: 1.949817033164436e-06, Validation Loss: 1.5858696854008202e-06\n",
      "Epoch 1894, Training Loss: 1.360375222247967e-06, Validation Loss: 1.617217523141862e-06\n",
      "Epoch 1895, Training Loss: 1.7918226831170614e-06, Validation Loss: 1.5126760051206457e-06\n",
      "Epoch 1896, Training Loss: 8.387052616853907e-07, Validation Loss: 1.5763664042267943e-06\n",
      "Epoch 1897, Training Loss: 1.7022930478560738e-06, Validation Loss: 1.6203213595484384e-06\n",
      "Epoch 1898, Training Loss: 1.6839994714246131e-06, Validation Loss: 1.6092879896732368e-06\n",
      "Epoch 1899, Training Loss: 2.775641405605711e-06, Validation Loss: 2.7069145137650184e-06\n",
      "Epoch 1900, Training Loss: 1.7831143850344233e-06, Validation Loss: 1.6544589039292721e-06\n",
      "Epoch 1901, Training Loss: 1.036119556374615e-06, Validation Loss: 1.4585633975057598e-06\n",
      "Epoch 1902, Training Loss: 1.6346469919881201e-06, Validation Loss: 1.6705881009306287e-06\n",
      "Epoch 1903, Training Loss: 8.507158213433286e-07, Validation Loss: 1.4990942147983768e-06\n",
      "Epoch 1904, Training Loss: 1.659900931372249e-06, Validation Loss: 1.5123959488197782e-06\n",
      "Epoch 1905, Training Loss: 1.9596327547333203e-06, Validation Loss: 1.521322213173068e-06\n",
      "Epoch 1906, Training Loss: 1.4099935015110532e-06, Validation Loss: 1.6701745708513763e-06\n",
      "Epoch 1907, Training Loss: 1.9222297851229087e-06, Validation Loss: 1.5083677319768873e-06\n",
      "Epoch 1908, Training Loss: 1.4858408121654065e-06, Validation Loss: 1.5869529533305306e-06\n",
      "Epoch 1909, Training Loss: 1.0240812571282731e-06, Validation Loss: 1.5388572054903915e-06\n",
      "Epoch 1910, Training Loss: 1.7965429606192629e-06, Validation Loss: 1.8339681914883285e-06\n",
      "Epoch 1911, Training Loss: 2.432133896945743e-06, Validation Loss: 1.5027656302078584e-06\n",
      "Epoch 1912, Training Loss: 1.6504156974406214e-06, Validation Loss: 1.4874508936772592e-06\n",
      "Epoch 1913, Training Loss: 2.8777292300219415e-06, Validation Loss: 1.9894961135566522e-06\n",
      "Epoch 1914, Training Loss: 1.8102143712894758e-06, Validation Loss: 2.0188938375230562e-06\n",
      "Epoch 1915, Training Loss: 1.909897719087894e-06, Validation Loss: 2.445730311233179e-06\n",
      "Epoch 1916, Training Loss: 1.368859329886618e-06, Validation Loss: 1.5946584636268854e-06\n",
      "Epoch 1917, Training Loss: 1.7553545603732346e-06, Validation Loss: 3.3160569270708298e-06\n",
      "Epoch 1918, Training Loss: 1.5604740610797307e-06, Validation Loss: 1.8242527706751139e-06\n",
      "Epoch 1919, Training Loss: 1.6003114069462754e-06, Validation Loss: 1.5088135516524726e-06\n",
      "Epoch 1920, Training Loss: 1.6565848000027472e-06, Validation Loss: 1.576116232357728e-06\n",
      "Epoch 1921, Training Loss: 1.5148023067013128e-06, Validation Loss: 1.781511831836e-06\n",
      "Epoch 1922, Training Loss: 2.591064003354404e-06, Validation Loss: 1.5367021762596002e-06\n",
      "Epoch 1923, Training Loss: 1.6614583273621975e-06, Validation Loss: 1.6158943202273201e-06\n",
      "Epoch 1924, Training Loss: 2.701151061046403e-06, Validation Loss: 1.5403970238297522e-06\n",
      "Epoch 1925, Training Loss: 1.2064647307852283e-06, Validation Loss: 1.6006641674281058e-06\n",
      "Epoch 1926, Training Loss: 3.083827323280275e-06, Validation Loss: 2.051945503159572e-06\n",
      "Epoch 1927, Training Loss: 1.6869817045517266e-06, Validation Loss: 1.930288493269565e-06\n",
      "Epoch 1928, Training Loss: 1.0466424100741278e-06, Validation Loss: 1.5221310903166776e-06\n",
      "Epoch 1929, Training Loss: 2.5100769107666565e-06, Validation Loss: 2.2791768296478166e-06\n",
      "Epoch 1930, Training Loss: 1.4954548532841727e-06, Validation Loss: 1.495790596765314e-06\n",
      "Epoch 1931, Training Loss: 1.6911739066927112e-06, Validation Loss: 1.6347152859163114e-06\n",
      "Epoch 1932, Training Loss: 1.6147554333656444e-06, Validation Loss: 1.6395650338995584e-06\n",
      "Epoch 1933, Training Loss: 1.5618834368069656e-06, Validation Loss: 2.503365097654287e-06\n",
      "Epoch 1934, Training Loss: 2.0720813154184725e-06, Validation Loss: 1.7542307228972172e-06\n",
      "Epoch 1935, Training Loss: 4.934623575536534e-06, Validation Loss: 2.7480864308678167e-06\n",
      "Epoch 1936, Training Loss: 2.1943894807918696e-06, Validation Loss: 2.442798800538689e-06\n",
      "Epoch 1937, Training Loss: 5.48758907825686e-06, Validation Loss: 3.382531706552217e-06\n",
      "Epoch 1938, Training Loss: 1.5623388662788784e-06, Validation Loss: 1.899109917488951e-06\n",
      "Epoch 1939, Training Loss: 1.672570533628459e-06, Validation Loss: 1.4713697778808789e-06\n",
      "Epoch 1940, Training Loss: 2.2511435417982284e-06, Validation Loss: 1.4781512947830513e-06\n",
      "Epoch 1941, Training Loss: 1.3902697446610546e-06, Validation Loss: 1.6991458402829666e-06\n",
      "Epoch 1942, Training Loss: 3.055453817069065e-06, Validation Loss: 4.262450747078827e-06\n",
      "Epoch 1943, Training Loss: 1.6567655620747246e-06, Validation Loss: 1.9425309668352553e-06\n",
      "Epoch 1944, Training Loss: 1.0511505479371408e-06, Validation Loss: 1.646184268271783e-06\n",
      "Epoch 1945, Training Loss: 1.3586499107987038e-06, Validation Loss: 1.4767189981238558e-06\n",
      "Epoch 1946, Training Loss: 1.9277197225164855e-06, Validation Loss: 1.5576808840949084e-06\n",
      "Epoch 1947, Training Loss: 1.106104718928691e-06, Validation Loss: 1.5892681546435719e-06\n",
      "Epoch 1948, Training Loss: 4.93467268825043e-06, Validation Loss: 2.4288984810586634e-06\n",
      "Epoch 1949, Training Loss: 3.547186224750476e-06, Validation Loss: 2.092206155087233e-06\n",
      "Epoch 1950, Training Loss: 1.5397736206068657e-06, Validation Loss: 1.6999162275528674e-06\n",
      "Epoch 1951, Training Loss: 1.3022770417592255e-06, Validation Loss: 1.4669049461946279e-06\n",
      "Epoch 1952, Training Loss: 1.1333626162013388e-06, Validation Loss: 1.6054344635183048e-06\n",
      "Epoch 1953, Training Loss: 4.680543042923091e-06, Validation Loss: 2.5940153020757373e-06\n",
      "Epoch 1954, Training Loss: 2.699432570807403e-06, Validation Loss: 2.102198170907784e-06\n",
      "Epoch 1955, Training Loss: 8.884570661393809e-07, Validation Loss: 1.7096284883064651e-06\n",
      "Epoch 1956, Training Loss: 1.711833192530321e-06, Validation Loss: 1.575655219196607e-06\n",
      "Epoch 1957, Training Loss: 1.3799960925098276e-06, Validation Loss: 2.608314760836484e-06\n",
      "Epoch 1958, Training Loss: 4.3397762965469155e-06, Validation Loss: 3.0327709453466096e-06\n",
      "Epoch 1959, Training Loss: 1.0667630476746126e-06, Validation Loss: 1.783456657000324e-06\n",
      "Epoch 1960, Training Loss: 1.8139766098101973e-06, Validation Loss: 1.5534646013438065e-06\n",
      "Epoch 1961, Training Loss: 4.008251835330157e-06, Validation Loss: 7.415483193485931e-06\n",
      "Epoch 1962, Training Loss: 1.9771021015912993e-06, Validation Loss: 1.6733447710443227e-06\n",
      "Epoch 1963, Training Loss: 1.6655939134579967e-06, Validation Loss: 1.6550457861606903e-06\n",
      "Epoch 1964, Training Loss: 1.0158055374631658e-06, Validation Loss: 1.631852904060356e-06\n",
      "Epoch 1965, Training Loss: 2.7102119020128157e-06, Validation Loss: 2.6129571867508055e-06\n",
      "Epoch 1966, Training Loss: 3.030316293006763e-06, Validation Loss: 1.6752083079473157e-06\n",
      "Epoch 1967, Training Loss: 2.0227803361194674e-06, Validation Loss: 1.7029654019520392e-06\n",
      "Epoch 1968, Training Loss: 2.008427600230789e-06, Validation Loss: 1.5053940132335933e-06\n",
      "Epoch 1969, Training Loss: 2.951655915239826e-06, Validation Loss: 5.516310802183172e-06\n",
      "Epoch 1970, Training Loss: 4.340708528616233e-06, Validation Loss: 4.827681680864864e-06\n",
      "Epoch 1971, Training Loss: 1.5134995692278608e-06, Validation Loss: 2.004595476528208e-06\n",
      "Epoch 1972, Training Loss: 1.3653233281729626e-06, Validation Loss: 1.5372179752526432e-06\n",
      "Epoch 1973, Training Loss: 3.296661361673614e-06, Validation Loss: 2.575576404176749e-06\n",
      "Epoch 1974, Training Loss: 1.2603700270119589e-06, Validation Loss: 2.4511068282651886e-06\n",
      "Epoch 1975, Training Loss: 1.3063290680292994e-06, Validation Loss: 1.5464822805387583e-06\n",
      "Epoch 1976, Training Loss: 1.5123562207008945e-06, Validation Loss: 1.9991841376522757e-06\n",
      "Epoch 1977, Training Loss: 3.6118813113716897e-06, Validation Loss: 1.9862605734829955e-06\n",
      "Epoch 1978, Training Loss: 1.2731064771287492e-06, Validation Loss: 1.6086554784691728e-06\n",
      "Epoch 1979, Training Loss: 1.4129036571830511e-06, Validation Loss: 1.5078885097325513e-06\n",
      "Epoch 1980, Training Loss: 1.0009331390392617e-06, Validation Loss: 1.506874987591603e-06\n",
      "Epoch 1981, Training Loss: 2.2341605472320225e-06, Validation Loss: 1.769843716122459e-06\n",
      "Epoch 1982, Training Loss: 1.2830913647121633e-06, Validation Loss: 1.727765030629633e-06\n",
      "Epoch 1983, Training Loss: 2.5286610707553336e-06, Validation Loss: 1.5149620725522508e-06\n",
      "Epoch 1984, Training Loss: 1.4069631788515835e-06, Validation Loss: 1.5405252973404614e-06\n",
      "Epoch 1985, Training Loss: 2.932004463218618e-06, Validation Loss: 1.4804963010557253e-06\n",
      "Epoch 1986, Training Loss: 2.077475073747337e-06, Validation Loss: 1.6515272853409043e-06\n",
      "Epoch 1987, Training Loss: 2.3533286821475485e-06, Validation Loss: 1.5448422834579454e-06\n",
      "Epoch 1988, Training Loss: 5.086044438940007e-06, Validation Loss: 1.653789125166035e-06\n",
      "Epoch 1989, Training Loss: 4.377638106234372e-06, Validation Loss: 5.22387709523196e-06\n",
      "Epoch 1990, Training Loss: 2.3485454221372493e-06, Validation Loss: 2.506468808426046e-06\n",
      "Epoch 1991, Training Loss: 1.2039985222145333e-06, Validation Loss: 1.831550609747019e-06\n",
      "Epoch 1992, Training Loss: 1.139563437391189e-06, Validation Loss: 1.658031456685375e-06\n",
      "Epoch 1993, Training Loss: 1.4821865761405206e-06, Validation Loss: 1.5325974532437624e-06\n",
      "Epoch 1994, Training Loss: 1.942943072208436e-06, Validation Loss: 1.5323739579549615e-06\n",
      "Epoch 1995, Training Loss: 1.8077947743222467e-06, Validation Loss: 1.958078295774033e-06\n",
      "Epoch 1996, Training Loss: 2.6707673441705992e-06, Validation Loss: 1.908725476001147e-06\n",
      "Epoch 1997, Training Loss: 1.4749848560313694e-06, Validation Loss: 2.1562008800996827e-06\n",
      "Epoch 1998, Training Loss: 1.8162618289352395e-06, Validation Loss: 1.6169258476117075e-06\n",
      "Epoch 1999, Training Loss: 1.707637466097367e-06, Validation Loss: 1.6414518353294452e-06\n",
      "Epoch 2000, Training Loss: 1.616895929146267e-06, Validation Loss: 1.4991355120852692e-06\n",
      "Epoch 2001, Training Loss: 4.289790922484826e-06, Validation Loss: 2.2725285240305347e-06\n",
      "Epoch 2002, Training Loss: 1.5966601267791702e-06, Validation Loss: 1.9149200842415214e-06\n",
      "Epoch 2003, Training Loss: 1.6950586996244965e-06, Validation Loss: 1.5404612217732455e-06\n",
      "Epoch 2004, Training Loss: 2.076797500194516e-06, Validation Loss: 1.6938352359105759e-06\n",
      "Epoch 2005, Training Loss: 1.1987032166871359e-06, Validation Loss: 1.7614083743442309e-06\n",
      "Epoch 2006, Training Loss: 1.0367683671574923e-06, Validation Loss: 1.638804608710358e-06\n",
      "Epoch 2007, Training Loss: 2.0719637632282684e-06, Validation Loss: 2.872196650650659e-06\n",
      "Epoch 2008, Training Loss: 1.2770130979333771e-06, Validation Loss: 1.5913216184431386e-06\n",
      "Epoch 2009, Training Loss: 1.3653623227583012e-06, Validation Loss: 1.934184231999051e-06\n",
      "Epoch 2010, Training Loss: 1.4061851061342168e-06, Validation Loss: 2.0088215653121536e-06\n",
      "Epoch 2011, Training Loss: 1.1221532076888252e-06, Validation Loss: 1.607981903506912e-06\n",
      "Epoch 2012, Training Loss: 2.106109150190605e-06, Validation Loss: 1.6625215107379303e-06\n",
      "Epoch 2013, Training Loss: 3.924715201719664e-06, Validation Loss: 1.6008967145787854e-06\n",
      "Epoch 2014, Training Loss: 1.3700064300792292e-06, Validation Loss: 1.6094491729890466e-06\n",
      "Epoch 2015, Training Loss: 1.978338332264684e-06, Validation Loss: 1.772956266226881e-06\n",
      "Epoch 2016, Training Loss: 2.452237595207407e-06, Validation Loss: 1.636471167176942e-06\n",
      "Epoch 2017, Training Loss: 3.3260082545893965e-06, Validation Loss: 3.6659693595521177e-06\n",
      "Epoch 2018, Training Loss: 3.02381772598892e-06, Validation Loss: 2.6484214025809116e-06\n",
      "Epoch 2019, Training Loss: 3.0564422104362166e-06, Validation Loss: 2.6518821711245547e-06\n",
      "Epoch 2020, Training Loss: 2.948150267911842e-06, Validation Loss: 5.3292991894992085e-06\n",
      "Epoch 2021, Training Loss: 2.1657040633726865e-05, Validation Loss: 8.592542205295995e-06\n",
      "Epoch 2022, Training Loss: 1.4895662161507062e-06, Validation Loss: 1.481965610383859e-06\n",
      "Epoch 2023, Training Loss: 1.7865008885564748e-06, Validation Loss: 1.920579984905732e-06\n",
      "Epoch 2024, Training Loss: 2.157548351533478e-06, Validation Loss: 4.6714869434943435e-06\n",
      "Epoch 2025, Training Loss: 3.7082243125041714e-06, Validation Loss: 4.224836302571374e-06\n",
      "Epoch 2026, Training Loss: 1.8578284652903676e-06, Validation Loss: 1.7189735782284614e-06\n",
      "Epoch 2027, Training Loss: 9.946277259587077e-07, Validation Loss: 1.7746451768885938e-06\n",
      "Epoch 2028, Training Loss: 1.2142862715336378e-06, Validation Loss: 1.4768636593749866e-06\n",
      "Epoch 2029, Training Loss: 1.9129297470499296e-06, Validation Loss: 1.7469423763142021e-06\n",
      "Epoch 2030, Training Loss: 1.9314886685606325e-06, Validation Loss: 1.4933130236745369e-06\n",
      "Epoch 2031, Training Loss: 1.676782176218694e-06, Validation Loss: 1.6023758172676628e-06\n",
      "Epoch 2032, Training Loss: 8.625014743302017e-06, Validation Loss: 1.3270004482637308e-05\n",
      "Epoch 2033, Training Loss: 1.513073925707431e-06, Validation Loss: 1.6763091245628144e-06\n",
      "Epoch 2034, Training Loss: 1.4854398386887624e-06, Validation Loss: 1.6031735874968748e-06\n",
      "Epoch 2035, Training Loss: 1.251733920071274e-06, Validation Loss: 1.4975281420407021e-06\n",
      "Epoch 2036, Training Loss: 2.714223228394985e-06, Validation Loss: 2.1840832598960553e-06\n",
      "Epoch 2037, Training Loss: 1.7529428077978082e-06, Validation Loss: 1.9208850932236774e-06\n",
      "Epoch 2038, Training Loss: 2.454334207868669e-06, Validation Loss: 1.571737327102345e-06\n",
      "Epoch 2039, Training Loss: 1.4991610441938974e-06, Validation Loss: 1.4962062959459832e-06\n",
      "Epoch 2040, Training Loss: 1.547943611512892e-06, Validation Loss: 1.5517016428964977e-06\n",
      "Epoch 2041, Training Loss: 1.5137396758291288e-06, Validation Loss: 1.5553935939867077e-06\n",
      "Epoch 2042, Training Loss: 2.098548748108442e-06, Validation Loss: 1.6254186812582437e-06\n",
      "Epoch 2043, Training Loss: 1.3406349808064988e-06, Validation Loss: 1.575034683126803e-06\n",
      "Epoch 2044, Training Loss: 2.4207206479331944e-06, Validation Loss: 2.8005648320083838e-06\n",
      "Epoch 2045, Training Loss: 1.3057849628239637e-06, Validation Loss: 1.4807886512846762e-06\n",
      "Epoch 2046, Training Loss: 3.865533926727949e-06, Validation Loss: 3.4373041770122887e-06\n",
      "Epoch 2047, Training Loss: 2.038057573372498e-06, Validation Loss: 2.1134959865052063e-06\n",
      "Epoch 2048, Training Loss: 5.8341447584098205e-06, Validation Loss: 2.1566990608914188e-06\n",
      "Epoch 2049, Training Loss: 9.860638101599761e-07, Validation Loss: 1.5178523652509869e-06\n",
      "Epoch 2050, Training Loss: 3.630333139881259e-06, Validation Loss: 3.0563797949140674e-06\n",
      "Epoch 2051, Training Loss: 2.4561215923313284e-06, Validation Loss: 2.866319823914156e-06\n",
      "Epoch 2052, Training Loss: 2.853819523807033e-06, Validation Loss: 2.6247435060004774e-06\n",
      "Epoch 2053, Training Loss: 1.1173013945153798e-06, Validation Loss: 1.5414685309647182e-06\n",
      "Epoch 2054, Training Loss: 1.4918231272531557e-06, Validation Loss: 1.7146636236780966e-06\n",
      "Epoch 2055, Training Loss: 2.138623131031636e-06, Validation Loss: 1.51918997923942e-06\n",
      "Epoch 2056, Training Loss: 2.0436170871107606e-06, Validation Loss: 2.015717609755351e-06\n",
      "Epoch 2057, Training Loss: 1.1956751677644206e-06, Validation Loss: 1.4848701807373788e-06\n",
      "Epoch 2058, Training Loss: 1.8739337974693626e-06, Validation Loss: 2.475347868613617e-06\n",
      "Epoch 2059, Training Loss: 4.225036718707997e-06, Validation Loss: 2.032724229098143e-06\n",
      "Epoch 2060, Training Loss: 7.288304004759993e-06, Validation Loss: 4.93787564473504e-06\n",
      "Epoch 2061, Training Loss: 1.7899126305565005e-06, Validation Loss: 1.7830523120210273e-06\n",
      "Epoch 2062, Training Loss: 2.5347578684886685e-06, Validation Loss: 1.5280087754974235e-06\n",
      "Epoch 2063, Training Loss: 1.5395987702504499e-06, Validation Loss: 1.793244190388473e-06\n",
      "Epoch 2064, Training Loss: 1.3058119066045037e-06, Validation Loss: 1.5319288573305216e-06\n",
      "Epoch 2065, Training Loss: 1.6725465457056998e-06, Validation Loss: 1.5645455835562938e-06\n",
      "Epoch 2066, Training Loss: 1.6564688394282712e-06, Validation Loss: 3.349409094173943e-06\n",
      "Epoch 2067, Training Loss: 1.0373322538725915e-06, Validation Loss: 1.6317677450156428e-06\n",
      "Epoch 2068, Training Loss: 3.221540055164951e-06, Validation Loss: 2.1345972867678777e-06\n",
      "Epoch 2069, Training Loss: 1.8902773035733844e-06, Validation Loss: 1.5702530089392721e-06\n",
      "Epoch 2070, Training Loss: 2.2209137569007e-06, Validation Loss: 1.9547867837599176e-06\n",
      "Epoch 2071, Training Loss: 2.149167585230316e-06, Validation Loss: 1.7144482110165736e-06\n",
      "Epoch 2072, Training Loss: 4.375322532723658e-06, Validation Loss: 2.7955300460246454e-06\n",
      "Epoch 2073, Training Loss: 2.6083278044097824e-06, Validation Loss: 2.5391413056489807e-06\n",
      "Epoch 2074, Training Loss: 2.705440238059964e-06, Validation Loss: 1.4560030444664815e-06\n",
      "Epoch 2075, Training Loss: 1.0938023251583218e-06, Validation Loss: 1.5022215427434624e-06\n",
      "Epoch 2076, Training Loss: 1.3757629631072632e-06, Validation Loss: 1.5308600240086021e-06\n",
      "Epoch 2077, Training Loss: 2.149945203200332e-06, Validation Loss: 1.6076784852850127e-06\n",
      "Epoch 2078, Training Loss: 2.90901584776293e-06, Validation Loss: 1.7015797585332087e-06\n",
      "Epoch 2079, Training Loss: 1.9750245883187745e-06, Validation Loss: 1.580459049283334e-06\n",
      "Epoch 2080, Training Loss: 1.8241292991660885e-06, Validation Loss: 1.704497481259055e-06\n",
      "Epoch 2081, Training Loss: 2.361015503993258e-06, Validation Loss: 1.7280222930815971e-06\n",
      "Epoch 2082, Training Loss: 1.4362257161337766e-06, Validation Loss: 1.5689188290898701e-06\n",
      "Epoch 2083, Training Loss: 2.0816419237235095e-06, Validation Loss: 1.8181325896592078e-06\n",
      "Epoch 2084, Training Loss: 1.934925876412308e-06, Validation Loss: 1.4622326229953754e-06\n",
      "Epoch 2085, Training Loss: 1.6250407952611567e-06, Validation Loss: 1.6089491666908272e-06\n",
      "Epoch 2086, Training Loss: 1.8388894886811613e-06, Validation Loss: 2.0589663156486286e-06\n",
      "Epoch 2087, Training Loss: 1.2863301890320145e-06, Validation Loss: 1.4723790327827525e-06\n",
      "Epoch 2088, Training Loss: 1.834073600548436e-06, Validation Loss: 1.884271590720664e-06\n",
      "Epoch 2089, Training Loss: 1.7566887891007354e-06, Validation Loss: 1.993232644007589e-06\n",
      "Epoch 2090, Training Loss: 2.96156645163137e-06, Validation Loss: 3.95260044830706e-06\n",
      "Epoch 2091, Training Loss: 1.0851863407879137e-06, Validation Loss: 1.6239859530235367e-06\n",
      "Epoch 2092, Training Loss: 2.9300640562723856e-06, Validation Loss: 4.663986780953773e-06\n",
      "Epoch 2093, Training Loss: 1.1882165154020186e-06, Validation Loss: 1.4832142747404846e-06\n",
      "Epoch 2094, Training Loss: 1.202661223942414e-06, Validation Loss: 1.518765151078094e-06\n",
      "Epoch 2095, Training Loss: 2.25367330131121e-06, Validation Loss: 1.4979739617162877e-06\n",
      "Epoch 2096, Training Loss: 1.0112571544595994e-06, Validation Loss: 1.5698879123493689e-06\n",
      "Epoch 2097, Training Loss: 2.1445246147777652e-06, Validation Loss: 1.8807193198412347e-06\n",
      "Epoch 2098, Training Loss: 3.5443415526970057e-06, Validation Loss: 1.5129920556156918e-06\n",
      "Epoch 2099, Training Loss: 3.0279322800197406e-06, Validation Loss: 1.6473731552563967e-06\n",
      "Epoch 2100, Training Loss: 1.786965526662243e-06, Validation Loss: 1.5380400382591282e-06\n",
      "Epoch 2101, Training Loss: 3.006126917171059e-06, Validation Loss: 1.6761447728600081e-06\n",
      "Epoch 2102, Training Loss: 3.5937071061198367e-06, Validation Loss: 3.6034031263104e-06\n",
      "Epoch 2103, Training Loss: 2.486762696207734e-06, Validation Loss: 1.7370728982499552e-06\n",
      "Epoch 2104, Training Loss: 2.6967022677126806e-06, Validation Loss: 2.61253981268061e-06\n",
      "Epoch 2105, Training Loss: 4.077069661434507e-06, Validation Loss: 5.675925007913963e-06\n",
      "Epoch 2106, Training Loss: 3.970382749685086e-06, Validation Loss: 3.978554293104362e-06\n",
      "Epoch 2107, Training Loss: 2.049160229944391e-06, Validation Loss: 1.6476631624141046e-06\n",
      "Epoch 2108, Training Loss: 1.642923280087416e-06, Validation Loss: 2.5507956887890563e-06\n",
      "Epoch 2109, Training Loss: 1.1356859204170178e-06, Validation Loss: 1.4442936770856076e-06\n",
      "Epoch 2110, Training Loss: 2.9950404041301226e-06, Validation Loss: 3.0992638481029936e-06\n",
      "Epoch 2111, Training Loss: 3.059415121242637e-06, Validation Loss: 3.2297265849714456e-06\n",
      "Epoch 2112, Training Loss: 2.4483738343406003e-06, Validation Loss: 1.6419296658151689e-06\n",
      "Epoch 2113, Training Loss: 1.3123833468853263e-06, Validation Loss: 1.549192902052275e-06\n",
      "Epoch 2114, Training Loss: 1.2593473002198152e-06, Validation Loss: 1.689692401373722e-06\n",
      "Epoch 2115, Training Loss: 1.1907129646715475e-06, Validation Loss: 1.5452308484145182e-06\n",
      "Epoch 2116, Training Loss: 2.9944653761049267e-06, Validation Loss: 1.9013993535267287e-06\n",
      "Epoch 2117, Training Loss: 2.018552095250925e-06, Validation Loss: 1.6010462000982896e-06\n",
      "Epoch 2118, Training Loss: 1.2382452041492797e-06, Validation Loss: 1.9066384778429922e-06\n",
      "Epoch 2119, Training Loss: 1.1992887039014022e-06, Validation Loss: 1.5401875619862699e-06\n",
      "Epoch 2120, Training Loss: 1.9197746041754726e-06, Validation Loss: 2.165131611911223e-06\n",
      "Epoch 2121, Training Loss: 2.593510316728498e-06, Validation Loss: 2.3747999968610044e-06\n",
      "Epoch 2122, Training Loss: 1.6171002243936528e-06, Validation Loss: 1.692923091291716e-06\n",
      "Epoch 2123, Training Loss: 1.7390166249242611e-06, Validation Loss: 1.690394454164712e-06\n",
      "Epoch 2124, Training Loss: 1.7248830772587098e-06, Validation Loss: 1.9175975508353226e-06\n",
      "Epoch 2125, Training Loss: 8.634114578853769e-07, Validation Loss: 1.4725247508870022e-06\n",
      "Epoch 2126, Training Loss: 8.824437713883526e-07, Validation Loss: 1.4879571161834939e-06\n",
      "Epoch 2127, Training Loss: 1.9876360966009088e-06, Validation Loss: 1.5875082809474013e-06\n",
      "Epoch 2128, Training Loss: 1.9535882529453374e-06, Validation Loss: 1.5352642358092378e-06\n",
      "Epoch 2129, Training Loss: 1.21319089885219e-06, Validation Loss: 1.5691738565454991e-06\n",
      "Epoch 2130, Training Loss: 1.083595179807162e-06, Validation Loss: 1.5090750103797529e-06\n",
      "Epoch 2131, Training Loss: 1.1648824056464946e-06, Validation Loss: 1.617915947367605e-06\n",
      "Epoch 2132, Training Loss: 3.7690122098865686e-06, Validation Loss: 2.0379528627261166e-06\n",
      "Epoch 2133, Training Loss: 1.964296643564012e-06, Validation Loss: 4.48252778404109e-06\n",
      "Epoch 2134, Training Loss: 9.706582204671577e-07, Validation Loss: 1.5056185785907027e-06\n",
      "Epoch 2135, Training Loss: 1.3658822126672021e-06, Validation Loss: 1.5227969816417725e-06\n",
      "Epoch 2136, Training Loss: 1.8287286138729542e-06, Validation Loss: 2.0637566944825054e-06\n",
      "Epoch 2137, Training Loss: 1.5828627510927618e-06, Validation Loss: 1.5852058082154648e-06\n",
      "Epoch 2138, Training Loss: 1.4145327440928668e-05, Validation Loss: 1.6227293761716235e-05\n",
      "Epoch 2139, Training Loss: 1.13168448478973e-06, Validation Loss: 1.6710148299069168e-06\n",
      "Epoch 2140, Training Loss: 1.78798609340447e-06, Validation Loss: 1.5393596614865219e-06\n",
      "Epoch 2141, Training Loss: 1.9861104192386847e-06, Validation Loss: 1.5302819296973275e-06\n",
      "Epoch 2142, Training Loss: 1.1969035540460027e-06, Validation Loss: 1.4784021630745135e-06\n",
      "Epoch 2143, Training Loss: 3.8232988117670175e-06, Validation Loss: 5.839224093703306e-06\n",
      "Epoch 2144, Training Loss: 4.0956160773930606e-06, Validation Loss: 3.097338106758827e-06\n",
      "Epoch 2145, Training Loss: 2.2167341739987023e-06, Validation Loss: 1.566003748677851e-06\n",
      "Epoch 2146, Training Loss: 1.3931728517491138e-06, Validation Loss: 1.4908588029102015e-06\n",
      "Epoch 2147, Training Loss: 1.7924610347108683e-06, Validation Loss: 1.5693544868276393e-06\n",
      "Epoch 2148, Training Loss: 3.8855214370414615e-06, Validation Loss: 1.6619428465442273e-06\n",
      "Epoch 2149, Training Loss: 1.3592687082564225e-06, Validation Loss: 1.5521355243679423e-06\n",
      "Epoch 2150, Training Loss: 5.061101546743885e-06, Validation Loss: 2.1162621625040097e-06\n",
      "Epoch 2151, Training Loss: 4.259996785549447e-06, Validation Loss: 1.6925298539509369e-06\n",
      "Epoch 2152, Training Loss: 3.0496626095555257e-06, Validation Loss: 2.5077882470028433e-06\n",
      "Epoch 2153, Training Loss: 1.404016529704677e-06, Validation Loss: 1.4427577782266983e-06\n",
      "Epoch 2154, Training Loss: 1.2468165095924633e-06, Validation Loss: 1.4727511602156546e-06\n",
      "Epoch 2155, Training Loss: 1.3614533145300811e-06, Validation Loss: 1.4703908994503363e-06\n",
      "Epoch 2156, Training Loss: 1.2752943803207017e-06, Validation Loss: 1.493942870479114e-06\n",
      "Epoch 2157, Training Loss: 1.589792077538732e-06, Validation Loss: 1.6794795861630757e-06\n",
      "Epoch 2158, Training Loss: 1.5568306253044284e-06, Validation Loss: 1.4434259666414172e-06\n",
      "Epoch 2159, Training Loss: 1.105554247260443e-06, Validation Loss: 1.566683213270014e-06\n",
      "Epoch 2160, Training Loss: 2.6024786166090053e-06, Validation Loss: 1.7058647215305144e-06\n",
      "Epoch 2161, Training Loss: 9.355661632071133e-07, Validation Loss: 1.4536713581999106e-06\n",
      "Epoch 2162, Training Loss: 1.7217060985785793e-06, Validation Loss: 1.5325121710986515e-06\n",
      "Epoch 2163, Training Loss: 1.1637243915174622e-06, Validation Loss: 1.5864740442680883e-06\n",
      "Epoch 2164, Training Loss: 1.93165146811225e-06, Validation Loss: 2.3041852677367856e-06\n",
      "Epoch 2165, Training Loss: 2.0025368030474056e-06, Validation Loss: 1.8179185922901965e-06\n",
      "Epoch 2166, Training Loss: 9.979356718758936e-07, Validation Loss: 1.449714726772605e-06\n",
      "Epoch 2167, Training Loss: 1.1081920092692599e-06, Validation Loss: 1.4768823302657223e-06\n",
      "Epoch 2168, Training Loss: 1.970011453522602e-06, Validation Loss: 3.059450527813447e-06\n",
      "Epoch 2169, Training Loss: 1.089490979211405e-06, Validation Loss: 1.4883731459249364e-06\n",
      "Epoch 2170, Training Loss: 3.8414909795392305e-06, Validation Loss: 3.9008683795325355e-06\n",
      "Epoch 2171, Training Loss: 4.9085756472777575e-06, Validation Loss: 2.850051962280298e-06\n",
      "Epoch 2172, Training Loss: 1.7826627072281553e-06, Validation Loss: 1.6305174953793388e-06\n",
      "Epoch 2173, Training Loss: 1.5819566669961205e-06, Validation Loss: 1.6776307018279888e-06\n",
      "Epoch 2174, Training Loss: 4.924363111058483e-06, Validation Loss: 3.4874564230767938e-06\n",
      "Epoch 2175, Training Loss: 1.9430872271186672e-06, Validation Loss: 1.9065231487011434e-06\n",
      "Epoch 2176, Training Loss: 1.9542674181138864e-06, Validation Loss: 2.8864993136423413e-06\n",
      "Epoch 2177, Training Loss: 1.1891770554939285e-05, Validation Loss: 4.539420645814507e-06\n",
      "Epoch 2178, Training Loss: 1.3794867754768347e-06, Validation Loss: 1.652089745901692e-06\n",
      "Epoch 2179, Training Loss: 1.248321950697573e-06, Validation Loss: 1.4892552712446478e-06\n",
      "Epoch 2180, Training Loss: 1.0535841283854097e-06, Validation Loss: 1.6865234444947905e-06\n",
      "Epoch 2181, Training Loss: 2.7389858132664813e-06, Validation Loss: 1.8476541319881102e-06\n",
      "Epoch 2182, Training Loss: 2.5998795081250137e-06, Validation Loss: 2.0525490347197565e-06\n",
      "Epoch 2183, Training Loss: 2.6725372208602494e-06, Validation Loss: 2.7601013417627474e-06\n",
      "Epoch 2184, Training Loss: 1.3655626389663666e-06, Validation Loss: 1.557940422455987e-06\n",
      "Epoch 2185, Training Loss: 6.10038114245981e-06, Validation Loss: 6.092091483484982e-06\n",
      "Epoch 2186, Training Loss: 4.297706254874356e-06, Validation Loss: 3.2823967772554886e-06\n",
      "Epoch 2187, Training Loss: 1.568780817251536e-06, Validation Loss: 2.5937443725831672e-06\n",
      "Epoch 2188, Training Loss: 2.2241363240027567e-06, Validation Loss: 1.5106848551043701e-06\n",
      "Epoch 2189, Training Loss: 1.5218172393360874e-06, Validation Loss: 1.6085419777302875e-06\n",
      "Epoch 2190, Training Loss: 3.9717797335470095e-06, Validation Loss: 1.5194188333781736e-06\n",
      "Epoch 2191, Training Loss: 1.4236678680390469e-06, Validation Loss: 1.4916296358759535e-06\n",
      "Epoch 2192, Training Loss: 9.197613053402165e-07, Validation Loss: 1.5320717815988374e-06\n",
      "Epoch 2193, Training Loss: 1.2819061794289155e-06, Validation Loss: 1.7034808131788299e-06\n",
      "Epoch 2194, Training Loss: 2.94185474558617e-06, Validation Loss: 2.019160344452815e-06\n",
      "Epoch 2195, Training Loss: 1.431204054824775e-06, Validation Loss: 1.4593114722855177e-06\n",
      "Epoch 2196, Training Loss: 1.8639827885635896e-06, Validation Loss: 1.5127088110145283e-06\n",
      "Epoch 2197, Training Loss: 2.6357251954323146e-06, Validation Loss: 2.066385769947976e-06\n",
      "Epoch 2198, Training Loss: 1.3995245353726204e-06, Validation Loss: 1.5424560648014472e-06\n",
      "Epoch 2199, Training Loss: 2.784929165500216e-06, Validation Loss: 3.542245680086369e-06\n",
      "Epoch 2200, Training Loss: 9.078307243726158e-07, Validation Loss: 1.489662520850236e-06\n",
      "Epoch 2201, Training Loss: 1.9599863207986346e-06, Validation Loss: 1.5505378198769418e-06\n",
      "Epoch 2202, Training Loss: 2.7203811896470143e-06, Validation Loss: 3.6363862369734696e-06\n",
      "Epoch 2203, Training Loss: 1.3046271760686068e-06, Validation Loss: 1.5424741464395447e-06\n",
      "Epoch 2204, Training Loss: 1.5563307442789664e-06, Validation Loss: 1.625740747742129e-06\n",
      "Epoch 2205, Training Loss: 2.019133717112709e-06, Validation Loss: 1.524168552512805e-06\n",
      "Epoch 2206, Training Loss: 1.018702505461988e-06, Validation Loss: 1.5303696459717697e-06\n",
      "Epoch 2207, Training Loss: 1.6631032622171915e-06, Validation Loss: 1.533039122263591e-06\n",
      "Epoch 2208, Training Loss: 1.8701501858231495e-06, Validation Loss: 1.4882664260266252e-06\n",
      "Epoch 2209, Training Loss: 2.1648452275258023e-06, Validation Loss: 2.0144236801153422e-06\n",
      "Epoch 2210, Training Loss: 1.0309631761629134e-06, Validation Loss: 1.5519943832450909e-06\n",
      "Epoch 2211, Training Loss: 2.8127924451837316e-06, Validation Loss: 2.2948275158527233e-06\n",
      "Epoch 2212, Training Loss: 1.2290601034692372e-06, Validation Loss: 1.5989854226868693e-06\n",
      "Epoch 2213, Training Loss: 1.4225237237042165e-06, Validation Loss: 1.6594214459102984e-06\n",
      "Epoch 2214, Training Loss: 1.1605120562308002e-06, Validation Loss: 1.560186631427111e-06\n",
      "Epoch 2215, Training Loss: 2.48594005824998e-06, Validation Loss: 1.7825604412108448e-06\n",
      "Epoch 2216, Training Loss: 3.105880296061514e-06, Validation Loss: 1.757116984419994e-06\n",
      "Epoch 2217, Training Loss: 3.441283070060308e-06, Validation Loss: 2.167526269462032e-06\n",
      "Epoch 2218, Training Loss: 1.2660135553232976e-06, Validation Loss: 1.5762790033066058e-06\n",
      "Epoch 2219, Training Loss: 2.2745571186533198e-06, Validation Loss: 1.773808918958033e-06\n",
      "Epoch 2220, Training Loss: 1.5938344404275995e-06, Validation Loss: 1.4503080424480343e-06\n",
      "Epoch 2221, Training Loss: 8.924000098886609e-07, Validation Loss: 1.4119719260237339e-06\n",
      "Epoch 2222, Training Loss: 1.630578708500252e-06, Validation Loss: 1.7786430838791922e-06\n",
      "Epoch 2223, Training Loss: 9.392574042976776e-07, Validation Loss: 1.4934360820731106e-06\n",
      "Epoch 2224, Training Loss: 5.252020400803303e-06, Validation Loss: 6.375563000680124e-06\n",
      "Epoch 2225, Training Loss: 1.6587699747105944e-06, Validation Loss: 2.081815682831107e-06\n",
      "Epoch 2226, Training Loss: 1.5910177353362087e-06, Validation Loss: 1.4357107272499674e-06\n",
      "Epoch 2227, Training Loss: 1.081393975255196e-06, Validation Loss: 1.443151838891901e-06\n",
      "Epoch 2228, Training Loss: 8.197906709028757e-07, Validation Loss: 1.5326132187840864e-06\n",
      "Epoch 2229, Training Loss: 1.500965822742728e-06, Validation Loss: 1.606200184559019e-06\n",
      "Epoch 2230, Training Loss: 7.788118523421872e-07, Validation Loss: 1.5794941619370912e-06\n",
      "Epoch 2231, Training Loss: 2.5073904907912947e-06, Validation Loss: 1.8393675769536e-06\n",
      "Epoch 2232, Training Loss: 8.426485464951838e-07, Validation Loss: 1.4317775723356812e-06\n",
      "Epoch 2233, Training Loss: 1.3397170732787345e-06, Validation Loss: 1.5739749204938816e-06\n",
      "Epoch 2234, Training Loss: 1.5765219814056763e-06, Validation Loss: 1.5319515183034076e-06\n",
      "Epoch 2235, Training Loss: 9.521380661681178e-07, Validation Loss: 1.401675066008002e-06\n",
      "Epoch 2236, Training Loss: 2.3916895770526025e-06, Validation Loss: 2.0001885521741067e-06\n",
      "Epoch 2237, Training Loss: 1.7043784055204014e-06, Validation Loss: 2.4034991354849686e-06\n",
      "Epoch 2238, Training Loss: 3.2265902518702205e-06, Validation Loss: 1.4475061573772967e-06\n",
      "Epoch 2239, Training Loss: 1.3587344938059687e-06, Validation Loss: 1.6625853086051318e-06\n",
      "Epoch 2240, Training Loss: 1.1372521839803085e-06, Validation Loss: 1.6804713542914197e-06\n",
      "Epoch 2241, Training Loss: 2.49511981564865e-06, Validation Loss: 1.6029719132017766e-06\n",
      "Epoch 2242, Training Loss: 2.0972131551388884e-06, Validation Loss: 1.5285817151605813e-06\n",
      "Epoch 2243, Training Loss: 9.295616791860084e-07, Validation Loss: 1.447407243130369e-06\n",
      "Epoch 2244, Training Loss: 1.5441246432601474e-06, Validation Loss: 1.667563470578365e-06\n",
      "Epoch 2245, Training Loss: 1.401962322233885e-06, Validation Loss: 1.4579250475412229e-06\n",
      "Epoch 2246, Training Loss: 1.2441573744581547e-06, Validation Loss: 2.242600175800565e-06\n",
      "Epoch 2247, Training Loss: 9.352092433800863e-07, Validation Loss: 1.4374399708517818e-06\n",
      "Epoch 2248, Training Loss: 1.8078065977533697e-06, Validation Loss: 1.714271961298286e-06\n",
      "Epoch 2249, Training Loss: 1.1059973985538818e-06, Validation Loss: 1.4837552689453326e-06\n",
      "Epoch 2250, Training Loss: 1.619650902284775e-06, Validation Loss: 1.732487967100421e-06\n",
      "Epoch 2251, Training Loss: 1.32687205223192e-06, Validation Loss: 1.5296952038758261e-06\n",
      "Epoch 2252, Training Loss: 1.5788700693519786e-06, Validation Loss: 1.4613013152652197e-06\n",
      "Epoch 2253, Training Loss: 3.318594281154219e-06, Validation Loss: 1.4445381669661777e-06\n",
      "Epoch 2254, Training Loss: 2.1094224393891636e-06, Validation Loss: 3.4248853034734137e-06\n",
      "Epoch 2255, Training Loss: 1.5227853964461247e-06, Validation Loss: 1.409781194238088e-06\n",
      "Epoch 2256, Training Loss: 1.1362246823409805e-06, Validation Loss: 1.4811812329348086e-06\n",
      "Epoch 2257, Training Loss: 1.1962232520090765e-06, Validation Loss: 1.5823267611001035e-06\n",
      "Epoch 2258, Training Loss: 1.8400654653305537e-06, Validation Loss: 1.4358662618868003e-06\n",
      "Epoch 2259, Training Loss: 9.379700713907368e-07, Validation Loss: 1.4211448479107705e-06\n",
      "Epoch 2260, Training Loss: 1.9417161638557445e-06, Validation Loss: 1.4205233503906558e-06\n",
      "Epoch 2261, Training Loss: 1.5376645023934543e-06, Validation Loss: 1.4581958999150149e-06\n",
      "Epoch 2262, Training Loss: 1.4149136404739693e-06, Validation Loss: 1.658323856335515e-06\n",
      "Epoch 2263, Training Loss: 1.8345748458159505e-06, Validation Loss: 1.5393217487364417e-06\n",
      "Epoch 2264, Training Loss: 4.332131538831163e-06, Validation Loss: 2.1619122106617424e-06\n",
      "Epoch 2265, Training Loss: 1.2111140677006915e-06, Validation Loss: 1.4861640575166285e-06\n",
      "Epoch 2266, Training Loss: 1.5289431303244783e-06, Validation Loss: 1.4234772251688375e-06\n",
      "Epoch 2267, Training Loss: 1.419002501279465e-06, Validation Loss: 1.5830781279103454e-06\n",
      "Epoch 2268, Training Loss: 1.6978920029941946e-06, Validation Loss: 2.1801227500821074e-06\n",
      "Epoch 2269, Training Loss: 2.573912524894695e-06, Validation Loss: 2.4242828121019253e-06\n",
      "Epoch 2270, Training Loss: 1.8814383793142042e-06, Validation Loss: 1.4639451017712859e-06\n",
      "Epoch 2271, Training Loss: 1.0250918194287806e-06, Validation Loss: 1.5356073665814411e-06\n",
      "Epoch 2272, Training Loss: 1.5528544281551149e-06, Validation Loss: 1.478650885798459e-06\n",
      "Epoch 2273, Training Loss: 1.16639853331435e-06, Validation Loss: 1.5215622716203572e-06\n",
      "Epoch 2274, Training Loss: 2.0090865291422233e-06, Validation Loss: 2.4057782292790538e-06\n",
      "Epoch 2275, Training Loss: 1.486671635575476e-06, Validation Loss: 1.485907699490526e-06\n",
      "Epoch 2276, Training Loss: 3.8066707475081785e-06, Validation Loss: 4.889749411081452e-06\n",
      "Epoch 2277, Training Loss: 1.8281782558915438e-06, Validation Loss: 1.5626585927690308e-06\n",
      "Epoch 2278, Training Loss: 1.1269667083979584e-06, Validation Loss: 1.4392502766084616e-06\n",
      "Epoch 2279, Training Loss: 2.391079760855064e-06, Validation Loss: 2.273323827182956e-06\n",
      "Epoch 2280, Training Loss: 1.8576365619082935e-06, Validation Loss: 1.6818027383135575e-06\n",
      "Epoch 2281, Training Loss: 1.8471419025445357e-06, Validation Loss: 1.421014332705616e-06\n",
      "Epoch 2282, Training Loss: 9.009753512145835e-07, Validation Loss: 1.555717972585707e-06\n",
      "Epoch 2283, Training Loss: 8.533497748430818e-07, Validation Loss: 1.4722222970118936e-06\n",
      "Epoch 2284, Training Loss: 1.8284985117134056e-06, Validation Loss: 1.4064574465745789e-06\n",
      "Epoch 2285, Training Loss: 9.141328973782947e-07, Validation Loss: 1.4476158939594673e-06\n",
      "Epoch 2286, Training Loss: 1.8353618997934973e-06, Validation Loss: 1.47537403757611e-06\n",
      "Epoch 2287, Training Loss: 2.1126797946635634e-06, Validation Loss: 2.0790150495827944e-06\n",
      "Epoch 2288, Training Loss: 1.7265837186641875e-06, Validation Loss: 1.670794065998701e-06\n",
      "Epoch 2289, Training Loss: 1.325356947745604e-06, Validation Loss: 1.4715224444594793e-06\n",
      "Epoch 2290, Training Loss: 1.3039966688666027e-06, Validation Loss: 1.4854676228105456e-06\n",
      "Epoch 2291, Training Loss: 1.4586438510377775e-06, Validation Loss: 1.488918697559943e-06\n",
      "Epoch 2292, Training Loss: 1.7629934063734254e-06, Validation Loss: 1.6040280671827502e-06\n",
      "Epoch 2293, Training Loss: 1.0956634923786623e-06, Validation Loss: 1.4703575030366056e-06\n",
      "Epoch 2294, Training Loss: 1.5289160728571005e-06, Validation Loss: 1.4630401797061912e-06\n",
      "Epoch 2295, Training Loss: 9.037714221449278e-07, Validation Loss: 1.4542116107248634e-06\n",
      "Epoch 2296, Training Loss: 1.164468585557188e-06, Validation Loss: 2.0858869026255164e-06\n",
      "Epoch 2297, Training Loss: 1.422660716343671e-06, Validation Loss: 1.4535192368836593e-06\n",
      "Epoch 2298, Training Loss: 4.227385034027975e-06, Validation Loss: 7.725702451665666e-06\n",
      "Epoch 2299, Training Loss: 1.7030910157700418e-06, Validation Loss: 1.634851680432658e-06\n",
      "Epoch 2300, Training Loss: 1.9688004613271914e-06, Validation Loss: 1.7798186267807131e-06\n",
      "Epoch 2301, Training Loss: 2.8101064799557207e-06, Validation Loss: 2.87772481289003e-06\n",
      "Epoch 2302, Training Loss: 2.5170556909870356e-06, Validation Loss: 2.0122171359026034e-06\n",
      "Epoch 2303, Training Loss: 1.7509677263660706e-06, Validation Loss: 2.207463587333773e-06\n",
      "Epoch 2304, Training Loss: 1.4094297284827917e-06, Validation Loss: 1.533431569589466e-06\n",
      "Epoch 2305, Training Loss: 1.2088386256436934e-06, Validation Loss: 1.446171769162369e-06\n",
      "Epoch 2306, Training Loss: 1.5740406524855644e-06, Validation Loss: 1.4769285168107567e-06\n",
      "Epoch 2307, Training Loss: 1.2189263998152455e-06, Validation Loss: 1.3653618613128403e-06\n",
      "Epoch 2308, Training Loss: 3.281129465904087e-06, Validation Loss: 1.385222990965286e-06\n",
      "Epoch 2309, Training Loss: 2.1671867216355167e-06, Validation Loss: 2.853142196455007e-06\n",
      "Epoch 2310, Training Loss: 1.2714516515188734e-06, Validation Loss: 1.931297886840416e-06\n",
      "Epoch 2311, Training Loss: 1.5333350802393397e-06, Validation Loss: 1.6138421833776267e-06\n",
      "Epoch 2312, Training Loss: 5.7919169194065034e-06, Validation Loss: 2.738423832435184e-06\n",
      "Epoch 2313, Training Loss: 1.4710816458318732e-06, Validation Loss: 1.9809740160491427e-06\n",
      "Epoch 2314, Training Loss: 1.63862387125846e-06, Validation Loss: 1.7631010685328678e-06\n",
      "Epoch 2315, Training Loss: 1.4684836742162588e-06, Validation Loss: 1.366099025957807e-06\n",
      "Epoch 2316, Training Loss: 1.703213911241619e-06, Validation Loss: 1.859092410207957e-06\n",
      "Epoch 2317, Training Loss: 1.1544110520844697e-06, Validation Loss: 1.4644294118636697e-06\n",
      "Epoch 2318, Training Loss: 1.455862161492405e-06, Validation Loss: 1.9022152959090365e-06\n",
      "Epoch 2319, Training Loss: 1.1690228802763158e-06, Validation Loss: 1.6163931755368226e-06\n",
      "Epoch 2320, Training Loss: 9.061600394488778e-07, Validation Loss: 1.3635417716089766e-06\n",
      "Epoch 2321, Training Loss: 2.9844213713658974e-06, Validation Loss: 2.629110034235008e-06\n",
      "Epoch 2322, Training Loss: 1.604600015525648e-06, Validation Loss: 1.3578833991999953e-06\n",
      "Epoch 2323, Training Loss: 3.5926714190281928e-06, Validation Loss: 4.0709486505770975e-06\n",
      "Epoch 2324, Training Loss: 1.0621278079270269e-06, Validation Loss: 1.375827539403449e-06\n",
      "Epoch 2325, Training Loss: 9.128716556006111e-07, Validation Loss: 1.457497269315076e-06\n",
      "Epoch 2326, Training Loss: 2.3329275791184045e-06, Validation Loss: 2.0248856273587557e-06\n",
      "Epoch 2327, Training Loss: 5.044869794801343e-06, Validation Loss: 3.33465491195827e-06\n",
      "Epoch 2328, Training Loss: 2.082713308482198e-06, Validation Loss: 1.396884551944718e-06\n",
      "Epoch 2329, Training Loss: 1.770350536389742e-06, Validation Loss: 2.53158029965075e-06\n",
      "Epoch 2330, Training Loss: 9.238406164513435e-07, Validation Loss: 1.357075620365374e-06\n",
      "Epoch 2331, Training Loss: 1.350538127553591e-06, Validation Loss: 1.4641504526045803e-06\n",
      "Epoch 2332, Training Loss: 9.55472046371142e-07, Validation Loss: 1.4374251864940382e-06\n",
      "Epoch 2333, Training Loss: 2.2412166345020523e-06, Validation Loss: 1.643656237957327e-06\n",
      "Epoch 2334, Training Loss: 3.4618947211129125e-06, Validation Loss: 3.5269798506195896e-06\n",
      "Epoch 2335, Training Loss: 1.2615546438610181e-06, Validation Loss: 1.4388548922519256e-06\n",
      "Epoch 2336, Training Loss: 1.5788787095516454e-06, Validation Loss: 1.3731069049398028e-06\n",
      "Epoch 2337, Training Loss: 1.3242010936664883e-06, Validation Loss: 1.3885599894814395e-06\n",
      "Epoch 2338, Training Loss: 1.1794709280366078e-06, Validation Loss: 1.392681789237865e-06\n",
      "Epoch 2339, Training Loss: 1.0926073628070299e-06, Validation Loss: 1.4662838911118244e-06\n",
      "Epoch 2340, Training Loss: 1.6680954786352231e-06, Validation Loss: 1.354392147920413e-06\n",
      "Epoch 2341, Training Loss: 8.303429126499395e-07, Validation Loss: 1.3572071303303584e-06\n",
      "Epoch 2342, Training Loss: 6.612753509216418e-07, Validation Loss: 1.3562552087145157e-06\n",
      "Epoch 2343, Training Loss: 1.2875569836978684e-06, Validation Loss: 1.636800397017784e-06\n",
      "Epoch 2344, Training Loss: 1.3861661045666551e-06, Validation Loss: 1.738729959580286e-06\n",
      "Epoch 2345, Training Loss: 1.5985972368071089e-06, Validation Loss: 1.5379160214877537e-06\n",
      "Epoch 2346, Training Loss: 4.205990080663469e-06, Validation Loss: 2.0705146702439493e-06\n",
      "Epoch 2347, Training Loss: 1.5224734397634165e-06, Validation Loss: 2.1710787333194374e-06\n",
      "Epoch 2348, Training Loss: 4.152285328018479e-06, Validation Loss: 2.4884914863294075e-06\n",
      "Epoch 2349, Training Loss: 1.8019567278315662e-06, Validation Loss: 1.7316185683704847e-06\n",
      "Epoch 2350, Training Loss: 1.5861759266044828e-06, Validation Loss: 1.4335954190778901e-06\n",
      "Epoch 2351, Training Loss: 1.2612449609150644e-06, Validation Loss: 1.3623269088112135e-06\n",
      "Epoch 2352, Training Loss: 1.3777703316009138e-06, Validation Loss: 1.5162601477791765e-06\n",
      "Epoch 2353, Training Loss: 3.7886252357566264e-06, Validation Loss: 4.5053998943840735e-06\n",
      "Epoch 2354, Training Loss: 6.292915259109577e-06, Validation Loss: 1.6823222929549053e-06\n",
      "Epoch 2355, Training Loss: 1.420428134224494e-06, Validation Loss: 2.0621672380801995e-06\n",
      "Epoch 2356, Training Loss: 2.43816521106055e-06, Validation Loss: 1.4211199881475486e-06\n",
      "Epoch 2357, Training Loss: 2.3504298951593228e-06, Validation Loss: 1.92655786113797e-06\n",
      "Epoch 2358, Training Loss: 1.7585674640940852e-06, Validation Loss: 1.6740927214564732e-06\n",
      "Epoch 2359, Training Loss: 2.6077275379066123e-06, Validation Loss: 1.4734699944873086e-06\n",
      "Epoch 2360, Training Loss: 1.4618237855756888e-06, Validation Loss: 1.318724960534255e-06\n",
      "Epoch 2361, Training Loss: 1.19579544843873e-06, Validation Loss: 1.3550305004193699e-06\n",
      "Epoch 2362, Training Loss: 1.3934694607087295e-06, Validation Loss: 1.3276734856031085e-06\n",
      "Epoch 2363, Training Loss: 2.018903387579485e-06, Validation Loss: 1.7629566536635619e-06\n",
      "Epoch 2364, Training Loss: 4.964817890140694e-06, Validation Loss: 2.7614189077652625e-06\n",
      "Epoch 2365, Training Loss: 1.5222581168927718e-06, Validation Loss: 1.7436384731588952e-06\n",
      "Epoch 2366, Training Loss: 2.9048542273812927e-06, Validation Loss: 4.9850103713055385e-06\n",
      "Epoch 2367, Training Loss: 1.3583608051703777e-06, Validation Loss: 1.372272957676116e-06\n",
      "Epoch 2368, Training Loss: 1.9344049633218674e-06, Validation Loss: 1.5005556949412273e-06\n",
      "Epoch 2369, Training Loss: 8.606295409663289e-07, Validation Loss: 1.3413579588547697e-06\n",
      "Epoch 2370, Training Loss: 1.7018971902871272e-06, Validation Loss: 1.9815855245150877e-06\n",
      "Epoch 2371, Training Loss: 1.793915430425841e-06, Validation Loss: 1.3922395327756982e-06\n",
      "Epoch 2372, Training Loss: 1.5183493360382272e-06, Validation Loss: 1.3298628708876414e-06\n",
      "Epoch 2373, Training Loss: 1.52972893374681e-06, Validation Loss: 1.5394392849960062e-06\n",
      "Epoch 2374, Training Loss: 8.264596544904634e-07, Validation Loss: 1.3216045539981452e-06\n",
      "Epoch 2375, Training Loss: 1.7849085907073459e-06, Validation Loss: 1.3402312140370107e-06\n",
      "Epoch 2376, Training Loss: 1.5461844213859877e-06, Validation Loss: 1.3709202919486346e-06\n",
      "Epoch 2377, Training Loss: 9.310703603659931e-07, Validation Loss: 1.3774890283654314e-06\n",
      "Epoch 2378, Training Loss: 1.3176540960557759e-06, Validation Loss: 1.5598959649581563e-06\n",
      "Epoch 2379, Training Loss: 1.5983632692950778e-06, Validation Loss: 1.4656241834093456e-06\n",
      "Epoch 2380, Training Loss: 1.53046892137354e-06, Validation Loss: 1.4410139776408156e-06\n",
      "Epoch 2381, Training Loss: 1.6461542600154644e-06, Validation Loss: 2.157508455418758e-06\n",
      "Epoch 2382, Training Loss: 1.4522942137773498e-06, Validation Loss: 1.6649810277158394e-06\n",
      "Epoch 2383, Training Loss: 1.2394039003993385e-06, Validation Loss: 1.4230437982637138e-06\n",
      "Epoch 2384, Training Loss: 2.080715148622403e-06, Validation Loss: 1.3456110265390745e-06\n",
      "Epoch 2385, Training Loss: 2.913349590016878e-06, Validation Loss: 1.5703326299143364e-06\n",
      "Epoch 2386, Training Loss: 1.4406571153813275e-06, Validation Loss: 1.3245184743699476e-06\n",
      "Epoch 2387, Training Loss: 9.789446266950108e-07, Validation Loss: 1.3180109344193046e-06\n",
      "Epoch 2388, Training Loss: 1.5545947462669574e-06, Validation Loss: 1.3118132553671015e-06\n",
      "Epoch 2389, Training Loss: 8.892695291251584e-07, Validation Loss: 1.3756931916122415e-06\n",
      "Epoch 2390, Training Loss: 1.0202323892372078e-06, Validation Loss: 1.2837317858408887e-06\n",
      "Epoch 2391, Training Loss: 2.7140968086314388e-06, Validation Loss: 1.6316293627899361e-06\n",
      "Epoch 2392, Training Loss: 1.947131522683776e-06, Validation Loss: 1.7318887596227298e-06\n",
      "Epoch 2393, Training Loss: 2.0036345631524455e-06, Validation Loss: 1.6754237162641188e-06\n",
      "Epoch 2394, Training Loss: 1.0587848464638228e-06, Validation Loss: 1.3817684131044287e-06\n",
      "Epoch 2395, Training Loss: 1.4128345355857164e-06, Validation Loss: 1.4573390588738668e-06\n",
      "Epoch 2396, Training Loss: 1.4633095588578726e-06, Validation Loss: 1.9524166391188574e-06\n",
      "Epoch 2397, Training Loss: 9.939544725057203e-07, Validation Loss: 1.2946697597871422e-06\n",
      "Epoch 2398, Training Loss: 1.9376952877792064e-06, Validation Loss: 1.4445608744637726e-06\n",
      "Epoch 2399, Training Loss: 2.4885216589609627e-06, Validation Loss: 1.6158718417326707e-06\n",
      "Epoch 2400, Training Loss: 1.5097961068022414e-06, Validation Loss: 1.3088830620278952e-06\n",
      "Epoch 2401, Training Loss: 2.850806140486384e-06, Validation Loss: 2.232417001717405e-06\n",
      "Epoch 2402, Training Loss: 1.631594386708457e-06, Validation Loss: 1.5959233770549553e-06\n",
      "Epoch 2403, Training Loss: 1.7922296819961048e-06, Validation Loss: 1.391212895011513e-06\n",
      "Epoch 2404, Training Loss: 1.2385320360408514e-06, Validation Loss: 1.2672934540524073e-06\n",
      "Epoch 2405, Training Loss: 3.06481592815544e-06, Validation Loss: 2.176823105476435e-06\n",
      "Epoch 2406, Training Loss: 4.8469019020558335e-06, Validation Loss: 3.604942434507231e-06\n",
      "Epoch 2407, Training Loss: 3.137836529276683e-06, Validation Loss: 1.9449586902390216e-06\n",
      "Epoch 2408, Training Loss: 6.379889782692771e-06, Validation Loss: 6.0833157922794145e-06\n",
      "Epoch 2409, Training Loss: 1.2121657846364542e-06, Validation Loss: 1.613661692850644e-06\n",
      "Epoch 2410, Training Loss: 1.366304559269338e-06, Validation Loss: 1.2617271998432599e-06\n",
      "Epoch 2411, Training Loss: 1.2856729654231458e-06, Validation Loss: 1.5943664528291775e-06\n",
      "Epoch 2412, Training Loss: 1.3733272226090776e-06, Validation Loss: 1.3410435126475514e-06\n",
      "Epoch 2413, Training Loss: 7.203675522760022e-07, Validation Loss: 1.3301858928478476e-06\n",
      "Epoch 2414, Training Loss: 1.8511816506361356e-06, Validation Loss: 1.8396410604173592e-06\n",
      "Epoch 2415, Training Loss: 2.768362492133747e-06, Validation Loss: 1.6815076767983508e-06\n",
      "Epoch 2416, Training Loss: 2.10038160730619e-06, Validation Loss: 1.567150155525576e-06\n",
      "Epoch 2417, Training Loss: 1.2747241271426901e-06, Validation Loss: 1.62886630955857e-06\n",
      "Epoch 2418, Training Loss: 1.2760951904056128e-06, Validation Loss: 1.3458493329780587e-06\n",
      "Epoch 2419, Training Loss: 1.0716360065998742e-06, Validation Loss: 1.2930436764908202e-06\n",
      "Epoch 2420, Training Loss: 1.0402764019090682e-06, Validation Loss: 1.2843896984499236e-06\n",
      "Epoch 2421, Training Loss: 1.9549329408619087e-06, Validation Loss: 1.2599223494255208e-06\n",
      "Epoch 2422, Training Loss: 1.1423958312661853e-06, Validation Loss: 1.3035770498434222e-06\n",
      "Epoch 2423, Training Loss: 1.3315764135768404e-06, Validation Loss: 1.276276910307301e-06\n",
      "Epoch 2424, Training Loss: 1.1138524769194191e-06, Validation Loss: 1.313708852320392e-06\n",
      "Epoch 2425, Training Loss: 1.1840231763926568e-06, Validation Loss: 1.2906540088508313e-06\n",
      "Epoch 2426, Training Loss: 1.3680431720786146e-06, Validation Loss: 1.5662758164870385e-06\n",
      "Epoch 2427, Training Loss: 2.562749386925134e-06, Validation Loss: 3.996118685055156e-06\n",
      "Epoch 2428, Training Loss: 1.8418098761685542e-06, Validation Loss: 2.499033044772265e-06\n",
      "Epoch 2429, Training Loss: 1.2700978686552844e-06, Validation Loss: 1.230223439156043e-06\n",
      "Epoch 2430, Training Loss: 1.3037300732321455e-06, Validation Loss: 1.3306651563670228e-06\n",
      "Epoch 2431, Training Loss: 4.312260898586828e-06, Validation Loss: 7.620682913077228e-06\n",
      "Epoch 2432, Training Loss: 3.025177193194395e-06, Validation Loss: 1.509039817062278e-06\n",
      "Epoch 2433, Training Loss: 1.1745881920433021e-06, Validation Loss: 1.3033061900474005e-06\n",
      "Epoch 2434, Training Loss: 9.784359917830443e-07, Validation Loss: 1.3232785887009362e-06\n",
      "Epoch 2435, Training Loss: 1.1271104085608386e-06, Validation Loss: 1.2674690998546272e-06\n",
      "Epoch 2436, Training Loss: 1.1092030263171182e-06, Validation Loss: 1.3856735780658257e-06\n",
      "Epoch 2437, Training Loss: 1.0041023870144272e-06, Validation Loss: 1.24292512753087e-06\n",
      "Epoch 2438, Training Loss: 2.5257809284084942e-06, Validation Loss: 1.8804349075965947e-06\n",
      "Epoch 2439, Training Loss: 2.0541974663501605e-06, Validation Loss: 1.8598934940102227e-06\n",
      "Epoch 2440, Training Loss: 1.6154774584720144e-06, Validation Loss: 3.929918725038901e-06\n",
      "Epoch 2441, Training Loss: 1.050505261446233e-06, Validation Loss: 1.3974880496522934e-06\n",
      "Epoch 2442, Training Loss: 7.668832040508278e-07, Validation Loss: 1.2464446938932147e-06\n",
      "Epoch 2443, Training Loss: 2.9292154977156315e-06, Validation Loss: 1.351257908612126e-06\n",
      "Epoch 2444, Training Loss: 8.130717787935282e-07, Validation Loss: 1.2317607694191336e-06\n",
      "Epoch 2445, Training Loss: 1.599287315912079e-06, Validation Loss: 1.9198252024215187e-06\n",
      "Epoch 2446, Training Loss: 9.229323723047855e-07, Validation Loss: 1.224721931773338e-06\n",
      "Epoch 2447, Training Loss: 1.6569827039347729e-06, Validation Loss: 1.3319252438974495e-06\n",
      "Epoch 2448, Training Loss: 1.3047432503299206e-06, Validation Loss: 1.2927922582302289e-06\n",
      "Epoch 2449, Training Loss: 1.9651106413220987e-06, Validation Loss: 1.2517415392617614e-06\n",
      "Epoch 2450, Training Loss: 8.757056093600113e-07, Validation Loss: 1.2088122159016426e-06\n",
      "Epoch 2451, Training Loss: 1.4536531125486363e-06, Validation Loss: 1.3624413074588808e-06\n",
      "Epoch 2452, Training Loss: 1.6523567865078803e-06, Validation Loss: 2.8274012233821988e-06\n",
      "Epoch 2453, Training Loss: 2.1114999526616884e-06, Validation Loss: 1.2831622669205425e-06\n",
      "Epoch 2454, Training Loss: 1.7293994005740387e-06, Validation Loss: 2.2648153949057523e-06\n",
      "Epoch 2455, Training Loss: 7.302041922230273e-07, Validation Loss: 1.2125840812355112e-06\n",
      "Epoch 2456, Training Loss: 2.2859935597807635e-06, Validation Loss: 1.7012184190398503e-06\n",
      "Epoch 2457, Training Loss: 1.1320508974677068e-06, Validation Loss: 1.3002935315708568e-06\n",
      "Epoch 2458, Training Loss: 1.3429441878543003e-06, Validation Loss: 1.543388768634935e-06\n",
      "Epoch 2459, Training Loss: 2.192987722082762e-06, Validation Loss: 1.3125335681107357e-06\n",
      "Epoch 2460, Training Loss: 1.257947019439598e-06, Validation Loss: 1.3982174911955547e-06\n",
      "Epoch 2461, Training Loss: 2.3070631414157106e-06, Validation Loss: 1.2919085016492201e-06\n",
      "Epoch 2462, Training Loss: 1.1522145086928504e-06, Validation Loss: 1.2588760936216586e-06\n",
      "Epoch 2463, Training Loss: 1.5676978364354e-06, Validation Loss: 1.5282241762109668e-06\n",
      "Epoch 2464, Training Loss: 4.216232809994835e-06, Validation Loss: 2.4781416270667877e-06\n",
      "Epoch 2465, Training Loss: 8.740682460484095e-06, Validation Loss: 8.053230888603788e-06\n",
      "Epoch 2466, Training Loss: 1.3811999224344618e-06, Validation Loss: 1.1799908322468785e-06\n",
      "Epoch 2467, Training Loss: 1.5166327784754685e-06, Validation Loss: 1.6327429713470385e-06\n",
      "Epoch 2468, Training Loss: 1.3880403457733337e-06, Validation Loss: 1.385839619502618e-06\n",
      "Epoch 2469, Training Loss: 8.726177611606545e-07, Validation Loss: 1.233304392465928e-06\n",
      "Epoch 2470, Training Loss: 1.0467292668181472e-06, Validation Loss: 1.7277802650279477e-06\n",
      "Epoch 2471, Training Loss: 1.8431904891258455e-06, Validation Loss: 2.345302417472722e-06\n",
      "Epoch 2472, Training Loss: 7.35250796424225e-06, Validation Loss: 1.2755636915499285e-05\n",
      "Epoch 2473, Training Loss: 2.591361408121884e-06, Validation Loss: 1.3101817038787092e-06\n",
      "Epoch 2474, Training Loss: 1.3028741250309395e-06, Validation Loss: 1.1731311038691933e-06\n",
      "Epoch 2475, Training Loss: 8.129022148750664e-07, Validation Loss: 1.2048297008054557e-06\n",
      "Epoch 2476, Training Loss: 1.623509660930722e-06, Validation Loss: 4.19517162525524e-06\n",
      "Epoch 2477, Training Loss: 1.3331073205335997e-06, Validation Loss: 1.2850013593431254e-06\n",
      "Epoch 2478, Training Loss: 2.2314980014925823e-06, Validation Loss: 1.5068714386795543e-06\n",
      "Epoch 2479, Training Loss: 8.102767424134072e-06, Validation Loss: 3.205999713259913e-06\n",
      "Epoch 2480, Training Loss: 1.2318949984546634e-06, Validation Loss: 1.2684512216185845e-06\n",
      "Epoch 2481, Training Loss: 1.10083583422238e-05, Validation Loss: 1.6907543215945522e-05\n",
      "Epoch 2482, Training Loss: 1.3451697213895386e-06, Validation Loss: 1.2354856657963233e-06\n",
      "Epoch 2483, Training Loss: 1.001769760478055e-06, Validation Loss: 1.1750920412462846e-06\n",
      "Epoch 2484, Training Loss: 1.0673818451323314e-06, Validation Loss: 1.449030328026861e-06\n",
      "Epoch 2485, Training Loss: 9.52322977809672e-07, Validation Loss: 1.5324800625323151e-06\n",
      "Epoch 2486, Training Loss: 1.3222966117609758e-06, Validation Loss: 1.2521497166460811e-06\n",
      "Epoch 2487, Training Loss: 8.372026059078053e-07, Validation Loss: 1.7484875459002555e-06\n",
      "Epoch 2488, Training Loss: 2.5776778329600347e-06, Validation Loss: 2.163867876988429e-06\n",
      "Epoch 2489, Training Loss: 7.080561204020341e-07, Validation Loss: 1.194959361762546e-06\n",
      "Epoch 2490, Training Loss: 1.5741487686682376e-06, Validation Loss: 1.3251620758336494e-06\n",
      "Epoch 2491, Training Loss: 9.710887525216094e-07, Validation Loss: 1.3182119084903767e-06\n",
      "Epoch 2492, Training Loss: 3.2198818189499434e-06, Validation Loss: 1.3413579668200894e-06\n",
      "Epoch 2493, Training Loss: 7.107931651262334e-06, Validation Loss: 4.117002548416057e-06\n",
      "Epoch 2494, Training Loss: 1.2785733360942686e-06, Validation Loss: 1.4129907481798858e-06\n",
      "Epoch 2495, Training Loss: 9.561439355820767e-07, Validation Loss: 1.205956281429008e-06\n",
      "Epoch 2496, Training Loss: 1.1004970019712346e-06, Validation Loss: 1.2381291543270154e-06\n",
      "Epoch 2497, Training Loss: 2.792606892398908e-06, Validation Loss: 1.2570985966553411e-06\n",
      "Epoch 2498, Training Loss: 8.878846529114526e-07, Validation Loss: 1.6043727399684627e-06\n",
      "Epoch 2499, Training Loss: 9.838274763751542e-07, Validation Loss: 1.7366647013143958e-06\n",
      "Epoch 2500, Training Loss: 1.1533679753483739e-06, Validation Loss: 1.3200783724673813e-06\n",
      "Epoch 2501, Training Loss: 2.3990314730326645e-06, Validation Loss: 1.6470202014345302e-06\n",
      "Epoch 2502, Training Loss: 8.534431685802701e-07, Validation Loss: 1.1842489383580422e-06\n",
      "Epoch 2503, Training Loss: 9.082179417418956e-07, Validation Loss: 1.130825195136851e-06\n",
      "Epoch 2504, Training Loss: 6.920372470631264e-07, Validation Loss: 1.1463531267115176e-06\n",
      "Epoch 2505, Training Loss: 1.4555229199686437e-06, Validation Loss: 1.173296046996998e-06\n",
      "Epoch 2506, Training Loss: 1.0192801482844516e-06, Validation Loss: 1.8462281377072082e-06\n",
      "Epoch 2507, Training Loss: 1.1794070360338083e-06, Validation Loss: 1.191802806162486e-06\n",
      "Epoch 2508, Training Loss: 9.5542873168597e-07, Validation Loss: 1.1531824831436213e-06\n",
      "Epoch 2509, Training Loss: 2.3655936729483074e-06, Validation Loss: 3.504969060723985e-06\n",
      "Epoch 2510, Training Loss: 1.1792124041676288e-06, Validation Loss: 1.448266680503725e-06\n",
      "Epoch 2511, Training Loss: 1.455710616937722e-06, Validation Loss: 1.5135080859650702e-06\n",
      "Epoch 2512, Training Loss: 7.827145509509137e-07, Validation Loss: 1.1589368628198484e-06\n",
      "Epoch 2513, Training Loss: 1.7764244830686948e-06, Validation Loss: 1.924426667402681e-06\n",
      "Epoch 2514, Training Loss: 8.10381038718333e-07, Validation Loss: 1.17488368694432e-06\n",
      "Epoch 2515, Training Loss: 1.298152596973523e-06, Validation Loss: 1.2351543235164536e-06\n",
      "Epoch 2516, Training Loss: 1.0324646382287028e-06, Validation Loss: 1.2199368268863458e-06\n",
      "Epoch 2517, Training Loss: 1.8616324268805329e-06, Validation Loss: 1.5541978981018706e-06\n",
      "Epoch 2518, Training Loss: 1.057586132446886e-06, Validation Loss: 1.6923169384171838e-06\n",
      "Epoch 2519, Training Loss: 8.543066769561847e-07, Validation Loss: 1.1329006667530767e-06\n",
      "Epoch 2520, Training Loss: 1.3559038052335382e-06, Validation Loss: 1.129843592204863e-06\n",
      "Epoch 2521, Training Loss: 2.185613539040787e-06, Validation Loss: 2.4096554645705506e-06\n",
      "Epoch 2522, Training Loss: 1.6462604435218964e-06, Validation Loss: 1.3625159812462042e-06\n",
      "Epoch 2523, Training Loss: 7.213411663542502e-07, Validation Loss: 1.1500362015389456e-06\n",
      "Epoch 2524, Training Loss: 3.4337824672547868e-06, Validation Loss: 4.657885503814208e-06\n",
      "Epoch 2525, Training Loss: 8.16649276202952e-07, Validation Loss: 1.1051643522764867e-06\n",
      "Epoch 2526, Training Loss: 1.5666396393498871e-06, Validation Loss: 1.662071142864716e-06\n",
      "Epoch 2527, Training Loss: 1.2362115739961155e-06, Validation Loss: 1.2335918179548577e-06\n",
      "Epoch 2528, Training Loss: 1.088927774617332e-06, Validation Loss: 1.332253507762231e-06\n",
      "Epoch 2529, Training Loss: 7.570483830932062e-07, Validation Loss: 1.1147025246289234e-06\n",
      "Epoch 2530, Training Loss: 1.6901525441426202e-06, Validation Loss: 1.1305003846002805e-06\n",
      "Epoch 2531, Training Loss: 1.593774641150958e-06, Validation Loss: 1.1949316355703616e-06\n",
      "Epoch 2532, Training Loss: 1.0466444564372068e-06, Validation Loss: 1.2713810201313689e-06\n",
      "Epoch 2533, Training Loss: 1.081770051314379e-06, Validation Loss: 1.0984564115767698e-06\n",
      "Epoch 2534, Training Loss: 1.201209784085222e-06, Validation Loss: 1.531201382497141e-06\n",
      "Epoch 2535, Training Loss: 2.0680959096353035e-06, Validation Loss: 1.0874262701905235e-06\n",
      "Epoch 2536, Training Loss: 8.950919436756521e-07, Validation Loss: 1.1173647450574602e-06\n",
      "Epoch 2537, Training Loss: 1.0361952718085377e-06, Validation Loss: 1.1906214170782622e-06\n",
      "Epoch 2538, Training Loss: 8.434964229309116e-07, Validation Loss: 1.1362860153037513e-06\n",
      "Epoch 2539, Training Loss: 9.964008995666518e-07, Validation Loss: 1.150121904035708e-06\n",
      "Epoch 2540, Training Loss: 3.158559366056579e-06, Validation Loss: 1.9059306603327974e-06\n",
      "Epoch 2541, Training Loss: 1.0399434131613816e-06, Validation Loss: 1.119875473067953e-06\n",
      "Epoch 2542, Training Loss: 1.913320375024341e-06, Validation Loss: 1.5383879681365355e-06\n",
      "Epoch 2543, Training Loss: 1.3509474001693889e-06, Validation Loss: 1.1984074969468623e-06\n",
      "Epoch 2544, Training Loss: 8.334376389029785e-07, Validation Loss: 1.2339118554545438e-06\n",
      "Epoch 2545, Training Loss: 1.359065549877414e-06, Validation Loss: 1.4341150913887354e-06\n",
      "Epoch 2546, Training Loss: 1.4320725085781305e-06, Validation Loss: 1.8902833811124225e-06\n",
      "Epoch 2547, Training Loss: 2.155927859348594e-06, Validation Loss: 2.6246913939816016e-06\n",
      "Epoch 2548, Training Loss: 1.258462020814477e-06, Validation Loss: 1.1260775162102785e-06\n",
      "Epoch 2549, Training Loss: 8.8265596787096e-07, Validation Loss: 1.2609674122417068e-06\n",
      "Epoch 2550, Training Loss: 7.396633350253978e-07, Validation Loss: 1.0970256634481633e-06\n",
      "Epoch 2551, Training Loss: 9.951778565664426e-07, Validation Loss: 1.0989719046291188e-06\n",
      "Epoch 2552, Training Loss: 1.0808037131937454e-06, Validation Loss: 1.1281877904812469e-06\n",
      "Epoch 2553, Training Loss: 7.635652536919224e-07, Validation Loss: 1.3183433181647432e-06\n",
      "Epoch 2554, Training Loss: 9.969918437491287e-07, Validation Loss: 1.077929650739334e-06\n",
      "Epoch 2555, Training Loss: 1.7410733335054829e-06, Validation Loss: 1.3709121227890184e-06\n",
      "Epoch 2556, Training Loss: 9.586634632796631e-07, Validation Loss: 1.1783086488408609e-06\n",
      "Epoch 2557, Training Loss: 8.894603524822742e-07, Validation Loss: 1.1248606517638457e-06\n",
      "Epoch 2558, Training Loss: 1.602141082912567e-06, Validation Loss: 1.2424776865506372e-06\n",
      "Epoch 2559, Training Loss: 1.134638296207413e-06, Validation Loss: 1.1558553142833354e-06\n",
      "Epoch 2560, Training Loss: 1.1599447589105694e-06, Validation Loss: 1.135808662737218e-06\n",
      "Epoch 2561, Training Loss: 1.0623347179716802e-06, Validation Loss: 1.0916989200705659e-06\n",
      "Epoch 2562, Training Loss: 1.1566546618269058e-06, Validation Loss: 1.3597048287068624e-06\n",
      "Epoch 2563, Training Loss: 3.509689804559457e-06, Validation Loss: 4.291578876105794e-06\n",
      "Epoch 2564, Training Loss: 1.1552598380148993e-06, Validation Loss: 1.1323028497667874e-06\n",
      "Epoch 2565, Training Loss: 8.377187441510614e-07, Validation Loss: 1.1011557967394416e-06\n",
      "Epoch 2566, Training Loss: 1.7515538957013632e-06, Validation Loss: 1.1919817403745902e-06\n",
      "Epoch 2567, Training Loss: 1.2101663742214441e-06, Validation Loss: 1.2392536531056096e-06\n",
      "Epoch 2568, Training Loss: 1.003573629532184e-06, Validation Loss: 1.1651762674759153e-06\n",
      "Epoch 2569, Training Loss: 2.085135520246695e-06, Validation Loss: 3.4959181850968125e-06\n",
      "Epoch 2570, Training Loss: 3.4245367714902386e-06, Validation Loss: 3.1483005322967354e-06\n",
      "Epoch 2571, Training Loss: 2.728602339630015e-06, Validation Loss: 2.0802062443378016e-06\n",
      "Epoch 2572, Training Loss: 1.3343391174203134e-06, Validation Loss: 1.0742522749172218e-06\n",
      "Epoch 2573, Training Loss: 2.6308989617973566e-06, Validation Loss: 1.1967885905847467e-06\n",
      "Epoch 2574, Training Loss: 1.3131134437571745e-06, Validation Loss: 1.87242201886614e-06\n",
      "Epoch 2575, Training Loss: 1.1145693861180916e-06, Validation Loss: 1.0866722960052438e-06\n",
      "Epoch 2576, Training Loss: 2.1693231246899813e-06, Validation Loss: 1.369066901776938e-06\n",
      "Epoch 2577, Training Loss: 1.0830558494490106e-06, Validation Loss: 1.0803458309464118e-06\n",
      "Epoch 2578, Training Loss: 1.8935983234769083e-06, Validation Loss: 1.362730917074717e-06\n",
      "Epoch 2579, Training Loss: 3.363628593433532e-06, Validation Loss: 2.250968251176861e-06\n",
      "Epoch 2580, Training Loss: 1.5766968317620922e-06, Validation Loss: 1.0710679587292785e-06\n",
      "Epoch 2581, Training Loss: 2.193240106862504e-06, Validation Loss: 1.6189468777150641e-06\n",
      "Epoch 2582, Training Loss: 1.333598902419908e-06, Validation Loss: 1.0989722735682514e-06\n",
      "Epoch 2583, Training Loss: 7.848573204682907e-07, Validation Loss: 1.0620457854080307e-06\n",
      "Epoch 2584, Training Loss: 1.3647317018694594e-06, Validation Loss: 1.4475647855708917e-06\n",
      "Epoch 2585, Training Loss: 1.204399268317502e-05, Validation Loss: 4.193062272426953e-06\n",
      "Epoch 2586, Training Loss: 9.88380179478554e-07, Validation Loss: 1.0995446026172315e-06\n",
      "Epoch 2587, Training Loss: 8.121360224322416e-07, Validation Loss: 1.0670553075695955e-06\n",
      "Epoch 2588, Training Loss: 9.659178203946794e-07, Validation Loss: 1.0569153192782497e-06\n",
      "Epoch 2589, Training Loss: 1.3210641327532358e-06, Validation Loss: 1.0830601561526243e-06\n",
      "Epoch 2590, Training Loss: 1.34168726617645e-06, Validation Loss: 1.4639025146456692e-06\n",
      "Epoch 2591, Training Loss: 1.0907394880632637e-06, Validation Loss: 1.0660418303240864e-06\n",
      "Epoch 2592, Training Loss: 1.3683869610758848e-06, Validation Loss: 1.0473709450191674e-06\n",
      "Epoch 2593, Training Loss: 3.843221747956704e-06, Validation Loss: 5.063364746104288e-06\n",
      "Epoch 2594, Training Loss: 7.75720479850861e-07, Validation Loss: 1.0655786692420585e-06\n",
      "Epoch 2595, Training Loss: 9.397774078934162e-07, Validation Loss: 1.0921976557192408e-06\n",
      "Epoch 2596, Training Loss: 1.2311252248764504e-06, Validation Loss: 1.0610255401750773e-06\n",
      "Epoch 2597, Training Loss: 1.6890338656594395e-06, Validation Loss: 1.0871135014072516e-06\n",
      "Epoch 2598, Training Loss: 1.4329574469229556e-06, Validation Loss: 1.4851521657318276e-06\n",
      "Epoch 2599, Training Loss: 9.77109721134184e-07, Validation Loss: 1.0566633394626097e-06\n",
      "Epoch 2600, Training Loss: 1.036960838973755e-06, Validation Loss: 1.1455112235415501e-06\n",
      "Epoch 2601, Training Loss: 7.044100129860453e-07, Validation Loss: 1.0760217160487007e-06\n",
      "Epoch 2602, Training Loss: 1.0321975878468947e-06, Validation Loss: 1.2647623286681033e-06\n",
      "Epoch 2603, Training Loss: 1.384548568239552e-06, Validation Loss: 1.2578888581223637e-06\n",
      "Epoch 2604, Training Loss: 1.2844063803640893e-06, Validation Loss: 1.105437537764872e-06\n",
      "Epoch 2605, Training Loss: 1.606589989933127e-06, Validation Loss: 1.3822941586095923e-06\n",
      "Epoch 2606, Training Loss: 1.901933046610793e-06, Validation Loss: 1.1411632260805378e-06\n",
      "Epoch 2607, Training Loss: 1.1057446727136266e-06, Validation Loss: 1.128485976212131e-06\n",
      "Epoch 2608, Training Loss: 2.87683064925659e-06, Validation Loss: 1.0775446030145607e-06\n",
      "Epoch 2609, Training Loss: 8.312143791044946e-07, Validation Loss: 1.1802913389633461e-06\n",
      "Epoch 2610, Training Loss: 1.7645220395934302e-06, Validation Loss: 1.1920498378852354e-06\n",
      "Epoch 2611, Training Loss: 9.628337238609674e-07, Validation Loss: 1.3807691481618749e-06\n",
      "Epoch 2612, Training Loss: 1.0062565252155764e-06, Validation Loss: 1.2415668639938408e-06\n",
      "Epoch 2613, Training Loss: 2.3956517907208763e-06, Validation Loss: 1.3424052438141612e-06\n",
      "Epoch 2614, Training Loss: 6.134266641311115e-07, Validation Loss: 1.0573306658124858e-06\n",
      "Epoch 2615, Training Loss: 1.0177381000175956e-06, Validation Loss: 1.0753622592537968e-06\n",
      "Epoch 2616, Training Loss: 8.953190899774199e-07, Validation Loss: 1.0506516488183792e-06\n",
      "Epoch 2617, Training Loss: 4.3392719817347825e-06, Validation Loss: 1.3660665985976469e-06\n",
      "Epoch 2618, Training Loss: 9.62976400842308e-07, Validation Loss: 1.1143810986291652e-06\n",
      "Epoch 2619, Training Loss: 2.312215883648605e-06, Validation Loss: 2.2452103191490155e-06\n",
      "Epoch 2620, Training Loss: 2.057220171991503e-06, Validation Loss: 1.4754077261722547e-06\n",
      "Epoch 2621, Training Loss: 1.4939371340005891e-06, Validation Loss: 2.4677634967116945e-06\n",
      "Epoch 2622, Training Loss: 9.626473911339417e-07, Validation Loss: 1.0916559904361962e-06\n",
      "Epoch 2623, Training Loss: 2.4597752599220257e-06, Validation Loss: 1.6228822283421764e-06\n",
      "Epoch 2624, Training Loss: 9.339697726318263e-07, Validation Loss: 1.0992394215252275e-06\n",
      "Epoch 2625, Training Loss: 9.839660606303369e-07, Validation Loss: 1.2088818642962668e-06\n",
      "Epoch 2626, Training Loss: 1.057103872881271e-05, Validation Loss: 6.4432121550574955e-06\n",
      "Epoch 2627, Training Loss: 7.2022146468953e-07, Validation Loss: 1.041740478746187e-06\n",
      "Epoch 2628, Training Loss: 2.19852699956391e-06, Validation Loss: 3.3773135922152726e-06\n",
      "Epoch 2629, Training Loss: 7.679610689592664e-07, Validation Loss: 1.1938863783279596e-06\n",
      "Epoch 2630, Training Loss: 1.3896777772970381e-06, Validation Loss: 1.0431358134248432e-06\n",
      "Epoch 2631, Training Loss: 1.0287860732205445e-06, Validation Loss: 1.0562731588133191e-06\n",
      "Epoch 2632, Training Loss: 2.3807720026525203e-06, Validation Loss: 1.6403355077017972e-06\n",
      "Epoch 2633, Training Loss: 8.061978178375284e-07, Validation Loss: 1.1027155093702536e-06\n",
      "Epoch 2634, Training Loss: 1.276891453017015e-06, Validation Loss: 1.0548222598737564e-06\n",
      "Epoch 2635, Training Loss: 9.71436861618713e-07, Validation Loss: 1.0399867700265427e-06\n",
      "Epoch 2636, Training Loss: 1.3246515209175413e-06, Validation Loss: 2.9384397356326497e-06\n",
      "Epoch 2637, Training Loss: 8.142346814565826e-07, Validation Loss: 1.0734453666558532e-06\n",
      "Epoch 2638, Training Loss: 1.4472584553004708e-06, Validation Loss: 1.0783453577042929e-06\n",
      "Epoch 2639, Training Loss: 3.3487485779915005e-06, Validation Loss: 2.4841154084005077e-06\n",
      "Epoch 2640, Training Loss: 7.40554605727084e-06, Validation Loss: 3.4962743015127155e-06\n",
      "Epoch 2641, Training Loss: 2.026914216912701e-06, Validation Loss: 1.4847889656121664e-06\n",
      "Epoch 2642, Training Loss: 1.1084878224210115e-06, Validation Loss: 1.2071131809563527e-06\n",
      "Epoch 2643, Training Loss: 8.919517995309434e-07, Validation Loss: 1.0436090622223888e-06\n",
      "Epoch 2644, Training Loss: 1.7012256421367056e-06, Validation Loss: 1.4163165143278546e-06\n",
      "Epoch 2645, Training Loss: 8.4806117683911e-07, Validation Loss: 1.0149010190949632e-06\n",
      "Epoch 2646, Training Loss: 1.6628295043119579e-06, Validation Loss: 1.5764403948069114e-06\n",
      "Epoch 2647, Training Loss: 1.1268585922152852e-06, Validation Loss: 1.032542766792477e-06\n",
      "Epoch 2648, Training Loss: 1.3082546956866281e-06, Validation Loss: 1.142544087954074e-06\n",
      "Epoch 2649, Training Loss: 7.242066999424424e-07, Validation Loss: 1.082798303323042e-06\n",
      "Epoch 2650, Training Loss: 1.7373379250784637e-06, Validation Loss: 2.0495624843892897e-06\n",
      "Epoch 2651, Training Loss: 2.427198523946572e-06, Validation Loss: 2.6680599659904973e-06\n",
      "Epoch 2652, Training Loss: 1.2384414276311873e-06, Validation Loss: 1.052256724488772e-06\n",
      "Epoch 2653, Training Loss: 9.371300393468118e-07, Validation Loss: 1.1156451977843113e-06\n",
      "Epoch 2654, Training Loss: 7.584519039482984e-07, Validation Loss: 1.2333165839310337e-06\n",
      "Epoch 2655, Training Loss: 1.16586113563244e-06, Validation Loss: 1.083668523386072e-06\n",
      "Epoch 2656, Training Loss: 1.5382387346107862e-06, Validation Loss: 1.1455889415292926e-06\n",
      "Epoch 2657, Training Loss: 4.330659066908993e-06, Validation Loss: 4.4094895042545215e-06\n",
      "Epoch 2658, Training Loss: 2.89584477286553e-06, Validation Loss: 1.1052642623694245e-06\n",
      "Epoch 2659, Training Loss: 1.1834071074190433e-06, Validation Loss: 1.7515996742047859e-06\n",
      "Epoch 2660, Training Loss: 9.837294783210382e-07, Validation Loss: 1.0623806713500792e-06\n",
      "Epoch 2661, Training Loss: 9.634114803702687e-07, Validation Loss: 2.180008554188036e-06\n",
      "Epoch 2662, Training Loss: 1.0251833373331465e-06, Validation Loss: 1.0962693896388397e-06\n",
      "Epoch 2663, Training Loss: 1.0897532547460287e-06, Validation Loss: 1.0251274991738555e-06\n",
      "Epoch 2664, Training Loss: 1.6360380641344818e-06, Validation Loss: 1.8119412721821072e-06\n",
      "Epoch 2665, Training Loss: 6.927122740307823e-06, Validation Loss: 2.02492183046155e-06\n",
      "Epoch 2666, Training Loss: 2.2880590222484898e-06, Validation Loss: 1.5296634986443215e-06\n",
      "Epoch 2667, Training Loss: 4.679102403315483e-06, Validation Loss: 1.2564748425963217e-06\n",
      "Epoch 2668, Training Loss: 6.550619104928046e-07, Validation Loss: 1.0472745809417986e-06\n",
      "Epoch 2669, Training Loss: 1.5220761042655795e-06, Validation Loss: 1.669764475997335e-06\n",
      "Epoch 2670, Training Loss: 9.53990820562467e-07, Validation Loss: 1.082953332524125e-06\n",
      "Epoch 2671, Training Loss: 9.112556540458172e-07, Validation Loss: 1.0078746987682668e-06\n",
      "Epoch 2672, Training Loss: 2.796689841488842e-06, Validation Loss: 2.8859216809575577e-06\n",
      "Epoch 2673, Training Loss: 7.097218031049124e-07, Validation Loss: 1.0299056523279572e-06\n",
      "Epoch 2674, Training Loss: 1.5335356238210807e-06, Validation Loss: 1.1074120284957313e-06\n",
      "Epoch 2675, Training Loss: 1.0250867035210831e-06, Validation Loss: 1.044873614676646e-06\n",
      "Epoch 2676, Training Loss: 9.775339094630908e-06, Validation Loss: 1.0605670213176584e-05\n",
      "Epoch 2677, Training Loss: 9.880976676868158e-07, Validation Loss: 1.0143429403652375e-06\n",
      "Epoch 2678, Training Loss: 1.4598440429836046e-06, Validation Loss: 1.063193576954198e-06\n",
      "Epoch 2679, Training Loss: 8.571120702072221e-07, Validation Loss: 1.1301277012242594e-06\n",
      "Epoch 2680, Training Loss: 9.714647148939548e-07, Validation Loss: 1.1029741114441911e-06\n",
      "Epoch 2681, Training Loss: 1.4494833067146828e-06, Validation Loss: 1.7963253375814843e-06\n",
      "Epoch 2682, Training Loss: 1.1418587746447884e-06, Validation Loss: 1.0604522143749372e-06\n",
      "Epoch 2683, Training Loss: 2.0857482923020143e-06, Validation Loss: 2.1008922791676754e-06\n",
      "Epoch 2684, Training Loss: 2.9082198125252035e-06, Validation Loss: 1.173451026776892e-06\n",
      "Epoch 2685, Training Loss: 9.892626167129492e-07, Validation Loss: 1.2805252619791702e-06\n",
      "Epoch 2686, Training Loss: 1.0704701708164066e-05, Validation Loss: 5.0104925895150924e-06\n",
      "Epoch 2687, Training Loss: 1.243233896275342e-06, Validation Loss: 1.14582059638312e-06\n",
      "Epoch 2688, Training Loss: 1.6241110643022694e-06, Validation Loss: 1.0486809707602733e-06\n",
      "Epoch 2689, Training Loss: 1.4205423894964042e-06, Validation Loss: 1.0878666428545481e-06\n",
      "Epoch 2690, Training Loss: 1.55397538037505e-06, Validation Loss: 1.2492191302907527e-06\n",
      "Epoch 2691, Training Loss: 1.6479682471981505e-06, Validation Loss: 3.3882607816745213e-06\n",
      "Epoch 2692, Training Loss: 1.3287487945490284e-06, Validation Loss: 1.268730427802589e-06\n",
      "Epoch 2693, Training Loss: 1.5547783505098778e-06, Validation Loss: 1.0114199312014373e-06\n",
      "Epoch 2694, Training Loss: 1.1646286566247e-06, Validation Loss: 1.1970092353752247e-06\n",
      "Epoch 2695, Training Loss: 9.435650554223685e-07, Validation Loss: 1.0656401432271965e-06\n",
      "Epoch 2696, Training Loss: 7.752464625809807e-07, Validation Loss: 1.0013865612505747e-06\n",
      "Epoch 2697, Training Loss: 1.1848183021356817e-06, Validation Loss: 1.0304091934178857e-06\n",
      "Epoch 2698, Training Loss: 8.773234867476276e-07, Validation Loss: 1.022629234535523e-06\n",
      "Epoch 2699, Training Loss: 4.5422293624142185e-06, Validation Loss: 1.0945116024944072e-06\n",
      "Epoch 2700, Training Loss: 1.0451107073095045e-06, Validation Loss: 1.7387147624741508e-06\n",
      "Epoch 2701, Training Loss: 1.2472902426452492e-06, Validation Loss: 2.2785272650560346e-06\n",
      "Epoch 2702, Training Loss: 7.303116831280931e-07, Validation Loss: 1.0070231014419935e-06\n",
      "Epoch 2703, Training Loss: 8.803381206234917e-07, Validation Loss: 1.183938789621908e-06\n",
      "Epoch 2704, Training Loss: 1.7450336144975154e-06, Validation Loss: 1.318556638119508e-06\n",
      "Epoch 2705, Training Loss: 1.1347987083354383e-06, Validation Loss: 1.0669189641037078e-06\n",
      "Epoch 2706, Training Loss: 1.303611838920915e-06, Validation Loss: 1.0133047883691631e-06\n",
      "Epoch 2707, Training Loss: 1.2043842616549227e-06, Validation Loss: 1.3493583137923955e-06\n",
      "Epoch 2708, Training Loss: 1.5566474758088589e-06, Validation Loss: 1.4879574105382681e-06\n",
      "Epoch 2709, Training Loss: 9.387512136527221e-07, Validation Loss: 1.0753249525921446e-06\n",
      "Epoch 2710, Training Loss: 2.0471397874644026e-06, Validation Loss: 1.1276311882321716e-06\n",
      "Epoch 2711, Training Loss: 2.1898806608078303e-06, Validation Loss: 1.6273900439717957e-06\n",
      "Epoch 2712, Training Loss: 1.6181714954655035e-06, Validation Loss: 2.369370269320101e-06\n",
      "Epoch 2713, Training Loss: 9.346333627036074e-07, Validation Loss: 1.2092219530404112e-06\n",
      "Epoch 2714, Training Loss: 7.726309831923572e-07, Validation Loss: 1.0385565119461362e-06\n",
      "Epoch 2715, Training Loss: 3.1286554076359607e-06, Validation Loss: 1.0254577071197679e-06\n",
      "Epoch 2716, Training Loss: 1.4353354345075786e-06, Validation Loss: 1.4361640489896324e-06\n",
      "Epoch 2717, Training Loss: 6.024565664120018e-07, Validation Loss: 1.0356892210997693e-06\n",
      "Epoch 2718, Training Loss: 4.192917913314886e-06, Validation Loss: 4.539831427496355e-06\n",
      "Epoch 2719, Training Loss: 8.536275686310546e-07, Validation Loss: 9.943275103384744e-07\n",
      "Epoch 2720, Training Loss: 1.0112742074852576e-06, Validation Loss: 1.1080573537270586e-06\n",
      "Epoch 2721, Training Loss: 1.4978811577748274e-06, Validation Loss: 2.1779318439645752e-06\n",
      "Epoch 2722, Training Loss: 7.48791421756323e-07, Validation Loss: 1.010846714200325e-06\n",
      "Epoch 2723, Training Loss: 6.436095532080799e-07, Validation Loss: 1.1400251480608858e-06\n",
      "Epoch 2724, Training Loss: 1.1586286063902662e-06, Validation Loss: 1.276240237069606e-06\n",
      "Epoch 2725, Training Loss: 2.2097103737905854e-06, Validation Loss: 1.0781735157043569e-06\n",
      "Epoch 2726, Training Loss: 5.934015916864155e-06, Validation Loss: 3.269370627731984e-06\n",
      "Epoch 2727, Training Loss: 1.0284859399689594e-06, Validation Loss: 1.0789343677622886e-06\n",
      "Epoch 2728, Training Loss: 6.506134013761766e-07, Validation Loss: 1.2459148810060926e-06\n",
      "Epoch 2729, Training Loss: 1.5182888546405593e-06, Validation Loss: 1.2454888696327103e-06\n",
      "Epoch 2730, Training Loss: 6.455351808654086e-07, Validation Loss: 1.2217909848062568e-06\n",
      "Epoch 2731, Training Loss: 4.053686097904574e-06, Validation Loss: 1.8511554943360799e-06\n",
      "Epoch 2732, Training Loss: 1.9707422325154766e-06, Validation Loss: 1.0861678792732227e-06\n",
      "Epoch 2733, Training Loss: 1.4834367902949452e-06, Validation Loss: 1.102296947381222e-06\n",
      "Epoch 2734, Training Loss: 8.637934456601215e-07, Validation Loss: 1.0465429282981615e-06\n",
      "Epoch 2735, Training Loss: 2.392693204456009e-06, Validation Loss: 3.093387866414513e-06\n",
      "Epoch 2736, Training Loss: 1.3442197541735368e-06, Validation Loss: 1.1367799324770129e-06\n",
      "Epoch 2737, Training Loss: 1.2371599495963892e-06, Validation Loss: 1.1196710116350306e-06\n",
      "Epoch 2738, Training Loss: 9.233067430614028e-07, Validation Loss: 1.127677226513639e-06\n",
      "Epoch 2739, Training Loss: 1.4203654927769094e-06, Validation Loss: 1.027924259466426e-06\n",
      "Epoch 2740, Training Loss: 1.431745999980194e-06, Validation Loss: 1.6281043184599011e-06\n",
      "Epoch 2741, Training Loss: 1.6624397858322482e-06, Validation Loss: 1.7887899382762402e-06\n",
      "Epoch 2742, Training Loss: 9.670129657024518e-07, Validation Loss: 1.149675674146151e-06\n",
      "Epoch 2743, Training Loss: 6.823399303357292e-07, Validation Loss: 1.2128342310189179e-06\n",
      "Epoch 2744, Training Loss: 7.041644494165666e-07, Validation Loss: 1.0165626251833532e-06\n",
      "Epoch 2745, Training Loss: 2.9050802368146833e-06, Validation Loss: 1.7788860739262424e-06\n",
      "Epoch 2746, Training Loss: 1.0567403023742372e-06, Validation Loss: 1.0082197564237516e-06\n",
      "Epoch 2747, Training Loss: 1.5031295106382458e-06, Validation Loss: 3.0476365819380893e-06\n",
      "Epoch 2748, Training Loss: 9.012373993755318e-07, Validation Loss: 1.068629409550699e-06\n",
      "Epoch 2749, Training Loss: 2.0503673567873193e-06, Validation Loss: 1.7356922856547239e-06\n",
      "Epoch 2750, Training Loss: 1.2676271126110805e-06, Validation Loss: 1.1104999255731872e-06\n",
      "Epoch 2751, Training Loss: 6.195734840730438e-07, Validation Loss: 9.92038510039898e-07\n",
      "Epoch 2752, Training Loss: 1.6987436310955673e-06, Validation Loss: 1.0661548752294408e-06\n",
      "Epoch 2753, Training Loss: 2.189605083913193e-06, Validation Loss: 1.454793295223026e-06\n",
      "Epoch 2754, Training Loss: 7.134564725674863e-07, Validation Loss: 1.0384590345342899e-06\n",
      "Epoch 2755, Training Loss: 8.302697551698657e-07, Validation Loss: 9.99978226445819e-07\n",
      "Epoch 2756, Training Loss: 8.961432058640639e-07, Validation Loss: 1.29210511506974e-06\n",
      "Epoch 2757, Training Loss: 8.94906179382815e-07, Validation Loss: 1.0372709502366801e-06\n",
      "Epoch 2758, Training Loss: 1.7783285102268565e-06, Validation Loss: 1.1995113892619994e-06\n",
      "Epoch 2759, Training Loss: 1.468559457862284e-05, Validation Loss: 8.884249816862399e-06\n",
      "Epoch 2760, Training Loss: 9.10235303308582e-06, Validation Loss: 1.4793966972325842e-05\n",
      "Epoch 2761, Training Loss: 6.267258640946238e-07, Validation Loss: 9.973080452231917e-07\n",
      "Epoch 2762, Training Loss: 7.385272056126269e-07, Validation Loss: 1.2270957904258037e-06\n",
      "Epoch 2763, Training Loss: 1.1453216757217888e-06, Validation Loss: 1.0062851094899436e-06\n",
      "Epoch 2764, Training Loss: 1.1158485904161353e-06, Validation Loss: 1.0282148893673214e-06\n",
      "Epoch 2765, Training Loss: 9.508904668109608e-07, Validation Loss: 1.0155495791512954e-06\n",
      "Epoch 2766, Training Loss: 2.009292529692175e-06, Validation Loss: 1.2718613184180033e-06\n",
      "Epoch 2767, Training Loss: 1.1877860970344045e-06, Validation Loss: 1.1941729833889458e-06\n",
      "Epoch 2768, Training Loss: 1.6517742551513948e-06, Validation Loss: 2.833695755835851e-06\n",
      "Epoch 2769, Training Loss: 1.75855325323937e-06, Validation Loss: 1.2310134899041497e-06\n",
      "Epoch 2770, Training Loss: 6.744579650330706e-07, Validation Loss: 1.0326721326381845e-06\n",
      "Epoch 2771, Training Loss: 1.6423018678324297e-06, Validation Loss: 1.071229853130914e-06\n",
      "Epoch 2772, Training Loss: 1.0553821994108148e-06, Validation Loss: 1.2162732937039362e-06\n",
      "Epoch 2773, Training Loss: 6.602244297937432e-07, Validation Loss: 1.168807521561732e-06\n",
      "Epoch 2774, Training Loss: 2.5691322207421763e-06, Validation Loss: 2.0262584190960836e-06\n",
      "Epoch 2775, Training Loss: 9.839434369496303e-07, Validation Loss: 1.0096317657753737e-06\n",
      "Epoch 2776, Training Loss: 1.3303346122484072e-06, Validation Loss: 1.0728236216677334e-06\n",
      "Epoch 2777, Training Loss: 7.788636935401883e-07, Validation Loss: 9.79401133114372e-07\n",
      "Epoch 2778, Training Loss: 1.1249080671404954e-06, Validation Loss: 1.044691591368684e-06\n",
      "Epoch 2779, Training Loss: 9.069555062524159e-07, Validation Loss: 1.1186085864129226e-06\n",
      "Epoch 2780, Training Loss: 1.208908997796243e-06, Validation Loss: 1.0040349077212195e-06\n",
      "Epoch 2781, Training Loss: 1.6839188674566685e-06, Validation Loss: 1.2168454574725294e-06\n",
      "Epoch 2782, Training Loss: 6.421968237191322e-07, Validation Loss: 1.0698208141194723e-06\n",
      "Epoch 2783, Training Loss: 1.3137039331923006e-06, Validation Loss: 1.2221109987720434e-06\n",
      "Epoch 2784, Training Loss: 1.0394271612312878e-06, Validation Loss: 1.023604637371165e-06\n",
      "Epoch 2785, Training Loss: 1.8821219782694243e-06, Validation Loss: 1.0922370491136614e-06\n",
      "Epoch 2786, Training Loss: 1.5361929399659857e-06, Validation Loss: 1.1470801062468282e-06\n",
      "Epoch 2787, Training Loss: 8.291391964121431e-07, Validation Loss: 1.1138731237525653e-06\n",
      "Epoch 2788, Training Loss: 6.551358069373237e-07, Validation Loss: 1.035847717568373e-06\n",
      "Epoch 2789, Training Loss: 1.0967199841616093e-06, Validation Loss: 1.3375191446455828e-06\n",
      "Epoch 2790, Training Loss: 2.4437399588350672e-06, Validation Loss: 1.5076105077600828e-06\n",
      "Epoch 2791, Training Loss: 1.5468343690372421e-06, Validation Loss: 1.036443053357532e-06\n",
      "Epoch 2792, Training Loss: 1.0309238405170618e-06, Validation Loss: 1.0576983326662775e-06\n",
      "Epoch 2793, Training Loss: 1.5283005723176757e-06, Validation Loss: 1.0236697699725297e-06\n",
      "Epoch 2794, Training Loss: 1.179958417196758e-06, Validation Loss: 1.272056833710565e-06\n",
      "Epoch 2795, Training Loss: 1.1084705420216778e-06, Validation Loss: 1.055231196135821e-06\n",
      "Epoch 2796, Training Loss: 1.981245804927312e-05, Validation Loss: 4.36218727714546e-06\n",
      "Epoch 2797, Training Loss: 1.0731105248851236e-06, Validation Loss: 1.23666840753668e-06\n",
      "Epoch 2798, Training Loss: 2.1005921553296503e-06, Validation Loss: 1.557046011535572e-06\n",
      "Epoch 2799, Training Loss: 9.145200010607368e-07, Validation Loss: 9.846804725202095e-07\n",
      "Epoch 2800, Training Loss: 5.368418896978255e-06, Validation Loss: 5.832722630205521e-06\n",
      "Epoch 2801, Training Loss: 1.6960078710326343e-06, Validation Loss: 1.394715179888344e-06\n",
      "Epoch 2802, Training Loss: 1.2280635246497695e-06, Validation Loss: 1.0554269737408474e-06\n",
      "Epoch 2803, Training Loss: 9.020281481753045e-07, Validation Loss: 1.025356615806104e-06\n",
      "Epoch 2804, Training Loss: 9.24789105738455e-07, Validation Loss: 9.753287755464357e-07\n",
      "Epoch 2805, Training Loss: 1.2058791298841243e-06, Validation Loss: 1.0392522629019228e-06\n",
      "Epoch 2806, Training Loss: 7.771370178488723e-07, Validation Loss: 1.036493800229205e-06\n",
      "Epoch 2807, Training Loss: 1.9654896732390625e-06, Validation Loss: 1.0334919727773442e-06\n",
      "Epoch 2808, Training Loss: 1.2715884167846525e-06, Validation Loss: 1.4586677622038182e-06\n",
      "Epoch 2809, Training Loss: 7.171290690166643e-07, Validation Loss: 1.0527304345507484e-06\n",
      "Epoch 2810, Training Loss: 7.349105999310268e-07, Validation Loss: 1.1359914737066943e-06\n",
      "Epoch 2811, Training Loss: 6.141629000921967e-07, Validation Loss: 1.0725441928168334e-06\n",
      "Epoch 2812, Training Loss: 1.2358782441879157e-06, Validation Loss: 1.214629061245328e-06\n",
      "Epoch 2813, Training Loss: 1.4833121895208023e-06, Validation Loss: 1.182171349596009e-06\n",
      "Epoch 2814, Training Loss: 8.248074436778552e-07, Validation Loss: 1.0221750909196745e-06\n",
      "Epoch 2815, Training Loss: 1.729249788695597e-06, Validation Loss: 1.3911216106358424e-06\n",
      "Epoch 2816, Training Loss: 1.6428056142103742e-06, Validation Loss: 1.0066905451749684e-06\n",
      "Epoch 2817, Training Loss: 1.8307108575754683e-06, Validation Loss: 1.5850864862760162e-06\n",
      "Epoch 2818, Training Loss: 1.427074494131375e-06, Validation Loss: 1.041227513415677e-06\n",
      "Epoch 2819, Training Loss: 1.0501746601221384e-06, Validation Loss: 9.979750991229235e-07\n",
      "Epoch 2820, Training Loss: 1.4619702142226743e-06, Validation Loss: 1.068859343714411e-06\n",
      "Epoch 2821, Training Loss: 1.454753373764106e-06, Validation Loss: 9.8408565393375e-07\n",
      "Epoch 2822, Training Loss: 9.44313796935603e-07, Validation Loss: 1.0968022782256e-06\n",
      "Epoch 2823, Training Loss: 2.4433093130937777e-06, Validation Loss: 3.788284351928738e-06\n",
      "Epoch 2824, Training Loss: 1.2173168215667829e-06, Validation Loss: 1.3988442970582531e-06\n",
      "Epoch 2825, Training Loss: 1.2539451290649595e-06, Validation Loss: 9.9295204352089e-07\n",
      "Epoch 2826, Training Loss: 1.1972274478466716e-06, Validation Loss: 1.255449279788255e-06\n",
      "Epoch 2827, Training Loss: 1.0476686611582409e-06, Validation Loss: 1.126729817070456e-06\n",
      "Epoch 2828, Training Loss: 7.42341853765538e-07, Validation Loss: 1.1746322965623481e-06\n",
      "Epoch 2829, Training Loss: 2.3720590434095357e-06, Validation Loss: 1.6854193939594367e-06\n",
      "Epoch 2830, Training Loss: 9.45373983540776e-07, Validation Loss: 1.0300687192608792e-06\n",
      "Epoch 2831, Training Loss: 1.9722635897778673e-06, Validation Loss: 1.4140203895937708e-06\n",
      "Epoch 2832, Training Loss: 1.8785913198371418e-06, Validation Loss: 1.3677056054443797e-06\n",
      "Epoch 2833, Training Loss: 7.056857498355384e-07, Validation Loss: 9.949400117539493e-07\n",
      "Epoch 2834, Training Loss: 1.1665645160974236e-06, Validation Loss: 1.0496865207022412e-06\n",
      "Epoch 2835, Training Loss: 9.079830078917439e-07, Validation Loss: 1.0942072802087061e-06\n",
      "Epoch 2836, Training Loss: 9.969575103241368e-07, Validation Loss: 1.1601905351902938e-06\n",
      "Epoch 2837, Training Loss: 3.3691317185002845e-06, Validation Loss: 2.9098243335196875e-06\n",
      "Epoch 2838, Training Loss: 1.168116227745486e-06, Validation Loss: 9.852855739725228e-07\n",
      "Epoch 2839, Training Loss: 7.683535727665003e-07, Validation Loss: 9.767360987555716e-07\n",
      "Epoch 2840, Training Loss: 1.985484232136514e-06, Validation Loss: 1.5486148026721607e-06\n",
      "Epoch 2841, Training Loss: 9.513388249615673e-06, Validation Loss: 8.595551998429757e-06\n",
      "Epoch 2842, Training Loss: 1.0039149174190243e-06, Validation Loss: 1.438472557623708e-06\n",
      "Epoch 2843, Training Loss: 1.420870830770582e-06, Validation Loss: 3.103382441275093e-06\n",
      "Epoch 2844, Training Loss: 2.541368530728505e-06, Validation Loss: 2.639712090006174e-06\n",
      "Epoch 2845, Training Loss: 3.4148895338148577e-06, Validation Loss: 4.157420365517728e-06\n",
      "Epoch 2846, Training Loss: 1.0187441148445942e-06, Validation Loss: 9.889809767616736e-07\n",
      "Epoch 2847, Training Loss: 1.425334858140559e-06, Validation Loss: 1.312336702334401e-06\n",
      "Epoch 2848, Training Loss: 1.4925033156032441e-06, Validation Loss: 2.215298609748072e-06\n",
      "Epoch 2849, Training Loss: 1.2503346624725964e-06, Validation Loss: 1.33949374779607e-06\n",
      "Epoch 2850, Training Loss: 1.1352420870025526e-06, Validation Loss: 1.0287189699266393e-06\n",
      "Epoch 2851, Training Loss: 6.291848535511235e-07, Validation Loss: 9.769750255843533e-07\n",
      "Epoch 2852, Training Loss: 1.4655170161859132e-06, Validation Loss: 1.457116294584191e-06\n",
      "Epoch 2853, Training Loss: 7.717916332694585e-07, Validation Loss: 1.0227591125150901e-06\n",
      "Epoch 2854, Training Loss: 1.2218964684507228e-06, Validation Loss: 1.0053135298267948e-06\n",
      "Epoch 2855, Training Loss: 1.2237489954713965e-06, Validation Loss: 9.98909406882179e-07\n",
      "Epoch 2856, Training Loss: 8.583223802816065e-07, Validation Loss: 1.011987836058336e-06\n",
      "Epoch 2857, Training Loss: 4.989950866729487e-06, Validation Loss: 2.7328346790176755e-06\n",
      "Epoch 2858, Training Loss: 1.9186506960977567e-06, Validation Loss: 2.6269082771794935e-06\n",
      "Epoch 2859, Training Loss: 1.702546569504193e-06, Validation Loss: 1.0057742410298819e-06\n",
      "Epoch 2860, Training Loss: 1.159859948529629e-06, Validation Loss: 1.306148657155515e-06\n",
      "Epoch 2861, Training Loss: 2.0791912902495824e-06, Validation Loss: 1.8990873904782625e-06\n",
      "Epoch 2862, Training Loss: 1.1554989214346278e-06, Validation Loss: 1.0398841555264892e-06\n",
      "Epoch 2863, Training Loss: 1.51084702793014e-06, Validation Loss: 1.6570169124490673e-06\n",
      "Epoch 2864, Training Loss: 1.2218310985190328e-06, Validation Loss: 1.0415929163083342e-06\n",
      "Epoch 2865, Training Loss: 1.0235750096398988e-06, Validation Loss: 1.1372576362417391e-06\n",
      "Epoch 2866, Training Loss: 1.950461410160642e-06, Validation Loss: 1.5458979624162086e-06\n",
      "Epoch 2867, Training Loss: 8.093965107036638e-07, Validation Loss: 1.078501884204476e-06\n",
      "Epoch 2868, Training Loss: 1.7182180727104424e-06, Validation Loss: 2.250578251738597e-06\n",
      "Epoch 2869, Training Loss: 1.0702422059694072e-06, Validation Loss: 1.302318638831792e-06\n",
      "Epoch 2870, Training Loss: 6.194968591444194e-07, Validation Loss: 1.014242967635921e-06\n",
      "Epoch 2871, Training Loss: 1.0939734238490928e-06, Validation Loss: 1.5551774539466595e-06\n",
      "Epoch 2872, Training Loss: 1.7416199398212484e-06, Validation Loss: 9.744151457574856e-07\n",
      "Epoch 2873, Training Loss: 8.176075425581075e-07, Validation Loss: 9.572853025547026e-07\n",
      "Epoch 2874, Training Loss: 9.694526852399576e-07, Validation Loss: 9.865058193351678e-07\n",
      "Epoch 2875, Training Loss: 2.113766640832182e-06, Validation Loss: 2.876342392529255e-06\n",
      "Epoch 2876, Training Loss: 1.3443041098071262e-06, Validation Loss: 9.700115920008866e-07\n",
      "Epoch 2877, Training Loss: 1.001887426355097e-06, Validation Loss: 1.1638940991737411e-06\n",
      "Epoch 2878, Training Loss: 1.978481350306538e-06, Validation Loss: 1.3687947926929114e-06\n",
      "Epoch 2879, Training Loss: 7.797493708494585e-07, Validation Loss: 9.58723939608491e-07\n",
      "Epoch 2880, Training Loss: 8.547546599402267e-07, Validation Loss: 1.019285761300519e-06\n",
      "Epoch 2881, Training Loss: 8.908100426197052e-07, Validation Loss: 9.687441798157768e-07\n",
      "Epoch 2882, Training Loss: 1.2566856639750767e-06, Validation Loss: 1.079064285025365e-06\n",
      "Epoch 2883, Training Loss: 1.4865190678392537e-06, Validation Loss: 1.612806389116225e-06\n",
      "Epoch 2884, Training Loss: 1.6247699932137039e-06, Validation Loss: 1.4571777977151584e-06\n",
      "Epoch 2885, Training Loss: 1.3511332781490637e-06, Validation Loss: 1.0689348060683986e-06\n",
      "Epoch 2886, Training Loss: 1.5232953956001438e-06, Validation Loss: 2.301018859760203e-06\n",
      "Epoch 2887, Training Loss: 1.115639634008403e-06, Validation Loss: 1.0091455765916273e-06\n",
      "Epoch 2888, Training Loss: 8.978831829153933e-07, Validation Loss: 1.0563677509690835e-06\n",
      "Epoch 2889, Training Loss: 6.68225084154983e-07, Validation Loss: 9.661848841727072e-07\n",
      "Epoch 2890, Training Loss: 1.3076396498945542e-06, Validation Loss: 1.1136675232789058e-06\n",
      "Epoch 2891, Training Loss: 7.106654607014207e-07, Validation Loss: 9.765904644682102e-07\n",
      "Epoch 2892, Training Loss: 1.0220235253655119e-06, Validation Loss: 9.873885293817676e-07\n",
      "Epoch 2893, Training Loss: 5.925969048803381e-07, Validation Loss: 9.592794192905588e-07\n",
      "Epoch 2894, Training Loss: 2.7246151148574427e-06, Validation Loss: 1.545208249353891e-06\n",
      "Epoch 2895, Training Loss: 1.742393578751944e-06, Validation Loss: 1.2672416519568854e-06\n",
      "Epoch 2896, Training Loss: 2.1024322904850123e-06, Validation Loss: 1.2178081377010565e-06\n",
      "Epoch 2897, Training Loss: 7.929461389721837e-07, Validation Loss: 1.032212689369192e-06\n",
      "Epoch 2898, Training Loss: 2.2324163637676975e-06, Validation Loss: 1.8572979467483197e-06\n",
      "Epoch 2899, Training Loss: 1.825702611313318e-06, Validation Loss: 1.029748437008638e-06\n",
      "Epoch 2900, Training Loss: 1.9956751202698797e-06, Validation Loss: 2.499454037270028e-06\n",
      "Epoch 2901, Training Loss: 6.243236612135661e-07, Validation Loss: 9.62901912429701e-07\n",
      "Epoch 2902, Training Loss: 1.4725410437677056e-06, Validation Loss: 1.2904548966732718e-06\n",
      "Epoch 2903, Training Loss: 9.573431043463643e-07, Validation Loss: 9.594282664982958e-07\n",
      "Epoch 2904, Training Loss: 1.6695569229341345e-06, Validation Loss: 1.019576772450587e-06\n",
      "Epoch 2905, Training Loss: 1.4666893548564985e-06, Validation Loss: 1.7920421761947026e-06\n",
      "Epoch 2906, Training Loss: 8.449402457699762e-07, Validation Loss: 9.681510924191726e-07\n",
      "Epoch 2907, Training Loss: 8.20880131868762e-07, Validation Loss: 9.876856848709424e-07\n",
      "Epoch 2908, Training Loss: 2.0104753275518306e-06, Validation Loss: 9.739010542393685e-07\n",
      "Epoch 2909, Training Loss: 1.690690055511368e-06, Validation Loss: 1.1903739210135722e-06\n",
      "Epoch 2910, Training Loss: 1.9144713405694347e-06, Validation Loss: 1.6478792235250745e-06\n",
      "Epoch 2911, Training Loss: 8.273840421679779e-07, Validation Loss: 1.545669957308138e-06\n",
      "Epoch 2912, Training Loss: 9.110585210692079e-07, Validation Loss: 1.0152913822224898e-06\n",
      "Epoch 2913, Training Loss: 1.0269319545841427e-06, Validation Loss: 1.2893111335876138e-06\n",
      "Epoch 2914, Training Loss: 2.9568395802925806e-06, Validation Loss: 2.6124414619800613e-06\n",
      "Epoch 2915, Training Loss: 7.536881412306684e-07, Validation Loss: 9.525779652576719e-07\n",
      "Epoch 2916, Training Loss: 1.2780628821928985e-06, Validation Loss: 1.0580926889534641e-06\n",
      "Epoch 2917, Training Loss: 7.486532922484912e-07, Validation Loss: 1.0168151363220326e-06\n",
      "Epoch 2918, Training Loss: 6.227096491784323e-07, Validation Loss: 9.89211283485118e-07\n",
      "Epoch 2919, Training Loss: 1.2066483350281487e-06, Validation Loss: 9.713664938108833e-07\n",
      "Epoch 2920, Training Loss: 3.6001506487082224e-06, Validation Loss: 2.9708553454977724e-06\n",
      "Epoch 2921, Training Loss: 6.289908469625516e-07, Validation Loss: 1.032981922210806e-06\n",
      "Epoch 2922, Training Loss: 1.452438255000743e-06, Validation Loss: 1.554854235387877e-06\n",
      "Epoch 2923, Training Loss: 4.2332112570875324e-06, Validation Loss: 1.618407141487661e-06\n",
      "Epoch 2924, Training Loss: 1.5036957847769372e-06, Validation Loss: 1.0761733054988228e-06\n",
      "Epoch 2925, Training Loss: 1.6487364291606355e-06, Validation Loss: 1.3437980390040284e-06\n",
      "Epoch 2926, Training Loss: 1.5480970887438161e-06, Validation Loss: 2.0527735466730173e-06\n",
      "Epoch 2927, Training Loss: 1.4095430742600001e-06, Validation Loss: 1.4707468076817436e-06\n",
      "Epoch 2928, Training Loss: 1.2208364523758064e-06, Validation Loss: 2.1040380938959907e-06\n",
      "Epoch 2929, Training Loss: 1.448323473596247e-06, Validation Loss: 1.1861466931888897e-06\n",
      "Epoch 2930, Training Loss: 2.9625030038005207e-06, Validation Loss: 2.745078170696065e-06\n",
      "Epoch 2931, Training Loss: 9.559213367538177e-07, Validation Loss: 2.0875303197250207e-06\n",
      "Epoch 2932, Training Loss: 1.4437976005865494e-06, Validation Loss: 1.507753547887596e-06\n",
      "Epoch 2933, Training Loss: 9.998293535318226e-07, Validation Loss: 1.2214153739941643e-06\n",
      "Epoch 2934, Training Loss: 2.897158992709592e-06, Validation Loss: 2.9587465299990036e-06\n",
      "Epoch 2935, Training Loss: 1.341349161521066e-06, Validation Loss: 1.405792697548761e-06\n",
      "Epoch 2936, Training Loss: 9.608393156668171e-07, Validation Loss: 1.4833969021455447e-06\n",
      "Epoch 2937, Training Loss: 8.90933449682052e-07, Validation Loss: 9.542509110650346e-07\n",
      "Epoch 2938, Training Loss: 7.972645335030393e-07, Validation Loss: 9.86346242298516e-07\n",
      "Epoch 2939, Training Loss: 2.983373178722104e-06, Validation Loss: 3.2875719454843337e-06\n",
      "Epoch 2940, Training Loss: 2.1793500764033524e-06, Validation Loss: 1.2064440017644638e-06\n",
      "Epoch 2941, Training Loss: 2.554802267695777e-05, Validation Loss: 8.479385214857201e-06\n",
      "Epoch 2942, Training Loss: 6.814213520556223e-06, Validation Loss: 2.2728724911613214e-06\n",
      "Epoch 2943, Training Loss: 1.2538071132439654e-06, Validation Loss: 1.0126647693080596e-06\n",
      "Epoch 2944, Training Loss: 6.040219432179583e-06, Validation Loss: 1.374135050140768e-06\n",
      "Epoch 2945, Training Loss: 8.153976409630559e-07, Validation Loss: 9.563809063817476e-07\n",
      "Epoch 2946, Training Loss: 1.754119011820876e-06, Validation Loss: 2.888069483832954e-06\n",
      "Epoch 2947, Training Loss: 1.922470573845203e-06, Validation Loss: 2.1102066454028074e-06\n",
      "Epoch 2948, Training Loss: 6.857885637145955e-07, Validation Loss: 1.1214170294763089e-06\n",
      "Epoch 2949, Training Loss: 7.927248475425586e-07, Validation Loss: 9.900919115057295e-07\n",
      "Epoch 2950, Training Loss: 1.0043343081633793e-06, Validation Loss: 1.0330434972106819e-06\n",
      "Epoch 2951, Training Loss: 6.809962087572785e-07, Validation Loss: 1.037748273114294e-06\n",
      "Epoch 2952, Training Loss: 2.732459961407585e-06, Validation Loss: 2.692992327309671e-06\n",
      "Epoch 2953, Training Loss: 1.172573661278875e-06, Validation Loss: 9.91385994478145e-07\n",
      "Epoch 2954, Training Loss: 5.621115519716113e-07, Validation Loss: 9.843387740573374e-07\n",
      "Epoch 2955, Training Loss: 1.498033270763699e-06, Validation Loss: 9.55353191670114e-07\n",
      "Epoch 2956, Training Loss: 8.278549898932397e-07, Validation Loss: 2.053950193857516e-06\n",
      "Epoch 2957, Training Loss: 9.820305422181264e-07, Validation Loss: 1.2053494319510498e-06\n",
      "Epoch 2958, Training Loss: 8.030542062442692e-07, Validation Loss: 9.537893423228548e-07\n",
      "Epoch 2959, Training Loss: 2.0109241631871555e-06, Validation Loss: 1.7553498673516086e-06\n",
      "Epoch 2960, Training Loss: 6.874334985695896e-07, Validation Loss: 9.899871166803997e-07\n",
      "Epoch 2961, Training Loss: 1.8966525203722995e-06, Validation Loss: 1.457799398241341e-06\n",
      "Epoch 2962, Training Loss: 1.2287957815715345e-06, Validation Loss: 1.0247431332080486e-06\n",
      "Epoch 2963, Training Loss: 1.1110341802123003e-06, Validation Loss: 9.457593592919027e-07\n",
      "Epoch 2964, Training Loss: 1.3641382565765525e-06, Validation Loss: 1.0470028903991234e-06\n",
      "Epoch 2965, Training Loss: 1.0664593901310582e-06, Validation Loss: 1.0197039337129986e-06\n",
      "Epoch 2966, Training Loss: 5.333270109986188e-07, Validation Loss: 9.547842160207867e-07\n",
      "Epoch 2967, Training Loss: 1.8357480939812376e-06, Validation Loss: 1.0922314614417933e-06\n",
      "Epoch 2968, Training Loss: 1.4615375221183058e-06, Validation Loss: 1.053953658943014e-06\n",
      "Epoch 2969, Training Loss: 1.0069752534036525e-06, Validation Loss: 1.1791908605118806e-06\n",
      "Epoch 2970, Training Loss: 1.8536716197559144e-06, Validation Loss: 1.7618791877452353e-06\n",
      "Epoch 2971, Training Loss: 8.54398422234226e-07, Validation Loss: 1.0846801628150579e-06\n",
      "Epoch 2972, Training Loss: 7.897601221884543e-07, Validation Loss: 9.502722126242612e-07\n",
      "Epoch 2973, Training Loss: 8.563137612327409e-07, Validation Loss: 9.995977135224548e-07\n",
      "Epoch 2974, Training Loss: 8.804863682598807e-07, Validation Loss: 9.778860014735566e-07\n",
      "Epoch 2975, Training Loss: 2.128121650457615e-06, Validation Loss: 1.6630045548875497e-06\n",
      "Epoch 2976, Training Loss: 9.061931223186548e-07, Validation Loss: 9.86712599581314e-07\n",
      "Epoch 2977, Training Loss: 2.354225671297172e-06, Validation Loss: 9.622586444232526e-07\n",
      "Epoch 2978, Training Loss: 9.479016966906784e-07, Validation Loss: 9.52252169193801e-07\n",
      "Epoch 2979, Training Loss: 7.732321023468103e-07, Validation Loss: 1.411755456328062e-06\n",
      "Epoch 2980, Training Loss: 7.655759759472858e-07, Validation Loss: 9.79243356240004e-07\n",
      "Epoch 2981, Training Loss: 6.494486797237187e-07, Validation Loss: 9.707950548863954e-07\n",
      "Epoch 2982, Training Loss: 1.5098960375325987e-06, Validation Loss: 1.5148540820043952e-06\n",
      "Epoch 2983, Training Loss: 1.096710320780403e-06, Validation Loss: 1.0004047599097108e-06\n",
      "Epoch 2984, Training Loss: 1.783201469152118e-06, Validation Loss: 1.5803783439385915e-06\n",
      "Epoch 2985, Training Loss: 1.0964066632368485e-06, Validation Loss: 1.0255026677296668e-06\n",
      "Epoch 2986, Training Loss: 9.494227697359747e-07, Validation Loss: 1.033421468110837e-06\n",
      "Epoch 2987, Training Loss: 8.001726996553771e-07, Validation Loss: 9.953954770698013e-07\n",
      "Epoch 2988, Training Loss: 1.463154603698058e-06, Validation Loss: 1.0551306323435663e-06\n",
      "Epoch 2989, Training Loss: 3.5308353290020023e-06, Validation Loss: 6.884270701187575e-06\n",
      "Epoch 2990, Training Loss: 2.9821246698702453e-06, Validation Loss: 3.8997678040489916e-06\n",
      "Epoch 2991, Training Loss: 9.973675787477987e-07, Validation Loss: 1.2611210701405673e-06\n",
      "Epoch 2992, Training Loss: 1.6464537111460231e-06, Validation Loss: 2.2928270786355738e-06\n",
      "Epoch 2993, Training Loss: 1.7047798337443965e-06, Validation Loss: 1.5737429374326709e-06\n",
      "Epoch 2994, Training Loss: 1.5846192127355607e-06, Validation Loss: 1.3028822529157166e-06\n",
      "Epoch 2995, Training Loss: 7.146329608076485e-07, Validation Loss: 1.0588515218028165e-06\n",
      "Epoch 2996, Training Loss: 1.9293772766104667e-06, Validation Loss: 1.350044379462346e-06\n",
      "Epoch 2997, Training Loss: 1.3195976862334646e-06, Validation Loss: 1.3974553715655882e-06\n",
      "Epoch 2998, Training Loss: 1.489671262788761e-06, Validation Loss: 1.0431908682673398e-06\n",
      "Epoch 2999, Training Loss: 1.6530582342966227e-06, Validation Loss: 1.6868669742571058e-06\n",
      "Epoch 3000, Training Loss: 1.5378245734609663e-06, Validation Loss: 1.0205239431152049e-06\n",
      "Epoch 3001, Training Loss: 1.365857315249741e-06, Validation Loss: 1.1821998100458586e-06\n",
      "Epoch 3002, Training Loss: 1.2081162594768102e-06, Validation Loss: 1.3140865958468718e-06\n",
      "Epoch 3003, Training Loss: 8.638913868708187e-07, Validation Loss: 1.1949159771997954e-06\n",
      "Epoch 3004, Training Loss: 1.4678785191790666e-06, Validation Loss: 9.659417561807998e-07\n",
      "Epoch 3005, Training Loss: 7.855678632040508e-07, Validation Loss: 1.1402128336250745e-06\n",
      "Epoch 3006, Training Loss: 1.6035326098062797e-06, Validation Loss: 9.687176382838886e-07\n",
      "Epoch 3007, Training Loss: 1.1956384469158365e-06, Validation Loss: 1.0165044761761586e-06\n",
      "Epoch 3008, Training Loss: 1.3093931556795724e-06, Validation Loss: 1.0170755774797031e-06\n",
      "Epoch 3009, Training Loss: 9.468453185945691e-07, Validation Loss: 9.449942933987453e-07\n",
      "Epoch 3010, Training Loss: 1.3331152786122402e-06, Validation Loss: 9.338498080358906e-07\n",
      "Epoch 3011, Training Loss: 1.0089604529639473e-06, Validation Loss: 1.249792494288222e-06\n",
      "Epoch 3012, Training Loss: 8.542394311916723e-07, Validation Loss: 1.0148858627205768e-06\n",
      "Epoch 3013, Training Loss: 8.372470574613544e-07, Validation Loss: 9.481105605920825e-07\n",
      "Epoch 3014, Training Loss: 1.1777773352150689e-06, Validation Loss: 1.1990202168655429e-06\n",
      "Epoch 3015, Training Loss: 1.463166427129181e-06, Validation Loss: 1.7197961854111156e-06\n",
      "Epoch 3016, Training Loss: 2.0995164504711283e-06, Validation Loss: 1.0292531335076643e-06\n",
      "Epoch 3017, Training Loss: 7.733838174317498e-07, Validation Loss: 9.52857387410462e-07\n",
      "Epoch 3018, Training Loss: 8.079190365606337e-07, Validation Loss: 1.0092390805760833e-06\n",
      "Epoch 3019, Training Loss: 1.5710027128079673e-06, Validation Loss: 1.4287168152752816e-06\n",
      "Epoch 3020, Training Loss: 1.1959842822761857e-06, Validation Loss: 1.196229269808216e-06\n",
      "Epoch 3021, Training Loss: 7.549801921413746e-07, Validation Loss: 1.1882190950794668e-06\n",
      "Epoch 3022, Training Loss: 1.1633037502178922e-06, Validation Loss: 1.0069831620611039e-06\n",
      "Epoch 3023, Training Loss: 6.901406663928356e-07, Validation Loss: 1.3492603327550992e-06\n",
      "Epoch 3024, Training Loss: 1.0097326139657525e-06, Validation Loss: 1.039241231658064e-06\n",
      "Epoch 3025, Training Loss: 6.09158360020956e-06, Validation Loss: 3.359980546462162e-06\n",
      "Epoch 3026, Training Loss: 1.7367376585752936e-06, Validation Loss: 6.2321165433745965e-06\n",
      "Epoch 3027, Training Loss: 9.440244639336015e-07, Validation Loss: 9.296128882272621e-07\n",
      "Epoch 3028, Training Loss: 7.050225576676894e-07, Validation Loss: 1.2994358433093257e-06\n",
      "Epoch 3029, Training Loss: 1.5320806596719194e-06, Validation Loss: 1.0964590113188394e-06\n",
      "Epoch 3030, Training Loss: 1.0466972071299097e-06, Validation Loss: 1.2361338683184128e-06\n",
      "Epoch 3031, Training Loss: 1.220410695168539e-06, Validation Loss: 1.3901689554864544e-06\n",
      "Epoch 3032, Training Loss: 9.698999292595545e-07, Validation Loss: 9.617881963407806e-07\n",
      "Epoch 3033, Training Loss: 7.275561983988155e-07, Validation Loss: 1.0659497240722324e-06\n",
      "Epoch 3034, Training Loss: 2.0604297787940595e-06, Validation Loss: 1.0316144724876409e-06\n",
      "Epoch 3035, Training Loss: 3.252483566029696e-06, Validation Loss: 3.333012571625184e-06\n",
      "Epoch 3036, Training Loss: 1.2400801097101066e-06, Validation Loss: 1.0433515233376202e-06\n",
      "Epoch 3037, Training Loss: 8.315102490996651e-07, Validation Loss: 9.90103266612252e-07\n",
      "Epoch 3038, Training Loss: 1.242391817868338e-06, Validation Loss: 1.2946341595142356e-06\n",
      "Epoch 3039, Training Loss: 8.260978461294144e-07, Validation Loss: 1.1924125926711054e-06\n",
      "Epoch 3040, Training Loss: 1.557741029500903e-06, Validation Loss: 1.2313969856587641e-06\n",
      "Epoch 3041, Training Loss: 2.6796969905262813e-06, Validation Loss: 1.3559425427562818e-06\n",
      "Epoch 3042, Training Loss: 1.2895536656287732e-06, Validation Loss: 1.0655788345224454e-06\n",
      "Epoch 3043, Training Loss: 1.3055730505584506e-06, Validation Loss: 1.1469925796918225e-06\n",
      "Epoch 3044, Training Loss: 1.5195299738479662e-06, Validation Loss: 1.0274263391768548e-06\n",
      "Epoch 3045, Training Loss: 8.077801112449379e-07, Validation Loss: 1.1565306312972515e-06\n",
      "Epoch 3046, Training Loss: 1.3064732229395304e-06, Validation Loss: 1.04735035430532e-06\n",
      "Epoch 3047, Training Loss: 1.038745494952309e-06, Validation Loss: 1.0692088063727975e-06\n",
      "Epoch 3048, Training Loss: 7.051241368571937e-07, Validation Loss: 9.718243012928283e-07\n",
      "Epoch 3049, Training Loss: 8.992615221359301e-07, Validation Loss: 1.1482899449191021e-06\n",
      "Epoch 3050, Training Loss: 1.5387966413982213e-05, Validation Loss: 3.3592244703375943e-06\n",
      "Epoch 3051, Training Loss: 1.0191040473728208e-06, Validation Loss: 1.549971164127018e-06\n",
      "Epoch 3052, Training Loss: 5.364957473830145e-07, Validation Loss: 9.342506591362572e-07\n",
      "Epoch 3053, Training Loss: 2.753035460045794e-06, Validation Loss: 2.3611922602334756e-06\n",
      "Epoch 3054, Training Loss: 1.9491540115268435e-06, Validation Loss: 1.2269242409703418e-06\n",
      "Epoch 3055, Training Loss: 2.0076759028597735e-06, Validation Loss: 3.2503520420956933e-06\n",
      "Epoch 3056, Training Loss: 1.3589551599579863e-06, Validation Loss: 2.3744065860934888e-06\n",
      "Epoch 3057, Training Loss: 9.026352927321568e-07, Validation Loss: 1.1879347945303344e-06\n",
      "Epoch 3058, Training Loss: 5.753392997576157e-07, Validation Loss: 9.377765219029058e-07\n",
      "Epoch 3059, Training Loss: 1.0900915867750882e-06, Validation Loss: 1.8934446637677478e-06\n",
      "Epoch 3060, Training Loss: 6.176654778755619e-07, Validation Loss: 9.385339552093561e-07\n",
      "Epoch 3061, Training Loss: 8.92127161478129e-07, Validation Loss: 9.630815865114e-07\n",
      "Epoch 3062, Training Loss: 8.869818657331052e-07, Validation Loss: 1.009806837712505e-06\n",
      "Epoch 3063, Training Loss: 1.4332275668493821e-06, Validation Loss: 3.963022730430028e-06\n",
      "Epoch 3064, Training Loss: 1.4707445643580286e-06, Validation Loss: 1.1646962713283449e-06\n",
      "Epoch 3065, Training Loss: 8.499018235852418e-07, Validation Loss: 9.52011014428854e-07\n",
      "Epoch 3066, Training Loss: 1.5973007521097315e-06, Validation Loss: 1.2024950203027449e-06\n",
      "Epoch 3067, Training Loss: 9.79921765065228e-07, Validation Loss: 9.340552971761023e-07\n",
      "Epoch 3068, Training Loss: 1.70669125054701e-06, Validation Loss: 1.5461510894189358e-06\n",
      "Epoch 3069, Training Loss: 1.8847520095732762e-06, Validation Loss: 1.4471020022984662e-06\n",
      "Epoch 3070, Training Loss: 3.2715568067942513e-06, Validation Loss: 2.8171644680407124e-06\n",
      "Epoch 3071, Training Loss: 2.6405155040265527e-06, Validation Loss: 1.4894543707501076e-06\n",
      "Epoch 3072, Training Loss: 5.963784133200534e-07, Validation Loss: 9.961587958424639e-07\n",
      "Epoch 3073, Training Loss: 7.579454859296675e-07, Validation Loss: 1.1051044735276768e-06\n",
      "Epoch 3074, Training Loss: 9.366964945911604e-07, Validation Loss: 1.3269231722064155e-06\n",
      "Epoch 3075, Training Loss: 1.013766450341791e-06, Validation Loss: 9.992197703198892e-07\n",
      "Epoch 3076, Training Loss: 1.1280287708359538e-06, Validation Loss: 1.301412139452339e-06\n",
      "Epoch 3077, Training Loss: 8.703090088602039e-07, Validation Loss: 1.0369194108021752e-06\n",
      "Epoch 3078, Training Loss: 1.4182909353621653e-06, Validation Loss: 2.462108430996627e-06\n",
      "Epoch 3079, Training Loss: 8.06104480943759e-07, Validation Loss: 1.2583138101082072e-06\n",
      "Epoch 3080, Training Loss: 1.5157918369368417e-06, Validation Loss: 1.012778165954697e-06\n",
      "Epoch 3081, Training Loss: 1.4950086324461154e-06, Validation Loss: 1.1246896131750333e-06\n",
      "Epoch 3082, Training Loss: 4.9688160288496874e-06, Validation Loss: 1.4663167748486054e-06\n",
      "Epoch 3083, Training Loss: 7.034456075416529e-07, Validation Loss: 1.0009112927013595e-06\n",
      "Epoch 3084, Training Loss: 1.0736696367530385e-06, Validation Loss: 9.637677687646683e-07\n",
      "Epoch 3085, Training Loss: 9.767964002094232e-07, Validation Loss: 9.431514174492381e-07\n",
      "Epoch 3086, Training Loss: 1.4218883279681904e-06, Validation Loss: 1.02856076165779e-06\n",
      "Epoch 3087, Training Loss: 5.313039537213626e-07, Validation Loss: 9.226800747815018e-07\n",
      "Epoch 3088, Training Loss: 1.188171836474794e-06, Validation Loss: 1.1443936793923267e-06\n",
      "Epoch 3089, Training Loss: 6.740382332282024e-07, Validation Loss: 1.0513238727537771e-06\n",
      "Epoch 3090, Training Loss: 1.1610673027462326e-06, Validation Loss: 1.2248897190634154e-06\n",
      "Epoch 3091, Training Loss: 7.59216959522746e-07, Validation Loss: 9.6865519669342e-07\n",
      "Epoch 3092, Training Loss: 6.241438086362905e-07, Validation Loss: 1.1833506740331243e-06\n",
      "Epoch 3093, Training Loss: 1.3699913097298122e-06, Validation Loss: 1.6061748092223677e-06\n",
      "Epoch 3094, Training Loss: 1.060413410414185e-06, Validation Loss: 1.046956608813341e-06\n",
      "Epoch 3095, Training Loss: 1.036562252920703e-06, Validation Loss: 9.242099509534618e-07\n",
      "Epoch 3096, Training Loss: 1.7553279576532077e-06, Validation Loss: 1.1496761949694504e-06\n",
      "Epoch 3097, Training Loss: 7.343380161728419e-07, Validation Loss: 9.550742814701534e-07\n",
      "Epoch 3098, Training Loss: 7.018085739218805e-07, Validation Loss: 1.0291151861160088e-06\n",
      "Epoch 3099, Training Loss: 9.515599117548845e-07, Validation Loss: 1.2274679345134657e-06\n",
      "Epoch 3100, Training Loss: 8.441851377938292e-07, Validation Loss: 9.666737857288893e-07\n",
      "Epoch 3101, Training Loss: 1.3027204204263398e-06, Validation Loss: 1.0459337880666293e-06\n",
      "Epoch 3102, Training Loss: 3.248913344577886e-06, Validation Loss: 1.772797493144443e-06\n",
      "Epoch 3103, Training Loss: 7.416592211484385e-07, Validation Loss: 1.0692415192172618e-06\n",
      "Epoch 3104, Training Loss: 1.0767680578283034e-06, Validation Loss: 1.1678198649866654e-06\n",
      "Epoch 3105, Training Loss: 1.3802331295664771e-06, Validation Loss: 1.0652110217584765e-06\n",
      "Epoch 3106, Training Loss: 1.601467715772742e-06, Validation Loss: 1.0253508516299893e-06\n",
      "Epoch 3107, Training Loss: 7.031520681266556e-07, Validation Loss: 1.0591807043948294e-06\n",
      "Epoch 3108, Training Loss: 1.5427019661728991e-06, Validation Loss: 3.811540135797792e-06\n",
      "Epoch 3109, Training Loss: 1.4999459381215274e-06, Validation Loss: 1.4462309811777023e-06\n",
      "Epoch 3110, Training Loss: 1.4292194236986688e-06, Validation Loss: 1.0129449937857937e-06\n",
      "Epoch 3111, Training Loss: 1.0929296649919706e-06, Validation Loss: 9.80593243881161e-07\n",
      "Epoch 3112, Training Loss: 1.0045746421383228e-06, Validation Loss: 9.960129602497465e-07\n",
      "Epoch 3113, Training Loss: 1.4138304322841577e-06, Validation Loss: 1.0579664310307346e-06\n",
      "Epoch 3114, Training Loss: 1.8207745142717613e-06, Validation Loss: 9.9635458105075e-07\n",
      "Epoch 3115, Training Loss: 1.7559902971697738e-06, Validation Loss: 2.0428932133260677e-06\n",
      "Epoch 3116, Training Loss: 1.6231961126322858e-06, Validation Loss: 1.4673720519202766e-06\n",
      "Epoch 3117, Training Loss: 6.690021905342292e-07, Validation Loss: 1.0706908486267561e-06\n",
      "Epoch 3118, Training Loss: 5.635587854158075e-07, Validation Loss: 9.823649140349853e-07\n",
      "Epoch 3119, Training Loss: 7.980796681295033e-07, Validation Loss: 9.445120875990391e-07\n",
      "Epoch 3120, Training Loss: 1.452619244446396e-06, Validation Loss: 1.5239339405339867e-06\n",
      "Epoch 3121, Training Loss: 8.476044399685634e-07, Validation Loss: 1.0258290517789658e-06\n",
      "Epoch 3122, Training Loss: 1.6135441001097206e-06, Validation Loss: 9.288942176067249e-07\n",
      "Epoch 3123, Training Loss: 6.969696073610976e-07, Validation Loss: 9.240950789123539e-07\n",
      "Epoch 3124, Training Loss: 2.098764753100113e-06, Validation Loss: 3.6109914403215236e-06\n",
      "Epoch 3125, Training Loss: 1.7633527704674634e-06, Validation Loss: 1.8430787805839243e-06\n",
      "Epoch 3126, Training Loss: 8.682786756253336e-07, Validation Loss: 1.1331438320371635e-06\n",
      "Epoch 3127, Training Loss: 1.0644288295225124e-06, Validation Loss: 9.50568511841553e-07\n",
      "Epoch 3128, Training Loss: 1.1150186765007675e-06, Validation Loss: 9.558810249942257e-07\n",
      "Epoch 3129, Training Loss: 3.47353307006415e-06, Validation Loss: 4.546430250374429e-06\n",
      "Epoch 3130, Training Loss: 1.0723586001404328e-06, Validation Loss: 1.1313625609574816e-06\n",
      "Epoch 3131, Training Loss: 9.956958137991023e-07, Validation Loss: 1.0728436696536516e-06\n",
      "Epoch 3132, Training Loss: 1.7866193502413807e-06, Validation Loss: 9.379398931472516e-07\n",
      "Epoch 3133, Training Loss: 6.739505806763191e-07, Validation Loss: 9.891202387931626e-07\n",
      "Epoch 3134, Training Loss: 4.776669356942875e-06, Validation Loss: 3.1197618876263547e-06\n",
      "Epoch 3135, Training Loss: 6.972294386287103e-07, Validation Loss: 9.641376020044266e-07\n",
      "Epoch 3136, Training Loss: 9.35332252538501e-07, Validation Loss: 9.72244457432008e-07\n",
      "Epoch 3137, Training Loss: 7.707518534516566e-07, Validation Loss: 9.444360443197931e-07\n",
      "Epoch 3138, Training Loss: 9.758639407664305e-07, Validation Loss: 1.4124694672364926e-06\n",
      "Epoch 3139, Training Loss: 4.623215318133589e-06, Validation Loss: 5.096257580728767e-06\n",
      "Epoch 3140, Training Loss: 1.2135275255786837e-06, Validation Loss: 9.381208427663053e-07\n",
      "Epoch 3141, Training Loss: 6.242922268029361e-07, Validation Loss: 9.71130886348115e-07\n",
      "Epoch 3142, Training Loss: 1.098289089895843e-06, Validation Loss: 1.1472742160224981e-06\n",
      "Epoch 3143, Training Loss: 3.600948730309028e-06, Validation Loss: 2.200568266286441e-06\n",
      "Epoch 3144, Training Loss: 1.1856147921207594e-06, Validation Loss: 9.328890671817147e-07\n",
      "Epoch 3145, Training Loss: 1.1496625802465132e-06, Validation Loss: 1.3638225840721388e-06\n",
      "Epoch 3146, Training Loss: 1.0099863629875472e-06, Validation Loss: 9.713415630839029e-07\n",
      "Epoch 3147, Training Loss: 2.34606295634876e-06, Validation Loss: 1.8971201519900877e-06\n",
      "Epoch 3148, Training Loss: 8.347707307621022e-07, Validation Loss: 1.072934408585943e-06\n",
      "Epoch 3149, Training Loss: 1.5179327874648152e-06, Validation Loss: 9.342656069640876e-07\n",
      "Epoch 3150, Training Loss: 1.8091526499119936e-06, Validation Loss: 1.1118798468148625e-06\n",
      "Epoch 3151, Training Loss: 7.490545499422296e-07, Validation Loss: 1.0512608547638002e-06\n",
      "Epoch 3152, Training Loss: 8.611900739197154e-07, Validation Loss: 9.767224767914347e-07\n",
      "Epoch 3153, Training Loss: 3.7215245356492233e-06, Validation Loss: 2.9059518426623355e-06\n",
      "Epoch 3154, Training Loss: 8.980518941825721e-07, Validation Loss: 1.045283978722292e-06\n",
      "Epoch 3155, Training Loss: 7.830773256500834e-07, Validation Loss: 1.09400849479029e-06\n",
      "Epoch 3156, Training Loss: 1.2237376267876243e-06, Validation Loss: 1.1373761808383834e-06\n",
      "Epoch 3157, Training Loss: 7.586165224893193e-07, Validation Loss: 1.0485877982417405e-06\n",
      "Epoch 3158, Training Loss: 1.3747911680184188e-06, Validation Loss: 1.006133142773309e-06\n",
      "Epoch 3159, Training Loss: 8.479466941935243e-07, Validation Loss: 9.448641663219031e-07\n",
      "Epoch 3160, Training Loss: 1.768906940924353e-06, Validation Loss: 3.664000073252624e-06\n",
      "Epoch 3161, Training Loss: 8.607084396317077e-07, Validation Loss: 9.79061526091048e-07\n",
      "Epoch 3162, Training Loss: 3.582709723559674e-06, Validation Loss: 3.6643653613722634e-06\n",
      "Epoch 3163, Training Loss: 8.327441491928766e-07, Validation Loss: 1.1068725151162537e-06\n",
      "Epoch 3164, Training Loss: 1.7517113519716077e-06, Validation Loss: 1.364137980324778e-06\n",
      "Epoch 3165, Training Loss: 9.014141824081889e-07, Validation Loss: 9.4702041460869e-07\n",
      "Epoch 3166, Training Loss: 1.153552489085996e-06, Validation Loss: 9.271688724858824e-07\n",
      "Epoch 3167, Training Loss: 1.3000151284359163e-06, Validation Loss: 9.76423169760753e-07\n",
      "Epoch 3168, Training Loss: 9.360378498968203e-07, Validation Loss: 1.0908515278900784e-06\n",
      "Epoch 3169, Training Loss: 8.612857413936581e-07, Validation Loss: 9.495797279926991e-07\n",
      "Epoch 3170, Training Loss: 8.942193971961387e-07, Validation Loss: 9.71009291577062e-07\n",
      "Epoch 3171, Training Loss: 1.0749899956863374e-06, Validation Loss: 9.171783472663212e-07\n",
      "Epoch 3172, Training Loss: 6.822019145147351e-07, Validation Loss: 9.858125530160816e-07\n",
      "Epoch 3173, Training Loss: 1.0440061259942013e-06, Validation Loss: 1.110752168425412e-06\n",
      "Epoch 3174, Training Loss: 1.273943325941218e-06, Validation Loss: 1.3487672653366406e-06\n",
      "Epoch 3175, Training Loss: 7.858053550080513e-07, Validation Loss: 1.0144521850889083e-06\n",
      "Epoch 3176, Training Loss: 8.069593491200067e-07, Validation Loss: 9.525289627910439e-07\n",
      "Epoch 3177, Training Loss: 6.93020410835743e-07, Validation Loss: 9.83644115979639e-07\n",
      "Epoch 3178, Training Loss: 1.425484697392676e-06, Validation Loss: 9.934716495747566e-07\n",
      "Epoch 3179, Training Loss: 4.231181264913175e-06, Validation Loss: 2.130616239462901e-06\n",
      "Epoch 3180, Training Loss: 1.0840321920113638e-06, Validation Loss: 9.147841821171024e-07\n",
      "Epoch 3181, Training Loss: 1.1653700084934826e-06, Validation Loss: 9.128079965834169e-07\n",
      "Epoch 3182, Training Loss: 1.0808323622768512e-06, Validation Loss: 9.92992361435502e-07\n",
      "Epoch 3183, Training Loss: 8.071194201875187e-07, Validation Loss: 9.367363793009911e-07\n",
      "Epoch 3184, Training Loss: 2.2371318664227147e-06, Validation Loss: 1.4221386276444343e-06\n",
      "Epoch 3185, Training Loss: 3.141483375657117e-06, Validation Loss: 3.397938714796087e-06\n",
      "Epoch 3186, Training Loss: 1.44557543535484e-06, Validation Loss: 9.505288825650472e-07\n",
      "Epoch 3187, Training Loss: 9.939074061549036e-07, Validation Loss: 1.0039467240273269e-06\n",
      "Epoch 3188, Training Loss: 1.1724305295501836e-06, Validation Loss: 1.1953729714949969e-06\n",
      "Epoch 3189, Training Loss: 1.047907517204294e-06, Validation Loss: 9.94836781932938e-07\n",
      "Epoch 3190, Training Loss: 7.217939810288954e-07, Validation Loss: 9.898762919269206e-07\n",
      "Epoch 3191, Training Loss: 6.305171496023831e-07, Validation Loss: 9.289734645738168e-07\n",
      "Epoch 3192, Training Loss: 1.614335815247614e-06, Validation Loss: 1.5719339488502445e-06\n",
      "Epoch 3193, Training Loss: 1.1857789559144294e-06, Validation Loss: 1.0054941184720358e-06\n",
      "Epoch 3194, Training Loss: 1.424973447683442e-06, Validation Loss: 1.0100702899421052e-06\n",
      "Epoch 3195, Training Loss: 1.1616607480391394e-06, Validation Loss: 1.2189722474721268e-06\n",
      "Epoch 3196, Training Loss: 1.1549386726983357e-06, Validation Loss: 1.5078080502265438e-06\n",
      "Epoch 3197, Training Loss: 9.917458783093025e-07, Validation Loss: 9.80682719043048e-07\n",
      "Epoch 3198, Training Loss: 1.4329314126371173e-06, Validation Loss: 1.3445780577938561e-06\n",
      "Epoch 3199, Training Loss: 1.4252007076720474e-06, Validation Loss: 1.6740214839805407e-06\n",
      "Epoch 3200, Training Loss: 1.2567959402076667e-06, Validation Loss: 1.0135961157786345e-06\n",
      "Epoch 3201, Training Loss: 2.707763542275643e-06, Validation Loss: 2.9526910505520406e-06\n",
      "Epoch 3202, Training Loss: 1.930164444274851e-06, Validation Loss: 1.132188358250732e-06\n",
      "Epoch 3203, Training Loss: 1.1094747378592729e-06, Validation Loss: 9.343694379857167e-07\n",
      "Epoch 3204, Training Loss: 9.462122534387163e-07, Validation Loss: 1.0683373314098327e-06\n",
      "Epoch 3205, Training Loss: 6.046812472959573e-07, Validation Loss: 9.123514365791767e-07\n",
      "Epoch 3206, Training Loss: 2.253018919873284e-06, Validation Loss: 1.4532623408364077e-06\n",
      "Epoch 3207, Training Loss: 1.1580522141230176e-06, Validation Loss: 9.63144613190817e-07\n",
      "Epoch 3208, Training Loss: 1.1218497775189462e-06, Validation Loss: 1.0973437362292987e-06\n",
      "Epoch 3209, Training Loss: 7.636939471922233e-07, Validation Loss: 9.366257911537174e-07\n",
      "Epoch 3210, Training Loss: 1.6510520026713493e-06, Validation Loss: 1.0865000993675447e-06\n",
      "Epoch 3211, Training Loss: 1.0712109315136331e-06, Validation Loss: 1.4874208398013813e-06\n",
      "Epoch 3212, Training Loss: 8.971444458438782e-07, Validation Loss: 1.0152696311063658e-06\n",
      "Epoch 3213, Training Loss: 1.3000908438698389e-06, Validation Loss: 1.0693509632549586e-06\n",
      "Epoch 3214, Training Loss: 2.1566497707681265e-06, Validation Loss: 1.6481741948873432e-06\n",
      "Epoch 3215, Training Loss: 1.6216644098676625e-06, Validation Loss: 1.129293577818386e-06\n",
      "Epoch 3216, Training Loss: 2.277182284160517e-05, Validation Loss: 2.6776535420302336e-06\n",
      "Epoch 3217, Training Loss: 7.11160680566536e-07, Validation Loss: 1.0074143362289528e-06\n",
      "Epoch 3218, Training Loss: 5.951814614491013e-07, Validation Loss: 9.340840020164471e-07\n",
      "Epoch 3219, Training Loss: 1.0245283874610323e-06, Validation Loss: 1.0921657703628897e-06\n",
      "Epoch 3220, Training Loss: 1.2137624025854166e-06, Validation Loss: 9.211066112931954e-07\n",
      "Epoch 3221, Training Loss: 9.379807011100638e-07, Validation Loss: 1.7024582406242433e-06\n",
      "Epoch 3222, Training Loss: 9.413139423486427e-07, Validation Loss: 1.518312077892614e-06\n",
      "Epoch 3223, Training Loss: 1.783511834219098e-06, Validation Loss: 9.424801739723004e-07\n",
      "Epoch 3224, Training Loss: 1.788304643923766e-06, Validation Loss: 1.699399012181133e-06\n",
      "Epoch 3225, Training Loss: 9.975058219424682e-07, Validation Loss: 1.0038114330698382e-06\n",
      "Epoch 3226, Training Loss: 9.269631391362054e-07, Validation Loss: 9.790766379320553e-07\n",
      "Epoch 3227, Training Loss: 1.115919758376549e-06, Validation Loss: 9.54546433482612e-07\n",
      "Epoch 3228, Training Loss: 3.623127213359112e-06, Validation Loss: 4.109000945819696e-06\n",
      "Epoch 3229, Training Loss: 1.295709694204561e-06, Validation Loss: 1.2241689267713207e-06\n",
      "Epoch 3230, Training Loss: 2.9368518426053924e-06, Validation Loss: 5.760393969177611e-06\n",
      "Epoch 3231, Training Loss: 4.6613031372544356e-06, Validation Loss: 3.9782709377128405e-06\n",
      "Epoch 3232, Training Loss: 7.524248530899058e-07, Validation Loss: 1.0465768058896225e-06\n",
      "Epoch 3233, Training Loss: 1.4607187495130347e-06, Validation Loss: 9.948721410738895e-07\n",
      "Epoch 3234, Training Loss: 1.5670228776798467e-06, Validation Loss: 1.0016507817714795e-06\n",
      "Epoch 3235, Training Loss: 1.2203303185742698e-06, Validation Loss: 1.9446464102367398e-06\n",
      "Epoch 3236, Training Loss: 1.056132759913453e-06, Validation Loss: 9.842665763983043e-07\n",
      "Epoch 3237, Training Loss: 5.924795800638094e-07, Validation Loss: 9.215613928587511e-07\n",
      "Epoch 3238, Training Loss: 8.486978799737699e-07, Validation Loss: 9.61581900892965e-07\n",
      "Epoch 3239, Training Loss: 2.05780361284269e-06, Validation Loss: 1.415607293111068e-06\n",
      "Epoch 3240, Training Loss: 1.5290365809050854e-06, Validation Loss: 9.331458310770688e-07\n",
      "Epoch 3241, Training Loss: 5.570235771301668e-06, Validation Loss: 2.8430820018413423e-06\n",
      "Epoch 3242, Training Loss: 1.869711809376895e-06, Validation Loss: 2.0621063142452202e-06\n",
      "Epoch 3243, Training Loss: 1.2180419162177714e-06, Validation Loss: 1.0460065769696305e-06\n",
      "Epoch 3244, Training Loss: 1.2375799087749328e-06, Validation Loss: 2.051747255038185e-06\n",
      "Epoch 3245, Training Loss: 7.244296966746333e-07, Validation Loss: 9.241039419599462e-07\n",
      "Epoch 3246, Training Loss: 5.904009867663262e-06, Validation Loss: 6.81059254408639e-06\n",
      "Epoch 3247, Training Loss: 3.1421700441569556e-06, Validation Loss: 3.437507796655728e-06\n",
      "Epoch 3248, Training Loss: 2.758400796665228e-06, Validation Loss: 1.404402245611167e-06\n",
      "Epoch 3249, Training Loss: 7.785189154674299e-06, Validation Loss: 4.145082859893383e-06\n",
      "Epoch 3250, Training Loss: 2.0849342945439275e-06, Validation Loss: 9.453439959218769e-07\n",
      "Epoch 3251, Training Loss: 8.254597787527018e-07, Validation Loss: 1.105487229231666e-06\n",
      "Epoch 3252, Training Loss: 1.8411600422041374e-06, Validation Loss: 1.0013432822283122e-06\n",
      "Epoch 3253, Training Loss: 7.413341336359736e-07, Validation Loss: 1.0949401044784797e-06\n",
      "Epoch 3254, Training Loss: 1.7944865930985543e-06, Validation Loss: 1.2645515045738985e-06\n",
      "Epoch 3255, Training Loss: 9.723146376927616e-07, Validation Loss: 9.621985726964665e-07\n",
      "Epoch 3256, Training Loss: 8.258906518676667e-07, Validation Loss: 1.1819846750843502e-06\n",
      "Epoch 3257, Training Loss: 6.773154268557846e-07, Validation Loss: 9.963476287747695e-07\n",
      "Epoch 3258, Training Loss: 8.164911378116813e-07, Validation Loss: 9.285695399461717e-07\n",
      "Epoch 3259, Training Loss: 8.080492079898249e-07, Validation Loss: 9.096435422825554e-07\n",
      "Epoch 3260, Training Loss: 7.062967597448733e-07, Validation Loss: 9.634714389533071e-07\n",
      "Epoch 3261, Training Loss: 6.594666501769098e-07, Validation Loss: 9.414394640223771e-07\n",
      "Epoch 3262, Training Loss: 5.300491466186941e-06, Validation Loss: 2.6191467167377218e-06\n",
      "Epoch 3263, Training Loss: 1.8330754301132401e-06, Validation Loss: 9.630610692957312e-07\n",
      "Epoch 3264, Training Loss: 3.375665983185172e-06, Validation Loss: 1.8555568335527331e-06\n",
      "Epoch 3265, Training Loss: 6.073868235034752e-07, Validation Loss: 9.174824812808261e-07\n",
      "Epoch 3266, Training Loss: 6.807008503528778e-07, Validation Loss: 1.0419154205227508e-06\n",
      "Epoch 3267, Training Loss: 1.1170602647325723e-06, Validation Loss: 1.030250950572307e-06\n",
      "Epoch 3268, Training Loss: 1.0845574252016377e-06, Validation Loss: 9.490669440542458e-07\n",
      "Epoch 3269, Training Loss: 7.578400982310995e-07, Validation Loss: 9.074494618040366e-07\n",
      "Epoch 3270, Training Loss: 1.4392364846571581e-06, Validation Loss: 1.127396694103881e-06\n",
      "Epoch 3271, Training Loss: 7.060612006171141e-07, Validation Loss: 9.267053705243714e-07\n",
      "Epoch 3272, Training Loss: 9.249171739611484e-07, Validation Loss: 1.1022348428686066e-06\n",
      "Epoch 3273, Training Loss: 1.072519808076322e-06, Validation Loss: 1.2577291911138035e-06\n",
      "Epoch 3274, Training Loss: 2.2158160390972625e-06, Validation Loss: 9.363744228936111e-06\n",
      "Epoch 3275, Training Loss: 9.340637916466221e-07, Validation Loss: 9.847074668081184e-07\n",
      "Epoch 3276, Training Loss: 8.206920938391704e-07, Validation Loss: 1.081919745199409e-06\n",
      "Epoch 3277, Training Loss: 5.677993044628238e-07, Validation Loss: 9.406863761599597e-07\n",
      "Epoch 3278, Training Loss: 8.164397513610311e-07, Validation Loss: 9.468634299215861e-07\n",
      "Epoch 3279, Training Loss: 1.8634411844686838e-06, Validation Loss: 1.2501240742604768e-06\n",
      "Epoch 3280, Training Loss: 1.2868918020103592e-06, Validation Loss: 1.5747374767681993e-06\n",
      "Epoch 3281, Training Loss: 7.902901302259124e-07, Validation Loss: 9.434257200737309e-07\n",
      "Epoch 3282, Training Loss: 8.885659781299182e-07, Validation Loss: 1.0053212855138995e-06\n",
      "Epoch 3283, Training Loss: 7.852106023165106e-07, Validation Loss: 9.092023696469768e-07\n",
      "Epoch 3284, Training Loss: 9.019651656672067e-07, Validation Loss: 9.569035084820542e-07\n",
      "Epoch 3285, Training Loss: 1.0966330137307523e-06, Validation Loss: 9.842463587872795e-07\n",
      "Epoch 3286, Training Loss: 1.209432184623438e-06, Validation Loss: 9.254451959185164e-07\n",
      "Epoch 3287, Training Loss: 4.046398316859268e-06, Validation Loss: 2.031190068670289e-06\n",
      "Epoch 3288, Training Loss: 1.227992697749869e-06, Validation Loss: 9.790437442386244e-07\n",
      "Epoch 3289, Training Loss: 1.8597537518871832e-06, Validation Loss: 1.900186217936161e-06\n",
      "Epoch 3290, Training Loss: 5.402067699833424e-07, Validation Loss: 8.993815428511649e-07\n",
      "Epoch 3291, Training Loss: 4.682234703068389e-06, Validation Loss: 5.493639031220094e-06\n",
      "Epoch 3292, Training Loss: 2.2708461528964108e-06, Validation Loss: 1.1599440619450833e-06\n",
      "Epoch 3293, Training Loss: 9.151852964350837e-07, Validation Loss: 1.0933522990697407e-06\n",
      "Epoch 3294, Training Loss: 1.7282212638747296e-06, Validation Loss: 1.4204989803135742e-06\n",
      "Epoch 3295, Training Loss: 7.502566177208791e-07, Validation Loss: 9.082136520551016e-07\n",
      "Epoch 3296, Training Loss: 1.3165658856451046e-06, Validation Loss: 1.033940008260085e-06\n",
      "Epoch 3297, Training Loss: 9.276109267375432e-07, Validation Loss: 1.0283051941896818e-06\n",
      "Epoch 3298, Training Loss: 2.0251684418326477e-06, Validation Loss: 3.6084769973944752e-06\n",
      "Epoch 3299, Training Loss: 1.6098196056191227e-06, Validation Loss: 3.5151972178647637e-06\n",
      "Epoch 3300, Training Loss: 7.159948722801346e-07, Validation Loss: 9.208789602797977e-07\n",
      "Epoch 3301, Training Loss: 1.1621251587712322e-06, Validation Loss: 1.303069649737061e-06\n",
      "Epoch 3302, Training Loss: 1.0112686368302093e-06, Validation Loss: 9.25689813425234e-07\n",
      "Epoch 3303, Training Loss: 8.816056720206689e-07, Validation Loss: 9.154261637243965e-07\n",
      "Epoch 3304, Training Loss: 5.1706967951759e-07, Validation Loss: 9.178023734134252e-07\n",
      "Epoch 3305, Training Loss: 8.213060027628671e-07, Validation Loss: 1.1008200498187305e-06\n",
      "Epoch 3306, Training Loss: 3.8707339626853354e-06, Validation Loss: 3.917595565679708e-06\n",
      "Epoch 3307, Training Loss: 8.603294077147439e-07, Validation Loss: 1.0426947653379021e-06\n",
      "Epoch 3308, Training Loss: 5.448463298307615e-07, Validation Loss: 9.624313392555015e-07\n",
      "Epoch 3309, Training Loss: 8.08130039331445e-07, Validation Loss: 1.4307465648694436e-06\n",
      "Epoch 3310, Training Loss: 1.2598374041772331e-06, Validation Loss: 1.0586425156118353e-06\n",
      "Epoch 3311, Training Loss: 2.794089596136473e-06, Validation Loss: 2.1635227563345057e-06\n",
      "Epoch 3312, Training Loss: 1.9116396288154647e-06, Validation Loss: 1.317382102106827e-06\n",
      "Epoch 3313, Training Loss: 5.513907126442064e-07, Validation Loss: 9.127548144962327e-07\n",
      "Epoch 3314, Training Loss: 1.2067832813045243e-06, Validation Loss: 1.0020365081777092e-06\n",
      "Epoch 3315, Training Loss: 4.639268809114583e-07, Validation Loss: 9.087319532447544e-07\n",
      "Epoch 3316, Training Loss: 9.299510566052049e-07, Validation Loss: 1.082244333974234e-06\n",
      "Epoch 3317, Training Loss: 1.5190942121989792e-06, Validation Loss: 4.3458101566675915e-06\n",
      "Epoch 3318, Training Loss: 9.567422694090055e-07, Validation Loss: 1.0664883467841278e-06\n",
      "Epoch 3319, Training Loss: 8.765666734689148e-07, Validation Loss: 8.987550753335298e-07\n",
      "Epoch 3320, Training Loss: 2.091508576995693e-06, Validation Loss: 2.4308108189418987e-06\n",
      "Epoch 3321, Training Loss: 7.211289130282239e-07, Validation Loss: 9.047498000275303e-07\n",
      "Epoch 3322, Training Loss: 1.5760188034619205e-06, Validation Loss: 1.1392098007241761e-06\n",
      "Epoch 3323, Training Loss: 1.2015302672807593e-06, Validation Loss: 9.231437941600384e-07\n",
      "Epoch 3324, Training Loss: 1.216577629747917e-06, Validation Loss: 1.0583666483450838e-06\n",
      "Epoch 3325, Training Loss: 1.0108773267347715e-06, Validation Loss: 1.0528050277797235e-06\n",
      "Epoch 3326, Training Loss: 1.0166456831939286e-06, Validation Loss: 1.0036724157909043e-06\n",
      "Epoch 3327, Training Loss: 3.0405717552639544e-06, Validation Loss: 1.241306640796286e-06\n",
      "Epoch 3328, Training Loss: 1.2468749446270522e-06, Validation Loss: 9.755871726944172e-07\n",
      "Epoch 3329, Training Loss: 6.680461410724092e-07, Validation Loss: 9.939186892114875e-07\n",
      "Epoch 3330, Training Loss: 1.2053023965563625e-06, Validation Loss: 1.236138844833013e-06\n",
      "Epoch 3331, Training Loss: 1.2852086911152583e-05, Validation Loss: 3.3674531084453654e-06\n",
      "Epoch 3332, Training Loss: 1.447558133804705e-06, Validation Loss: 9.298095666375505e-07\n",
      "Epoch 3333, Training Loss: 9.19384001463186e-06, Validation Loss: 8.08370897714082e-06\n",
      "Epoch 3334, Training Loss: 1.6610263173788553e-06, Validation Loss: 1.9748043501216683e-06\n",
      "Epoch 3335, Training Loss: 1.5349919522122946e-06, Validation Loss: 9.235137215353947e-07\n",
      "Epoch 3336, Training Loss: 1.0149267382075777e-06, Validation Loss: 1.4041989348049013e-06\n",
      "Epoch 3337, Training Loss: 5.386108341554063e-07, Validation Loss: 1.1955249172121518e-06\n",
      "Epoch 3338, Training Loss: 7.439763862748805e-07, Validation Loss: 9.519751797221022e-07\n",
      "Epoch 3339, Training Loss: 4.78779838886112e-06, Validation Loss: 2.9120935966608493e-06\n",
      "Epoch 3340, Training Loss: 1.8839031099560088e-06, Validation Loss: 1.1289354651847128e-06\n",
      "Epoch 3341, Training Loss: 1.3322372751645162e-06, Validation Loss: 9.600905853774423e-07\n",
      "Epoch 3342, Training Loss: 5.752458491770085e-07, Validation Loss: 1.1151660834338533e-06\n",
      "Epoch 3343, Training Loss: 6.13346799127612e-07, Validation Loss: 9.494906375195545e-07\n",
      "Epoch 3344, Training Loss: 8.44675696498598e-07, Validation Loss: 9.807933408619009e-07\n",
      "Epoch 3345, Training Loss: 1.4474039744527545e-06, Validation Loss: 1.5510013413896923e-06\n",
      "Epoch 3346, Training Loss: 1.8539269603934372e-06, Validation Loss: 1.125071304965734e-06\n",
      "Epoch 3347, Training Loss: 2.929249149019597e-06, Validation Loss: 1.7313392456031626e-06\n",
      "Epoch 3348, Training Loss: 1.50977871271607e-06, Validation Loss: 1.442918716246863e-06\n",
      "Epoch 3349, Training Loss: 4.2909073272312526e-06, Validation Loss: 3.3857533482289323e-06\n",
      "Epoch 3350, Training Loss: 1.8934442778117955e-06, Validation Loss: 1.0708472531127216e-06\n",
      "Epoch 3351, Training Loss: 2.25638973461173e-06, Validation Loss: 1.1132492456879985e-06\n",
      "Epoch 3352, Training Loss: 1.0477078831172548e-06, Validation Loss: 8.971667796955606e-07\n",
      "Epoch 3353, Training Loss: 9.151947324426146e-07, Validation Loss: 9.119400998593603e-07\n",
      "Epoch 3354, Training Loss: 6.233128146959643e-07, Validation Loss: 1.0764760024972166e-06\n",
      "Epoch 3355, Training Loss: 1.6943624814302893e-06, Validation Loss: 1.5394813715736826e-06\n",
      "Epoch 3356, Training Loss: 1.8444736724632094e-06, Validation Loss: 1.1401713332225261e-06\n",
      "Epoch 3357, Training Loss: 2.8254421522433404e-06, Validation Loss: 1.106818911591238e-06\n",
      "Epoch 3358, Training Loss: 1.7766068367564003e-06, Validation Loss: 1.3251403547685046e-06\n",
      "Epoch 3359, Training Loss: 4.831771320823464e-07, Validation Loss: 9.032431338924351e-07\n",
      "Epoch 3360, Training Loss: 7.961384085319878e-07, Validation Loss: 9.317122529065291e-07\n",
      "Epoch 3361, Training Loss: 1.1022161743312608e-06, Validation Loss: 1.3109706927048572e-06\n",
      "Epoch 3362, Training Loss: 9.347951390736853e-07, Validation Loss: 1.0690458342995935e-06\n",
      "Epoch 3363, Training Loss: 1.0213586847385159e-06, Validation Loss: 1.003823713239642e-06\n",
      "Epoch 3364, Training Loss: 8.519782568328083e-07, Validation Loss: 1.0516166820747991e-06\n",
      "Epoch 3365, Training Loss: 9.085392775887158e-07, Validation Loss: 1.0635993529756158e-06\n",
      "Epoch 3366, Training Loss: 9.949380910256878e-07, Validation Loss: 1.153964051731248e-06\n",
      "Epoch 3367, Training Loss: 1.0256374025630066e-06, Validation Loss: 9.32205248094019e-07\n",
      "Epoch 3368, Training Loss: 1.5941503761496278e-06, Validation Loss: 1.0022070934674105e-06\n",
      "Epoch 3369, Training Loss: 7.405721476061444e-07, Validation Loss: 9.314473240152484e-07\n",
      "Epoch 3370, Training Loss: 7.225106628538924e-07, Validation Loss: 9.295147866239882e-07\n",
      "Epoch 3371, Training Loss: 5.801291536045028e-07, Validation Loss: 1.0002677596669965e-06\n",
      "Epoch 3372, Training Loss: 7.303971756300598e-07, Validation Loss: 1.1585135067944528e-06\n",
      "Epoch 3373, Training Loss: 6.513286621157022e-07, Validation Loss: 1.0363289421854988e-06\n",
      "Epoch 3374, Training Loss: 8.203514880733564e-07, Validation Loss: 9.854516966917835e-07\n",
      "Epoch 3375, Training Loss: 8.338047337019816e-07, Validation Loss: 1.1962500813786588e-06\n",
      "Epoch 3376, Training Loss: 8.920860068428738e-07, Validation Loss: 8.971574479613076e-07\n",
      "Epoch 3377, Training Loss: 1.4668885341961868e-06, Validation Loss: 1.96929701140072e-06\n",
      "Epoch 3378, Training Loss: 1.2286116088944254e-06, Validation Loss: 1.3646065913148467e-06\n",
      "Epoch 3379, Training Loss: 1.0640250138749252e-06, Validation Loss: 1.016229858740502e-06\n",
      "Epoch 3380, Training Loss: 6.302690280790557e-07, Validation Loss: 8.926269733501323e-07\n",
      "Epoch 3381, Training Loss: 1.6493962675667717e-06, Validation Loss: 1.3241255922100214e-06\n",
      "Epoch 3382, Training Loss: 6.156777772048372e-07, Validation Loss: 9.071029106510819e-07\n",
      "Epoch 3383, Training Loss: 8.613805562163179e-07, Validation Loss: 1.031872790887574e-06\n",
      "Epoch 3384, Training Loss: 1.1481633919174783e-06, Validation Loss: 1.1409594103816124e-06\n",
      "Epoch 3385, Training Loss: 1.7552854387758998e-06, Validation Loss: 9.259151687972477e-07\n",
      "Epoch 3386, Training Loss: 1.4640315839642426e-06, Validation Loss: 9.889438726725872e-07\n",
      "Epoch 3387, Training Loss: 7.331117330977577e-07, Validation Loss: 1.0600471919813942e-06\n",
      "Epoch 3388, Training Loss: 8.583312478549487e-07, Validation Loss: 1.0352157293599685e-06\n",
      "Epoch 3389, Training Loss: 7.560787480542785e-07, Validation Loss: 9.15205340981432e-07\n",
      "Epoch 3390, Training Loss: 8.918975140659313e-07, Validation Loss: 1.0447114641175656e-06\n",
      "Epoch 3391, Training Loss: 1.0973672033287585e-06, Validation Loss: 9.471969518026121e-07\n",
      "Epoch 3392, Training Loss: 2.697077889024513e-06, Validation Loss: 2.7640918352129344e-06\n",
      "Epoch 3393, Training Loss: 7.831451398487843e-07, Validation Loss: 9.048203906832856e-07\n",
      "Epoch 3394, Training Loss: 1.0143925237571239e-06, Validation Loss: 1.0221477240720328e-06\n",
      "Epoch 3395, Training Loss: 1.4661361547041452e-06, Validation Loss: 2.040057100370826e-06\n",
      "Epoch 3396, Training Loss: 2.222075181634864e-06, Validation Loss: 1.0369987256555157e-06\n",
      "Epoch 3397, Training Loss: 1.8900582290370949e-06, Validation Loss: 1.2954084943242372e-06\n",
      "Epoch 3398, Training Loss: 1.102427631849423e-06, Validation Loss: 8.911249765217336e-07\n",
      "Epoch 3399, Training Loss: 2.240341245851596e-06, Validation Loss: 2.5352198403846502e-06\n",
      "Epoch 3400, Training Loss: 1.0610705203362158e-06, Validation Loss: 1.2829746919656815e-06\n",
      "Epoch 3401, Training Loss: 7.591840471832256e-07, Validation Loss: 8.864046858043424e-07\n",
      "Epoch 3402, Training Loss: 8.30300677989726e-07, Validation Loss: 9.238628027635789e-07\n",
      "Epoch 3403, Training Loss: 1.2126954516133992e-06, Validation Loss: 1.311087058424465e-06\n",
      "Epoch 3404, Training Loss: 8.917554623621982e-07, Validation Loss: 9.195428111860953e-07\n",
      "Epoch 3405, Training Loss: 1.0119990747625707e-06, Validation Loss: 9.797738894454817e-07\n",
      "Epoch 3406, Training Loss: 5.983932283015747e-07, Validation Loss: 9.929805043327995e-07\n",
      "Epoch 3407, Training Loss: 2.9825212095602183e-06, Validation Loss: 3.078290280433402e-06\n",
      "Epoch 3408, Training Loss: 1.5509765489696292e-06, Validation Loss: 9.097147768620077e-07\n",
      "Epoch 3409, Training Loss: 1.2426015700839343e-06, Validation Loss: 2.2489629362874895e-06\n",
      "Epoch 3410, Training Loss: 9.188058243125852e-07, Validation Loss: 1.001549358448802e-06\n",
      "Epoch 3411, Training Loss: 1.465714376536198e-06, Validation Loss: 1.2978503033099613e-06\n",
      "Epoch 3412, Training Loss: 2.2903664103068877e-06, Validation Loss: 1.339960999612926e-06\n",
      "Epoch 3413, Training Loss: 2.6160967081523268e-06, Validation Loss: 1.1206878938737413e-06\n",
      "Epoch 3414, Training Loss: 5.388063755162875e-07, Validation Loss: 1.0028016890249142e-06\n",
      "Epoch 3415, Training Loss: 8.138653129208251e-07, Validation Loss: 1.0168934922594823e-06\n",
      "Epoch 3416, Training Loss: 2.060432052530814e-06, Validation Loss: 9.542882551998961e-07\n",
      "Epoch 3417, Training Loss: 1.6448213955300162e-06, Validation Loss: 8.853474067219413e-07\n",
      "Epoch 3418, Training Loss: 6.503244094346883e-07, Validation Loss: 9.488225398008639e-07\n",
      "Epoch 3419, Training Loss: 1.397388473378669e-06, Validation Loss: 1.4682334147286741e-06\n",
      "Epoch 3420, Training Loss: 6.725402954543824e-07, Validation Loss: 9.58516821741402e-07\n",
      "Epoch 3421, Training Loss: 2.031930307566654e-06, Validation Loss: 9.443490335192469e-07\n",
      "Epoch 3422, Training Loss: 1.38089762913296e-06, Validation Loss: 1.0825912669194512e-06\n",
      "Epoch 3423, Training Loss: 6.862700274723466e-07, Validation Loss: 1.0062695619097352e-06\n",
      "Epoch 3424, Training Loss: 8.108225983960438e-07, Validation Loss: 9.251618595351447e-07\n",
      "Epoch 3425, Training Loss: 2.3604011403222103e-06, Validation Loss: 1.2384395326091852e-06\n",
      "Epoch 3426, Training Loss: 1.0605160696286475e-06, Validation Loss: 9.470132731563631e-07\n",
      "Epoch 3427, Training Loss: 1.320210117228271e-06, Validation Loss: 1.5793168665895443e-06\n",
      "Epoch 3428, Training Loss: 1.378478373226244e-06, Validation Loss: 1.2634761997916686e-06\n",
      "Epoch 3429, Training Loss: 9.872170494418242e-07, Validation Loss: 9.077536635237601e-07\n",
      "Epoch 3430, Training Loss: 9.73693204286974e-07, Validation Loss: 9.192286622301311e-07\n",
      "Epoch 3431, Training Loss: 2.1480238956428366e-06, Validation Loss: 1.832263195587318e-06\n",
      "Epoch 3432, Training Loss: 6.092784587963251e-07, Validation Loss: 9.6173932493987e-07\n",
      "Epoch 3433, Training Loss: 8.262696837846306e-07, Validation Loss: 1.075211013217578e-06\n",
      "Epoch 3434, Training Loss: 1.4526801805914147e-06, Validation Loss: 1.0248589998279564e-06\n",
      "Epoch 3435, Training Loss: 6.393433409357385e-07, Validation Loss: 8.841641188140854e-07\n",
      "Epoch 3436, Training Loss: 7.114119284779008e-07, Validation Loss: 9.441088738761898e-07\n",
      "Epoch 3437, Training Loss: 1.4522536275762832e-06, Validation Loss: 9.695741266786038e-07\n",
      "Epoch 3438, Training Loss: 8.962479114416055e-07, Validation Loss: 9.981540313436974e-07\n",
      "Epoch 3439, Training Loss: 9.685171562523465e-07, Validation Loss: 9.446957555645182e-07\n",
      "Epoch 3440, Training Loss: 9.226736210621311e-07, Validation Loss: 9.515736656898889e-07\n",
      "Epoch 3441, Training Loss: 1.2263615190022392e-06, Validation Loss: 1.0400213496523298e-06\n",
      "Epoch 3442, Training Loss: 8.386593322029512e-07, Validation Loss: 9.010187623887435e-07\n",
      "Epoch 3443, Training Loss: 1.605836928320059e-06, Validation Loss: 1.2626635604826745e-06\n",
      "Epoch 3444, Training Loss: 1.4398918892766233e-06, Validation Loss: 1.3695487913172406e-06\n",
      "Epoch 3445, Training Loss: 1.6406875147367828e-06, Validation Loss: 8.919505926039576e-07\n",
      "Epoch 3446, Training Loss: 1.2582595445564948e-06, Validation Loss: 4.142308414444345e-06\n",
      "Epoch 3447, Training Loss: 7.407917337332037e-07, Validation Loss: 9.991876650120926e-07\n",
      "Epoch 3448, Training Loss: 2.7678761398419738e-06, Validation Loss: 1.6064512912669867e-06\n",
      "Epoch 3449, Training Loss: 1.1133631687698653e-06, Validation Loss: 8.949611844420627e-07\n",
      "Epoch 3450, Training Loss: 1.982004505407531e-06, Validation Loss: 1.1813067850910956e-06\n",
      "Epoch 3451, Training Loss: 2.2125909708847757e-06, Validation Loss: 2.753685677793803e-06\n",
      "Epoch 3452, Training Loss: 2.7559085538086947e-06, Validation Loss: 3.086151167689463e-06\n",
      "Epoch 3453, Training Loss: 3.3048513614630792e-06, Validation Loss: 5.3541258922989395e-06\n",
      "Epoch 3454, Training Loss: 1.2408431757648941e-06, Validation Loss: 1.347217120322167e-06\n",
      "Epoch 3455, Training Loss: 1.0005403510149335e-06, Validation Loss: 9.441648860053073e-07\n",
      "Epoch 3456, Training Loss: 9.855025382421445e-07, Validation Loss: 9.383140843221125e-07\n",
      "Epoch 3457, Training Loss: 8.47441640416946e-07, Validation Loss: 9.59051574575065e-07\n",
      "Epoch 3458, Training Loss: 1.1891202120750677e-06, Validation Loss: 1.1469183356697103e-06\n",
      "Epoch 3459, Training Loss: 1.2515806702140253e-06, Validation Loss: 1.0935424348749302e-06\n",
      "Epoch 3460, Training Loss: 9.93095795820409e-07, Validation Loss: 1.0795311103355493e-06\n",
      "Epoch 3461, Training Loss: 2.8774866223102435e-06, Validation Loss: 9.001680515663625e-07\n",
      "Epoch 3462, Training Loss: 1.494995331086102e-06, Validation Loss: 1.4231882261671794e-06\n",
      "Epoch 3463, Training Loss: 1.8906957848230377e-06, Validation Loss: 1.1725024373801925e-06\n",
      "Epoch 3464, Training Loss: 4.389376499602804e-06, Validation Loss: 2.669226392221401e-06\n",
      "Epoch 3465, Training Loss: 7.161924031606759e-07, Validation Loss: 9.430930767312773e-07\n",
      "Epoch 3466, Training Loss: 7.982509373505309e-07, Validation Loss: 9.835392074674714e-07\n",
      "Epoch 3467, Training Loss: 1.0551134437264409e-06, Validation Loss: 1.0495981900119913e-06\n",
      "Epoch 3468, Training Loss: 1.498821802670136e-06, Validation Loss: 9.962809798661552e-07\n",
      "Epoch 3469, Training Loss: 8.207503015000839e-07, Validation Loss: 9.243956412050303e-07\n",
      "Epoch 3470, Training Loss: 1.8154621557187056e-06, Validation Loss: 1.3172739403045945e-06\n",
      "Epoch 3471, Training Loss: 1.1587367225729395e-06, Validation Loss: 9.727967756062192e-07\n",
      "Epoch 3472, Training Loss: 8.053047508838063e-07, Validation Loss: 9.737272897008707e-07\n",
      "Epoch 3473, Training Loss: 7.339754120039288e-07, Validation Loss: 1.0780139549604046e-06\n",
      "Epoch 3474, Training Loss: 1.8629571059136651e-06, Validation Loss: 2.8926599671724762e-06\n",
      "Epoch 3475, Training Loss: 7.988712695805589e-07, Validation Loss: 9.264000960208893e-07\n",
      "Epoch 3476, Training Loss: 5.330437033990165e-07, Validation Loss: 9.460472777255125e-07\n",
      "Epoch 3477, Training Loss: 9.61979139901814e-07, Validation Loss: 9.877134403899862e-07\n",
      "Epoch 3478, Training Loss: 2.068044295810978e-06, Validation Loss: 1.4910231375961881e-06\n",
      "Epoch 3479, Training Loss: 1.5188164752544253e-06, Validation Loss: 1.7157327803195898e-06\n",
      "Epoch 3480, Training Loss: 1.7719430616125464e-06, Validation Loss: 1.6852669819090112e-06\n",
      "Epoch 3481, Training Loss: 1.1598891660469235e-06, Validation Loss: 1.2784431016666385e-06\n",
      "Epoch 3482, Training Loss: 1.1458846529421862e-06, Validation Loss: 1.0517442387067228e-06\n",
      "Epoch 3483, Training Loss: 7.070788683449791e-07, Validation Loss: 1.039571459336246e-06\n",
      "Epoch 3484, Training Loss: 6.457341896748403e-07, Validation Loss: 9.424338094747797e-07\n",
      "Epoch 3485, Training Loss: 7.707993745498243e-07, Validation Loss: 1.3057754124054751e-06\n",
      "Epoch 3486, Training Loss: 7.813632691977546e-07, Validation Loss: 9.382092386273537e-07\n",
      "Epoch 3487, Training Loss: 6.730183486070018e-07, Validation Loss: 9.17321218492848e-07\n",
      "Epoch 3488, Training Loss: 1.0043524980574148e-06, Validation Loss: 1.0042035599726197e-06\n",
      "Epoch 3489, Training Loss: 6.67554559186101e-07, Validation Loss: 9.165875035554923e-07\n",
      "Epoch 3490, Training Loss: 2.4347657472389983e-06, Validation Loss: 1.920345912396303e-06\n",
      "Epoch 3491, Training Loss: 8.825113582133781e-07, Validation Loss: 9.303443728750626e-07\n",
      "Epoch 3492, Training Loss: 1.4687366274301894e-06, Validation Loss: 1.7671635134412928e-06\n",
      "Epoch 3493, Training Loss: 4.83215171698248e-06, Validation Loss: 9.268999372197521e-07\n",
      "Epoch 3494, Training Loss: 1.5467365983568016e-06, Validation Loss: 1.306344406338832e-06\n",
      "Epoch 3495, Training Loss: 5.624294772132998e-07, Validation Loss: 1.0692077913376077e-06\n",
      "Epoch 3496, Training Loss: 1.6735650660848478e-06, Validation Loss: 1.5591569085497276e-06\n",
      "Epoch 3497, Training Loss: 1.3195760857342975e-06, Validation Loss: 9.650331126820698e-07\n",
      "Epoch 3498, Training Loss: 1.233354282703658e-06, Validation Loss: 2.419410238408496e-06\n",
      "Epoch 3499, Training Loss: 8.861994729159051e-07, Validation Loss: 1.012352939527379e-06\n",
      "Epoch 3500, Training Loss: 7.00108273576916e-07, Validation Loss: 8.977826383123182e-07\n",
      "Epoch 3501, Training Loss: 1.1431096709202393e-06, Validation Loss: 1.6740019168102928e-06\n",
      "Epoch 3502, Training Loss: 1.2725024589599343e-06, Validation Loss: 1.10173221127338e-06\n",
      "Epoch 3503, Training Loss: 1.524021399745834e-06, Validation Loss: 1.1875743194552088e-06\n",
      "Epoch 3504, Training Loss: 6.20113269178546e-07, Validation Loss: 1.0253424339161881e-06\n",
      "Epoch 3505, Training Loss: 9.542533234707662e-07, Validation Loss: 8.992374078542334e-07\n",
      "Epoch 3506, Training Loss: 1.1752601949410746e-06, Validation Loss: 1.153003714754994e-06\n",
      "Epoch 3507, Training Loss: 1.0584856227069395e-06, Validation Loss: 9.252730260732567e-07\n",
      "Epoch 3508, Training Loss: 1.0774762131404714e-06, Validation Loss: 1.6321299176090444e-06\n",
      "Epoch 3509, Training Loss: 1.4059417026146548e-06, Validation Loss: 1.570458497174334e-06\n",
      "Epoch 3510, Training Loss: 2.371769369347021e-06, Validation Loss: 1.0918481854575944e-06\n",
      "Epoch 3511, Training Loss: 1.6311012132064207e-06, Validation Loss: 1.4798573084347013e-06\n",
      "Epoch 3512, Training Loss: 9.528716873319354e-07, Validation Loss: 1.580451666880082e-06\n",
      "Epoch 3513, Training Loss: 8.428659725723264e-07, Validation Loss: 1.0920802134142444e-06\n",
      "Epoch 3514, Training Loss: 8.97628069651546e-07, Validation Loss: 9.082482778435377e-07\n",
      "Epoch 3515, Training Loss: 6.308823685685638e-07, Validation Loss: 9.319206139066032e-07\n",
      "Epoch 3516, Training Loss: 1.2723467079922557e-06, Validation Loss: 1.0204180108784078e-06\n",
      "Epoch 3517, Training Loss: 5.572388090513414e-06, Validation Loss: 7.636591464201927e-06\n",
      "Epoch 3518, Training Loss: 1.318544718742487e-06, Validation Loss: 1.4704904996199202e-06\n",
      "Epoch 3519, Training Loss: 2.2286674266069895e-06, Validation Loss: 1.6188667237876906e-06\n",
      "Epoch 3520, Training Loss: 5.905521334170771e-07, Validation Loss: 9.112217961866257e-07\n",
      "Epoch 3521, Training Loss: 7.190251380961854e-07, Validation Loss: 1.0159858219780128e-06\n",
      "Epoch 3522, Training Loss: 6.886898518132512e-07, Validation Loss: 8.964817422759893e-07\n",
      "Epoch 3523, Training Loss: 5.189407374928123e-07, Validation Loss: 8.958407527130753e-07\n",
      "Epoch 3524, Training Loss: 1.3209933058533352e-06, Validation Loss: 9.196738349045094e-07\n",
      "Epoch 3525, Training Loss: 2.0323311673564604e-06, Validation Loss: 3.863286077262998e-06\n",
      "Epoch 3526, Training Loss: 3.1706513254903257e-06, Validation Loss: 1.7932140767726966e-06\n",
      "Epoch 3527, Training Loss: 1.593692104506772e-06, Validation Loss: 2.0416889275679026e-06\n",
      "Epoch 3528, Training Loss: 2.391052248640335e-06, Validation Loss: 2.966703399359141e-06\n",
      "Epoch 3529, Training Loss: 1.6420825659224647e-06, Validation Loss: 3.3603022135938757e-06\n",
      "Epoch 3530, Training Loss: 1.1243133712923736e-06, Validation Loss: 9.930795045689754e-07\n",
      "Epoch 3531, Training Loss: 7.514190656365827e-07, Validation Loss: 9.039480107513688e-07\n",
      "Epoch 3532, Training Loss: 1.7436220787203638e-06, Validation Loss: 8.967567501933479e-07\n",
      "Epoch 3533, Training Loss: 9.579658808434033e-07, Validation Loss: 1.0663034867361227e-06\n",
      "Epoch 3534, Training Loss: 8.066842269727204e-07, Validation Loss: 9.979604478222272e-07\n",
      "Epoch 3535, Training Loss: 5.812910330860177e-07, Validation Loss: 9.018788675817562e-07\n",
      "Epoch 3536, Training Loss: 1.113714915845776e-06, Validation Loss: 9.426930169130249e-07\n",
      "Epoch 3537, Training Loss: 1.8786743112286786e-06, Validation Loss: 1.9993093959273655e-06\n",
      "Epoch 3538, Training Loss: 2.6182449346379144e-06, Validation Loss: 9.285368061022278e-07\n",
      "Epoch 3539, Training Loss: 9.751920515554957e-07, Validation Loss: 8.887237647799061e-07\n",
      "Epoch 3540, Training Loss: 2.7159464934811695e-06, Validation Loss: 1.6263904010386095e-06\n",
      "Epoch 3541, Training Loss: 6.795729632358416e-07, Validation Loss: 9.428648292240416e-07\n",
      "Epoch 3542, Training Loss: 1.5738569345558062e-06, Validation Loss: 1.5716276616649628e-06\n",
      "Epoch 3543, Training Loss: 1.5343210861828993e-06, Validation Loss: 9.664023966011581e-07\n",
      "Epoch 3544, Training Loss: 7.802483992236375e-07, Validation Loss: 9.164198902352413e-07\n",
      "Epoch 3545, Training Loss: 1.2102552773285424e-06, Validation Loss: 1.133694829778581e-06\n",
      "Epoch 3546, Training Loss: 1.5581363186356612e-06, Validation Loss: 1.784202774302731e-06\n",
      "Epoch 3547, Training Loss: 5.540111374102707e-07, Validation Loss: 9.126500093521931e-07\n",
      "Epoch 3548, Training Loss: 1.1905804058187641e-06, Validation Loss: 1.4950998867708066e-06\n",
      "Epoch 3549, Training Loss: 7.943972377688624e-07, Validation Loss: 1.04694318018924e-06\n",
      "Epoch 3550, Training Loss: 8.622188829576771e-07, Validation Loss: 1.145531483694624e-06\n",
      "Epoch 3551, Training Loss: 6.633085263274552e-07, Validation Loss: 9.085949222269406e-07\n",
      "Epoch 3552, Training Loss: 1.228853989232448e-06, Validation Loss: 1.4296058228123651e-06\n",
      "Epoch 3553, Training Loss: 1.2812928389394074e-06, Validation Loss: 1.2151752703812814e-06\n",
      "Epoch 3554, Training Loss: 8.665341511004954e-07, Validation Loss: 9.445618990887209e-07\n",
      "Epoch 3555, Training Loss: 1.3016615412198007e-06, Validation Loss: 9.012171466446877e-07\n",
      "Epoch 3556, Training Loss: 1.3520184438675642e-06, Validation Loss: 1.917744031981007e-06\n",
      "Epoch 3557, Training Loss: 8.823698181004147e-07, Validation Loss: 1.798711516773443e-06\n",
      "Epoch 3558, Training Loss: 1.7909405869431794e-06, Validation Loss: 1.7378376801070482e-06\n",
      "Epoch 3559, Training Loss: 1.793457158782985e-06, Validation Loss: 1.7868664243251492e-06\n",
      "Epoch 3560, Training Loss: 1.3285207387525588e-06, Validation Loss: 1.085062574199994e-06\n",
      "Epoch 3561, Training Loss: 6.456230039475486e-07, Validation Loss: 9.271734101837714e-07\n",
      "Epoch 3562, Training Loss: 2.0300421965657733e-06, Validation Loss: 1.9396433971897246e-06\n",
      "Epoch 3563, Training Loss: 1.2632619927899214e-06, Validation Loss: 9.684321309888004e-07\n",
      "Epoch 3564, Training Loss: 1.6525214050489012e-06, Validation Loss: 9.008545129678852e-07\n",
      "Epoch 3565, Training Loss: 1.2000660944977426e-06, Validation Loss: 1.950787703694943e-06\n",
      "Epoch 3566, Training Loss: 2.7823577966046287e-06, Validation Loss: 2.7957537701353616e-06\n",
      "Epoch 3567, Training Loss: 1.5049344028739142e-06, Validation Loss: 1.7849841613172713e-06\n",
      "Epoch 3568, Training Loss: 7.555026400041243e-07, Validation Loss: 8.92901273802265e-07\n",
      "Epoch 3569, Training Loss: 3.074741925956914e-06, Validation Loss: 2.4744285287765214e-06\n",
      "Epoch 3570, Training Loss: 1.0464424349265755e-06, Validation Loss: 1.2066906971334797e-06\n",
      "Epoch 3571, Training Loss: 9.759781960383407e-07, Validation Loss: 9.046984041633203e-07\n",
      "Epoch 3572, Training Loss: 1.9192152649338823e-06, Validation Loss: 2.113385089701789e-06\n",
      "Epoch 3573, Training Loss: 8.900215107132681e-07, Validation Loss: 9.973377334184768e-07\n",
      "Epoch 3574, Training Loss: 1.311582195739902e-06, Validation Loss: 9.038164309088058e-07\n",
      "Epoch 3575, Training Loss: 7.36555875846534e-07, Validation Loss: 9.210921509791452e-07\n",
      "Epoch 3576, Training Loss: 7.868979992053937e-07, Validation Loss: 9.53599520061619e-07\n",
      "Epoch 3577, Training Loss: 7.594703106406087e-07, Validation Loss: 8.938746635843475e-07\n",
      "Epoch 3578, Training Loss: 1.857895313150948e-06, Validation Loss: 1.7505113309180971e-06\n",
      "Epoch 3579, Training Loss: 1.638179924157157e-06, Validation Loss: 1.6636507461663794e-06\n",
      "Epoch 3580, Training Loss: 7.553881005151197e-07, Validation Loss: 9.443097250278946e-07\n",
      "Epoch 3581, Training Loss: 1.786920620361343e-06, Validation Loss: 2.831125622288959e-06\n",
      "Epoch 3582, Training Loss: 1.8232651655125665e-06, Validation Loss: 1.3347342410936549e-06\n",
      "Epoch 3583, Training Loss: 1.2292676956349169e-06, Validation Loss: 8.983800354900459e-07\n",
      "Epoch 3584, Training Loss: 1.1295819604129065e-06, Validation Loss: 8.818489010273144e-07\n",
      "Epoch 3585, Training Loss: 6.684453524030687e-07, Validation Loss: 8.967124291624262e-07\n",
      "Epoch 3586, Training Loss: 1.4119195839157328e-06, Validation Loss: 1.2723395757724586e-06\n",
      "Epoch 3587, Training Loss: 8.972944556262519e-07, Validation Loss: 8.972366064047313e-07\n",
      "Epoch 3588, Training Loss: 6.173427209432703e-07, Validation Loss: 9.457391684733173e-07\n",
      "Epoch 3589, Training Loss: 8.431053970525682e-07, Validation Loss: 9.449368297711169e-07\n",
      "Epoch 3590, Training Loss: 6.951079285499873e-07, Validation Loss: 9.384929745439274e-07\n",
      "Epoch 3591, Training Loss: 6.487804853350099e-07, Validation Loss: 1.3241981367225275e-06\n",
      "Epoch 3592, Training Loss: 1.297091671403905e-06, Validation Loss: 9.437851726914371e-07\n",
      "Epoch 3593, Training Loss: 1.6542643379580113e-06, Validation Loss: 3.414503522829474e-06\n",
      "Epoch 3594, Training Loss: 7.35742787583149e-07, Validation Loss: 1.0146203800585374e-06\n",
      "Epoch 3595, Training Loss: 1.3706943491342827e-06, Validation Loss: 9.161595267394393e-07\n",
      "Epoch 3596, Training Loss: 1.1817129461633158e-06, Validation Loss: 9.346277808246993e-07\n",
      "Epoch 3597, Training Loss: 6.837810815341072e-07, Validation Loss: 9.058348693867345e-07\n",
      "Epoch 3598, Training Loss: 7.415524123643991e-07, Validation Loss: 9.325678647540018e-07\n",
      "Epoch 3599, Training Loss: 1.2291902748984285e-06, Validation Loss: 2.0194548121896336e-06\n",
      "Epoch 3600, Training Loss: 1.3413207398116356e-06, Validation Loss: 1.3241196044618614e-06\n",
      "Epoch 3601, Training Loss: 7.877653729337908e-07, Validation Loss: 8.967743995318142e-07\n",
      "Epoch 3602, Training Loss: 8.756450142755057e-07, Validation Loss: 9.164334482957896e-07\n",
      "Epoch 3603, Training Loss: 6.067905360396253e-07, Validation Loss: 9.168706849771871e-07\n",
      "Epoch 3604, Training Loss: 1.1742557717298041e-06, Validation Loss: 8.939134487179802e-07\n",
      "Epoch 3605, Training Loss: 5.64604874853103e-07, Validation Loss: 8.863200613412084e-07\n",
      "Epoch 3606, Training Loss: 7.08469883647922e-07, Validation Loss: 8.795014908573185e-07\n",
      "Epoch 3607, Training Loss: 9.621732033338048e-07, Validation Loss: 1.216548681965317e-06\n",
      "Epoch 3608, Training Loss: 5.725502774112101e-07, Validation Loss: 9.77796361031283e-07\n",
      "Epoch 3609, Training Loss: 6.7249800395075e-07, Validation Loss: 9.54556710012416e-07\n",
      "Epoch 3610, Training Loss: 3.664994437713176e-06, Validation Loss: 1.781577445434065e-06\n",
      "Epoch 3611, Training Loss: 1.8139829762731097e-06, Validation Loss: 1.6029099723402778e-06\n",
      "Epoch 3612, Training Loss: 8.408333087572828e-07, Validation Loss: 1.1496566091531129e-06\n",
      "Epoch 3613, Training Loss: 6.19339402874175e-07, Validation Loss: 9.312578031689566e-07\n",
      "Epoch 3614, Training Loss: 4.521349126207497e-07, Validation Loss: 9.126555573784919e-07\n",
      "Epoch 3615, Training Loss: 1.451138814445585e-06, Validation Loss: 1.004595783364389e-06\n",
      "Epoch 3616, Training Loss: 1.3058919421382598e-06, Validation Loss: 1.089935071136705e-06\n",
      "Epoch 3617, Training Loss: 9.531146929475653e-07, Validation Loss: 9.027587142769033e-07\n",
      "Epoch 3618, Training Loss: 8.898573469195981e-07, Validation Loss: 9.328295814490268e-07\n",
      "Epoch 3619, Training Loss: 4.803746378456708e-07, Validation Loss: 8.861729945634857e-07\n",
      "Epoch 3620, Training Loss: 3.0688713650306454e-06, Validation Loss: 7.384074463047069e-06\n",
      "Epoch 3621, Training Loss: 1.7716574802761897e-05, Validation Loss: 9.456519318559536e-06\n",
      "Epoch 3622, Training Loss: 1.625770892133005e-06, Validation Loss: 1.3076166431543753e-06\n",
      "Epoch 3623, Training Loss: 9.954042070603464e-07, Validation Loss: 9.478625996423619e-07\n",
      "Epoch 3624, Training Loss: 8.012297030290938e-07, Validation Loss: 8.9621476086661e-07\n",
      "Epoch 3625, Training Loss: 1.4636236755904974e-06, Validation Loss: 1.0730677096617117e-06\n",
      "Epoch 3626, Training Loss: 6.903923690515512e-07, Validation Loss: 1.2120260222380552e-06\n",
      "Epoch 3627, Training Loss: 1.1207744137209374e-06, Validation Loss: 9.511801061157162e-07\n",
      "Epoch 3628, Training Loss: 3.1035465326567646e-06, Validation Loss: 4.4632880463682714e-06\n",
      "Epoch 3629, Training Loss: 2.2496765268442687e-06, Validation Loss: 1.348330148135041e-06\n",
      "Epoch 3630, Training Loss: 8.562348625673621e-07, Validation Loss: 1.049159904623825e-06\n",
      "Epoch 3631, Training Loss: 1.0082496828545118e-06, Validation Loss: 8.999832344224664e-07\n",
      "Epoch 3632, Training Loss: 1.5708862974861404e-06, Validation Loss: 9.835778611733267e-07\n",
      "Epoch 3633, Training Loss: 9.624973245081492e-07, Validation Loss: 1.018760780104e-06\n",
      "Epoch 3634, Training Loss: 7.338635441556107e-07, Validation Loss: 9.482021677442366e-07\n",
      "Epoch 3635, Training Loss: 8.903763841772161e-07, Validation Loss: 9.536066001447771e-07\n",
      "Epoch 3636, Training Loss: 8.74409920470498e-07, Validation Loss: 1.2648376684647835e-06\n",
      "Epoch 3637, Training Loss: 1.2106456779292785e-06, Validation Loss: 8.931855915492471e-07\n",
      "Epoch 3638, Training Loss: 2.6087936930707656e-06, Validation Loss: 1.6307098632843837e-06\n",
      "Epoch 3639, Training Loss: 1.626962102818652e-06, Validation Loss: 1.3451924334128835e-06\n",
      "Epoch 3640, Training Loss: 6.116214308349299e-07, Validation Loss: 9.958027999925383e-07\n",
      "Epoch 3641, Training Loss: 7.81315293352236e-07, Validation Loss: 9.099895112430418e-07\n",
      "Epoch 3642, Training Loss: 1.2475475159590133e-06, Validation Loss: 1.1025250547712405e-06\n",
      "Epoch 3643, Training Loss: 9.096951316678314e-07, Validation Loss: 8.828688365179439e-07\n",
      "Epoch 3644, Training Loss: 7.983641125974827e-07, Validation Loss: 9.20089560696588e-07\n",
      "Epoch 3645, Training Loss: 1.1192435067641782e-06, Validation Loss: 8.933235869138516e-07\n",
      "Epoch 3646, Training Loss: 6.590993052668637e-06, Validation Loss: 3.040634655222374e-06\n",
      "Epoch 3647, Training Loss: 1.093377136385243e-06, Validation Loss: 1.11173426013839e-06\n",
      "Epoch 3648, Training Loss: 9.479678055868135e-07, Validation Loss: 1.334941404218243e-06\n",
      "Epoch 3649, Training Loss: 9.85270389719517e-07, Validation Loss: 9.372792980357306e-07\n",
      "Epoch 3650, Training Loss: 8.56265160109615e-07, Validation Loss: 1.1450662250180283e-06\n",
      "Epoch 3651, Training Loss: 2.3360153136309236e-06, Validation Loss: 1.4385064127674494e-06\n",
      "Epoch 3652, Training Loss: 3.309698513476178e-06, Validation Loss: 1.591001147919701e-06\n",
      "Epoch 3653, Training Loss: 8.970202429736673e-07, Validation Loss: 2.7525181530728813e-06\n",
      "Epoch 3654, Training Loss: 6.420109457394574e-07, Validation Loss: 9.160636188576614e-07\n",
      "Epoch 3655, Training Loss: 9.447387583350064e-07, Validation Loss: 1.2275473139945148e-06\n",
      "Epoch 3656, Training Loss: 1.1435777196311392e-06, Validation Loss: 8.677182378937253e-07\n",
      "Epoch 3657, Training Loss: 8.664702590976958e-07, Validation Loss: 9.53497326999397e-07\n",
      "Epoch 3658, Training Loss: 7.990281574166147e-07, Validation Loss: 9.461747780571073e-07\n",
      "Epoch 3659, Training Loss: 7.662420102860779e-07, Validation Loss: 8.924082751752052e-07\n",
      "Epoch 3660, Training Loss: 7.287690095836297e-07, Validation Loss: 9.507308441547498e-07\n",
      "Epoch 3661, Training Loss: 6.788486643927172e-07, Validation Loss: 1.0097726270258506e-06\n",
      "Epoch 3662, Training Loss: 9.247413572666119e-07, Validation Loss: 8.954253506026304e-07\n",
      "Epoch 3663, Training Loss: 1.1047443422285141e-06, Validation Loss: 9.157392150954927e-07\n",
      "Epoch 3664, Training Loss: 1.0327825066269725e-06, Validation Loss: 9.428320193474992e-07\n",
      "Epoch 3665, Training Loss: 1.0483845471753739e-06, Validation Loss: 8.891949914723922e-07\n",
      "Epoch 3666, Training Loss: 1.151960191236867e-06, Validation Loss: 1.0865692258526894e-06\n",
      "Epoch 3667, Training Loss: 1.0580943126115017e-06, Validation Loss: 9.202341314327207e-07\n",
      "Epoch 3668, Training Loss: 5.777670253337419e-07, Validation Loss: 1.0042371589589162e-06\n",
      "Epoch 3669, Training Loss: 1.037591005115246e-06, Validation Loss: 8.852890180310314e-07\n",
      "Epoch 3670, Training Loss: 8.585592468079994e-07, Validation Loss: 1.0296972982070227e-06\n",
      "Epoch 3671, Training Loss: 8.693521067471011e-07, Validation Loss: 9.305707592129056e-07\n",
      "Epoch 3672, Training Loss: 1.7986508282774594e-06, Validation Loss: 1.9979267069180758e-06\n",
      "Epoch 3673, Training Loss: 1.965145202120766e-06, Validation Loss: 1.1627110942157695e-06\n",
      "Epoch 3674, Training Loss: 7.064615488161508e-07, Validation Loss: 8.883128804540505e-07\n",
      "Epoch 3675, Training Loss: 1.2618529581231996e-06, Validation Loss: 1.480997732603106e-06\n",
      "Epoch 3676, Training Loss: 8.220262088798336e-07, Validation Loss: 8.773954966785545e-07\n",
      "Epoch 3677, Training Loss: 9.168278666038532e-06, Validation Loss: 8.611054225555238e-06\n",
      "Epoch 3678, Training Loss: 9.522961477159697e-07, Validation Loss: 8.984865796082306e-07\n",
      "Epoch 3679, Training Loss: 7.685462151130196e-07, Validation Loss: 9.449985403624602e-07\n",
      "Epoch 3680, Training Loss: 1.23123152206972e-06, Validation Loss: 9.704735868876778e-07\n",
      "Epoch 3681, Training Loss: 1.589639737176185e-06, Validation Loss: 1.4362000703383106e-06\n",
      "Epoch 3682, Training Loss: 1.4704758086736547e-06, Validation Loss: 1.0883343327639952e-06\n",
      "Epoch 3683, Training Loss: 1.353395191472373e-06, Validation Loss: 8.94107863710224e-07\n",
      "Epoch 3684, Training Loss: 1.0244928034808254e-06, Validation Loss: 9.914481062319601e-07\n",
      "Epoch 3685, Training Loss: 8.079459803411737e-07, Validation Loss: 8.973445053514088e-07\n",
      "Epoch 3686, Training Loss: 8.336505175066122e-07, Validation Loss: 9.63085481190742e-07\n",
      "Epoch 3687, Training Loss: 8.01961846264021e-07, Validation Loss: 1.4418281987899178e-06\n",
      "Epoch 3688, Training Loss: 5.337041670827603e-07, Validation Loss: 8.929386644618357e-07\n",
      "Epoch 3689, Training Loss: 1.8976919591295882e-06, Validation Loss: 1.0784160908306554e-06\n",
      "Epoch 3690, Training Loss: 7.966923476487864e-07, Validation Loss: 9.457880968986768e-07\n",
      "Epoch 3691, Training Loss: 1.0569243613645085e-06, Validation Loss: 9.239446225289791e-07\n",
      "Epoch 3692, Training Loss: 1.5160374005063204e-06, Validation Loss: 1.829426987772358e-06\n",
      "Epoch 3693, Training Loss: 1.0146067097593914e-06, Validation Loss: 9.215261580854073e-07\n",
      "Epoch 3694, Training Loss: 1.1100669325969648e-06, Validation Loss: 1.3442148975007942e-06\n",
      "Epoch 3695, Training Loss: 1.8170115936300135e-06, Validation Loss: 1.1994250506258296e-06\n",
      "Epoch 3696, Training Loss: 1.1019463954653475e-06, Validation Loss: 9.689675845895795e-07\n",
      "Epoch 3697, Training Loss: 9.503243063591071e-07, Validation Loss: 9.050154235309069e-07\n",
      "Epoch 3698, Training Loss: 1.3429521459329408e-06, Validation Loss: 1.4428999742113386e-06\n",
      "Epoch 3699, Training Loss: 1.3051915175310569e-06, Validation Loss: 1.14407316524066e-06\n",
      "Epoch 3700, Training Loss: 7.89866817285656e-07, Validation Loss: 1.1896128207635154e-06\n",
      "Epoch 3701, Training Loss: 7.694761166021635e-07, Validation Loss: 1.1257648524244047e-06\n",
      "Epoch 3702, Training Loss: 9.294396932091331e-07, Validation Loss: 9.798572640775208e-07\n",
      "Epoch 3703, Training Loss: 1.0099132623508922e-06, Validation Loss: 1.07464507688086e-06\n",
      "Epoch 3704, Training Loss: 9.552328492645756e-07, Validation Loss: 9.797641226961474e-07\n",
      "Epoch 3705, Training Loss: 7.471434173567104e-07, Validation Loss: 9.893200879438375e-07\n",
      "Epoch 3706, Training Loss: 1.1237596027058316e-06, Validation Loss: 1.0200771804543684e-06\n",
      "Epoch 3707, Training Loss: 1.0274970918544568e-06, Validation Loss: 8.992365608148804e-07\n",
      "Epoch 3708, Training Loss: 1.0551353852861212e-06, Validation Loss: 9.570645324914671e-07\n",
      "Epoch 3709, Training Loss: 8.71581505634822e-07, Validation Loss: 9.500193294689392e-07\n",
      "Epoch 3710, Training Loss: 1.158015265900758e-06, Validation Loss: 9.872214421346862e-07\n",
      "Epoch 3711, Training Loss: 9.200197723657766e-07, Validation Loss: 9.015372829867519e-07\n",
      "Epoch 3712, Training Loss: 1.0154344636248425e-06, Validation Loss: 9.184743591133481e-07\n",
      "Epoch 3713, Training Loss: 1.0026574273069855e-06, Validation Loss: 9.053108538042162e-07\n",
      "Epoch 3714, Training Loss: 6.332797397590184e-07, Validation Loss: 9.980566947731882e-07\n",
      "Epoch 3715, Training Loss: 1.3722299172513885e-06, Validation Loss: 1.1321003731467452e-06\n",
      "Epoch 3716, Training Loss: 3.012848083017161e-06, Validation Loss: 1.9151573001558075e-06\n",
      "Epoch 3717, Training Loss: 1.0202522844338091e-06, Validation Loss: 8.948729672576206e-07\n",
      "Epoch 3718, Training Loss: 3.2241448479908286e-06, Validation Loss: 1.4410230106755745e-06\n",
      "Epoch 3719, Training Loss: 1.074626311492466e-06, Validation Loss: 1.031443912361109e-06\n",
      "Epoch 3720, Training Loss: 1.4048523553356063e-06, Validation Loss: 1.0286799863841305e-06\n",
      "Epoch 3721, Training Loss: 2.2827832708571805e-06, Validation Loss: 2.501726696533922e-06\n",
      "Epoch 3722, Training Loss: 8.194117526727496e-07, Validation Loss: 1.3733467343841466e-06\n",
      "Epoch 3723, Training Loss: 1.168747189694841e-06, Validation Loss: 9.367961313288036e-07\n",
      "Epoch 3724, Training Loss: 1.1841356126751634e-06, Validation Loss: 9.707654115865494e-07\n",
      "Epoch 3725, Training Loss: 6.742504865542287e-07, Validation Loss: 1.2666813514460144e-06\n",
      "Epoch 3726, Training Loss: 9.876376907413942e-07, Validation Loss: 9.470794477663869e-07\n",
      "Epoch 3727, Training Loss: 1.3334797586139757e-06, Validation Loss: 1.3389756189469738e-06\n",
      "Epoch 3728, Training Loss: 7.97240943484212e-07, Validation Loss: 9.98964057484694e-07\n",
      "Epoch 3729, Training Loss: 1.5291659565264126e-06, Validation Loss: 1.070230039124381e-06\n",
      "Epoch 3730, Training Loss: 2.8823051252402365e-06, Validation Loss: 2.4030860771698866e-06\n",
      "Epoch 3731, Training Loss: 1.0759621318356949e-06, Validation Loss: 9.268267288702492e-07\n",
      "Epoch 3732, Training Loss: 1.987204768738593e-06, Validation Loss: 1.0214651432295124e-06\n",
      "Epoch 3733, Training Loss: 7.626980504937819e-07, Validation Loss: 1.019691189201254e-06\n",
      "Epoch 3734, Training Loss: 7.647058737347834e-07, Validation Loss: 1.0810994021589195e-06\n",
      "Epoch 3735, Training Loss: 9.421220852345868e-07, Validation Loss: 1.0451221520258752e-06\n",
      "Epoch 3736, Training Loss: 7.717476364632603e-07, Validation Loss: 8.814266022170976e-07\n",
      "Epoch 3737, Training Loss: 1.0602134352666326e-06, Validation Loss: 1.1134999881636133e-06\n",
      "Epoch 3738, Training Loss: 1.5038219771668082e-06, Validation Loss: 9.89280143313078e-07\n",
      "Epoch 3739, Training Loss: 5.84134158998495e-06, Validation Loss: 4.198494145535374e-06\n",
      "Epoch 3740, Training Loss: 8.928619763537426e-07, Validation Loss: 9.221529013117268e-07\n",
      "Epoch 3741, Training Loss: 2.2557246666110586e-06, Validation Loss: 1.1695980321260292e-06\n",
      "Epoch 3742, Training Loss: 1.4262153626987129e-06, Validation Loss: 1.4604658306948035e-06\n",
      "Epoch 3743, Training Loss: 1.940879656103789e-06, Validation Loss: 1.0274363819969335e-06\n",
      "Epoch 3744, Training Loss: 7.837220437068027e-07, Validation Loss: 9.348323405569097e-07\n",
      "Epoch 3745, Training Loss: 1.1496407523736707e-06, Validation Loss: 1.1338722465972554e-06\n",
      "Epoch 3746, Training Loss: 9.342174962512217e-07, Validation Loss: 8.767937441001442e-07\n",
      "Epoch 3747, Training Loss: 7.613257366756443e-07, Validation Loss: 9.213928783711784e-07\n",
      "Epoch 3748, Training Loss: 1.3045776086073602e-06, Validation Loss: 1.970989206670687e-06\n",
      "Epoch 3749, Training Loss: 5.173493491383852e-07, Validation Loss: 8.924408826602118e-07\n",
      "Epoch 3750, Training Loss: 1.0549015314609278e-06, Validation Loss: 9.523050838996806e-07\n",
      "Epoch 3751, Training Loss: 1.0269925496686483e-06, Validation Loss: 9.748526199502381e-07\n",
      "Epoch 3752, Training Loss: 8.975152923085261e-07, Validation Loss: 1.4712955306002267e-06\n",
      "Epoch 3753, Training Loss: 7.421289183184854e-07, Validation Loss: 8.866561569256927e-07\n",
      "Epoch 3754, Training Loss: 7.676134146095137e-07, Validation Loss: 9.02709185012986e-07\n",
      "Epoch 3755, Training Loss: 1.2907565860587056e-06, Validation Loss: 8.951559995105984e-07\n",
      "Epoch 3756, Training Loss: 1.423531784894294e-06, Validation Loss: 1.271476620262153e-06\n",
      "Epoch 3757, Training Loss: 9.171893680104404e-07, Validation Loss: 9.593049841656165e-07\n",
      "Epoch 3758, Training Loss: 1.4658276086265687e-06, Validation Loss: 8.98208634479181e-07\n",
      "Epoch 3759, Training Loss: 1.0717670875237673e-06, Validation Loss: 9.598979598667127e-07\n",
      "Epoch 3760, Training Loss: 6.686602205263625e-07, Validation Loss: 9.347021705759584e-07\n",
      "Epoch 3761, Training Loss: 1.236100729329337e-06, Validation Loss: 9.339810991356094e-07\n",
      "Epoch 3762, Training Loss: 7.661534482394927e-07, Validation Loss: 1.2444441303171278e-06\n",
      "Epoch 3763, Training Loss: 1.2647003586607752e-06, Validation Loss: 1.6564710284429873e-06\n",
      "Epoch 3764, Training Loss: 1.0005499007093022e-06, Validation Loss: 9.894492155540696e-07\n",
      "Epoch 3765, Training Loss: 1.3335852599993814e-06, Validation Loss: 8.991524736487755e-07\n",
      "Epoch 3766, Training Loss: 8.667947781759722e-07, Validation Loss: 1.085068557784464e-06\n",
      "Epoch 3767, Training Loss: 5.456428198158392e-07, Validation Loss: 9.007689705016396e-07\n",
      "Epoch 3768, Training Loss: 2.344372205698164e-06, Validation Loss: 1.6822987641242169e-06\n",
      "Epoch 3769, Training Loss: 7.986479317878548e-07, Validation Loss: 9.824520694784987e-07\n",
      "Epoch 3770, Training Loss: 8.084388127826969e-07, Validation Loss: 9.930660654451348e-07\n",
      "Epoch 3771, Training Loss: 8.876765491550032e-07, Validation Loss: 9.325933081579319e-07\n",
      "Epoch 3772, Training Loss: 1.67531288752798e-06, Validation Loss: 9.179510004886866e-07\n",
      "Epoch 3773, Training Loss: 9.029264447235619e-07, Validation Loss: 8.810093117827604e-07\n",
      "Epoch 3774, Training Loss: 6.602069788641529e-07, Validation Loss: 9.311605524066656e-07\n",
      "Epoch 3775, Training Loss: 3.2961593205982354e-06, Validation Loss: 2.769171784560168e-06\n",
      "Epoch 3776, Training Loss: 1.8734139075604617e-06, Validation Loss: 2.77470588603166e-06\n",
      "Epoch 3777, Training Loss: 7.782273314660415e-07, Validation Loss: 1.0259927736784147e-06\n",
      "Epoch 3778, Training Loss: 8.107307394311647e-07, Validation Loss: 8.95094248911626e-07\n",
      "Epoch 3779, Training Loss: 2.6155194063903764e-06, Validation Loss: 2.9930640301442107e-06\n",
      "Epoch 3780, Training Loss: 8.614774742454756e-07, Validation Loss: 9.345654806186579e-07\n",
      "Epoch 3781, Training Loss: 1.3159773288862198e-06, Validation Loss: 9.400565224762425e-07\n",
      "Epoch 3782, Training Loss: 3.2029147405410185e-06, Validation Loss: 2.2856968588579095e-06\n",
      "Epoch 3783, Training Loss: 1.4468816971202614e-06, Validation Loss: 1.4108516271410556e-06\n",
      "Epoch 3784, Training Loss: 7.016809604465379e-07, Validation Loss: 9.043994363828518e-07\n",
      "Epoch 3785, Training Loss: 8.221823009080254e-07, Validation Loss: 9.213220956425969e-07\n",
      "Epoch 3786, Training Loss: 1.15132684186392e-06, Validation Loss: 1.4438202514217554e-06\n",
      "Epoch 3787, Training Loss: 6.668959713351796e-07, Validation Loss: 1.2042473102903073e-06\n",
      "Epoch 3788, Training Loss: 2.969598881463753e-06, Validation Loss: 1.5611564123762159e-06\n",
      "Epoch 3789, Training Loss: 8.598101430834504e-07, Validation Loss: 8.667161709688875e-07\n",
      "Epoch 3790, Training Loss: 7.625288844792522e-07, Validation Loss: 9.799743897610535e-07\n",
      "Epoch 3791, Training Loss: 1.501762199040968e-06, Validation Loss: 1.93448166645545e-06\n",
      "Epoch 3792, Training Loss: 1.3126700650900602e-06, Validation Loss: 1.5711663096156887e-06\n",
      "Epoch 3793, Training Loss: 2.1944288164377213e-06, Validation Loss: 1.6821515639273869e-06\n",
      "Epoch 3794, Training Loss: 1.9959852579631843e-06, Validation Loss: 1.6670340345956953e-06\n",
      "Epoch 3795, Training Loss: 6.092460580475745e-07, Validation Loss: 8.684905735894769e-07\n",
      "Epoch 3796, Training Loss: 1.3469600617099786e-06, Validation Loss: 1.6444196841751068e-06\n",
      "Epoch 3797, Training Loss: 8.670076567796059e-07, Validation Loss: 9.24032498833938e-07\n",
      "Epoch 3798, Training Loss: 1.6334109886884107e-06, Validation Loss: 1.37463293404331e-06\n",
      "Epoch 3799, Training Loss: 6.238903438315901e-07, Validation Loss: 8.821515643541286e-07\n",
      "Epoch 3800, Training Loss: 1.0897101674345322e-06, Validation Loss: 9.279532750981023e-07\n",
      "Epoch 3801, Training Loss: 1.3900933026889106e-06, Validation Loss: 1.7141658364455728e-06\n",
      "Epoch 3802, Training Loss: 8.87711735231278e-07, Validation Loss: 1.0573464820412089e-06\n",
      "Epoch 3803, Training Loss: 6.321399155240215e-07, Validation Loss: 9.134203144344755e-07\n",
      "Epoch 3804, Training Loss: 9.279293635700014e-07, Validation Loss: 9.286248938502224e-07\n",
      "Epoch 3805, Training Loss: 7.184210062405327e-07, Validation Loss: 8.848210549473198e-07\n",
      "Epoch 3806, Training Loss: 1.4660097349405987e-06, Validation Loss: 9.096946392662413e-07\n",
      "Epoch 3807, Training Loss: 8.435214908786293e-07, Validation Loss: 1.1558369496953534e-06\n",
      "Epoch 3808, Training Loss: 1.2199036518723005e-06, Validation Loss: 1.2181952308837587e-06\n",
      "Epoch 3809, Training Loss: 1.8670051531444187e-06, Validation Loss: 1.5032754262462215e-06\n",
      "Epoch 3810, Training Loss: 6.89595708536217e-07, Validation Loss: 9.463043640352904e-07\n",
      "Epoch 3811, Training Loss: 1.1364206784492126e-06, Validation Loss: 8.791700649415905e-07\n",
      "Epoch 3812, Training Loss: 2.8664999263128266e-06, Validation Loss: 5.877016577591592e-06\n",
      "Epoch 3813, Training Loss: 8.680232213009731e-07, Validation Loss: 9.664529533913348e-07\n",
      "Epoch 3814, Training Loss: 7.963899406604469e-07, Validation Loss: 1.1464310755125054e-06\n",
      "Epoch 3815, Training Loss: 1.144385578299989e-06, Validation Loss: 9.344731506137282e-07\n",
      "Epoch 3816, Training Loss: 3.452780219959095e-06, Validation Loss: 4.631372696317708e-06\n",
      "Epoch 3817, Training Loss: 2.4990245037770364e-06, Validation Loss: 1.2165341590148881e-06\n",
      "Epoch 3818, Training Loss: 9.321635161541053e-07, Validation Loss: 1.3967704264713152e-06\n",
      "Epoch 3819, Training Loss: 9.670791314420057e-07, Validation Loss: 8.652720257153994e-07\n",
      "Epoch 3820, Training Loss: 3.816124717559433e-06, Validation Loss: 3.6995329783804333e-06\n",
      "Epoch 3821, Training Loss: 5.914049552302458e-07, Validation Loss: 9.131655554463311e-07\n",
      "Epoch 3822, Training Loss: 1.0908854619628983e-06, Validation Loss: 1.0869048983700722e-06\n",
      "Epoch 3823, Training Loss: 1.0084718269354198e-06, Validation Loss: 1.3753270805302388e-06\n",
      "Epoch 3824, Training Loss: 6.407761929949629e-07, Validation Loss: 8.771312053749211e-07\n",
      "Epoch 3825, Training Loss: 1.4834943158348324e-06, Validation Loss: 1.1034143792918478e-06\n",
      "Epoch 3826, Training Loss: 5.382959898270201e-06, Validation Loss: 4.232097606893531e-06\n",
      "Epoch 3827, Training Loss: 6.451620038205874e-07, Validation Loss: 9.382988052093888e-07\n",
      "Epoch 3828, Training Loss: 7.095035243764869e-07, Validation Loss: 1.109624028771531e-06\n",
      "Epoch 3829, Training Loss: 1.2092671113350661e-06, Validation Loss: 1.0389743061869534e-06\n",
      "Epoch 3830, Training Loss: 6.637751539528836e-07, Validation Loss: 9.116203936445675e-07\n",
      "Epoch 3831, Training Loss: 1.684230255705188e-06, Validation Loss: 8.921606677770674e-07\n",
      "Epoch 3832, Training Loss: 1.067515540853492e-06, Validation Loss: 1.0624529566276307e-06\n",
      "Epoch 3833, Training Loss: 6.422593514798791e-07, Validation Loss: 8.769570230191918e-07\n",
      "Epoch 3834, Training Loss: 8.807909921415558e-07, Validation Loss: 8.72267051929713e-07\n",
      "Epoch 3835, Training Loss: 2.2439078293245984e-06, Validation Loss: 2.3452861522896077e-06\n",
      "Epoch 3836, Training Loss: 1.4694713854623842e-06, Validation Loss: 9.099575299407928e-07\n",
      "Epoch 3837, Training Loss: 5.974749228698784e-07, Validation Loss: 9.434556647885207e-07\n",
      "Epoch 3838, Training Loss: 2.060815859294962e-06, Validation Loss: 2.585533425450762e-06\n",
      "Epoch 3839, Training Loss: 6.897395223859348e-07, Validation Loss: 9.166171244076186e-07\n",
      "Epoch 3840, Training Loss: 8.905641948331322e-07, Validation Loss: 1.007188394138721e-06\n",
      "Epoch 3841, Training Loss: 1.3395663245319156e-06, Validation Loss: 1.3130887099908301e-06\n",
      "Epoch 3842, Training Loss: 1.48372396324703e-06, Validation Loss: 1.2541671210806706e-06\n",
      "Epoch 3843, Training Loss: 1.5543258768957458e-06, Validation Loss: 1.3844569257865486e-06\n",
      "Epoch 3844, Training Loss: 7.720008738942852e-07, Validation Loss: 9.447156704933891e-07\n",
      "Epoch 3845, Training Loss: 1.1112529136880767e-06, Validation Loss: 9.995826583438366e-07\n",
      "Epoch 3846, Training Loss: 5.857730229763547e-07, Validation Loss: 8.85256902585555e-07\n",
      "Epoch 3847, Training Loss: 5.523487516256864e-07, Validation Loss: 9.185626257189791e-07\n",
      "Epoch 3848, Training Loss: 8.804604476608802e-07, Validation Loss: 9.241540581231119e-07\n",
      "Epoch 3849, Training Loss: 1.263592935174529e-06, Validation Loss: 1.0941913915679846e-06\n",
      "Epoch 3850, Training Loss: 8.28629140414705e-07, Validation Loss: 1.0052587168403735e-06\n",
      "Epoch 3851, Training Loss: 1.4055387964617694e-06, Validation Loss: 9.538099627256216e-07\n",
      "Epoch 3852, Training Loss: 2.644370852067368e-06, Validation Loss: 2.271004512868397e-06\n",
      "Epoch 3853, Training Loss: 1.422342620571726e-06, Validation Loss: 9.487370288338375e-07\n",
      "Epoch 3854, Training Loss: 2.8077065508114174e-06, Validation Loss: 2.9619995318640505e-06\n",
      "Epoch 3855, Training Loss: 8.152503141900524e-06, Validation Loss: 3.7762427544606864e-05\n",
      "Epoch 3856, Training Loss: 8.572696970077232e-07, Validation Loss: 1.200632865744804e-06\n",
      "Epoch 3857, Training Loss: 9.612274425307987e-07, Validation Loss: 9.759915904477722e-07\n",
      "Epoch 3858, Training Loss: 9.33448404794035e-07, Validation Loss: 1.2268494075145615e-06\n",
      "Epoch 3859, Training Loss: 6.819597047069692e-07, Validation Loss: 9.370079081838794e-07\n",
      "Epoch 3860, Training Loss: 1.561582735121192e-06, Validation Loss: 1.6311462603486578e-06\n",
      "Epoch 3861, Training Loss: 1.6603761423539254e-06, Validation Loss: 1.8700045182262687e-06\n",
      "Epoch 3862, Training Loss: 1.0717060376919108e-06, Validation Loss: 1.4123408008906911e-06\n",
      "Epoch 3863, Training Loss: 2.8652939363382757e-06, Validation Loss: 1.3712616667298732e-06\n",
      "Epoch 3864, Training Loss: 7.205426300060935e-07, Validation Loss: 1.322570148085494e-06\n",
      "Epoch 3865, Training Loss: 1.5338594039349118e-06, Validation Loss: 9.77447275639409e-07\n",
      "Epoch 3866, Training Loss: 1.244345071427233e-06, Validation Loss: 1.1227476216732427e-06\n",
      "Epoch 3867, Training Loss: 1.9107844764221227e-06, Validation Loss: 9.059592713899414e-07\n",
      "Epoch 3868, Training Loss: 8.93248568445415e-07, Validation Loss: 1.0548650678559686e-06\n",
      "Epoch 3869, Training Loss: 9.79950414148334e-07, Validation Loss: 9.385382343964103e-07\n",
      "Epoch 3870, Training Loss: 1.0398389349575154e-06, Validation Loss: 8.970246950443681e-07\n",
      "Epoch 3871, Training Loss: 7.236099008878227e-07, Validation Loss: 8.711710630221576e-07\n",
      "Epoch 3872, Training Loss: 5.004982313039363e-07, Validation Loss: 8.65954750114707e-07\n",
      "Epoch 3873, Training Loss: 6.387778057614923e-07, Validation Loss: 9.36854244673089e-07\n",
      "Epoch 3874, Training Loss: 8.230130106312572e-07, Validation Loss: 1.1580507066862378e-06\n",
      "Epoch 3875, Training Loss: 1.5620101976310252e-06, Validation Loss: 9.23870528218674e-07\n",
      "Epoch 3876, Training Loss: 7.246100039992598e-07, Validation Loss: 8.785828711575484e-07\n",
      "Epoch 3877, Training Loss: 4.052932581544155e-06, Validation Loss: 1.850234269252602e-06\n",
      "Epoch 3878, Training Loss: 5.125277198203548e-07, Validation Loss: 9.457974976053584e-07\n",
      "Epoch 3879, Training Loss: 3.1626520922145573e-06, Validation Loss: 2.0909527352536327e-06\n",
      "Epoch 3880, Training Loss: 6.485258836619323e-07, Validation Loss: 1.090178473570087e-06\n",
      "Epoch 3881, Training Loss: 2.87198304249614e-06, Validation Loss: 1.1225341619633206e-06\n",
      "Epoch 3882, Training Loss: 1.668681534283678e-06, Validation Loss: 2.008262115642265e-06\n",
      "Epoch 3883, Training Loss: 4.187289050605614e-06, Validation Loss: 1.8438739635324281e-06\n",
      "Epoch 3884, Training Loss: 1.5947107385727577e-06, Validation Loss: 1.410178579700204e-06\n",
      "Epoch 3885, Training Loss: 8.044593187150895e-07, Validation Loss: 9.558045550272782e-07\n",
      "Epoch 3886, Training Loss: 1.7224982684638235e-06, Validation Loss: 1.9058846055776004e-06\n",
      "Epoch 3887, Training Loss: 6.880902105876885e-07, Validation Loss: 1.0350543145067935e-06\n",
      "Epoch 3888, Training Loss: 6.931740017535049e-07, Validation Loss: 1.1004968566041474e-06\n",
      "Epoch 3889, Training Loss: 1.3605025515062152e-06, Validation Loss: 9.045850596533218e-07\n",
      "Epoch 3890, Training Loss: 1.0110519497175119e-06, Validation Loss: 9.173614096864625e-07\n",
      "Epoch 3891, Training Loss: 7.083011723807431e-07, Validation Loss: 1.1320721120109216e-06\n",
      "Epoch 3892, Training Loss: 1.2479573570089997e-06, Validation Loss: 1.5914469875085861e-06\n",
      "Epoch 3893, Training Loss: 6.219539727680967e-07, Validation Loss: 8.733067964466656e-07\n",
      "Epoch 3894, Training Loss: 7.469652700820006e-07, Validation Loss: 8.86670163940632e-07\n",
      "Epoch 3895, Training Loss: 7.705846201133681e-07, Validation Loss: 9.393475554255708e-07\n",
      "Epoch 3896, Training Loss: 8.599049579061102e-07, Validation Loss: 8.65158834718838e-07\n",
      "Epoch 3897, Training Loss: 4.21600452682469e-06, Validation Loss: 2.689646671396001e-06\n",
      "Epoch 3898, Training Loss: 7.492008080589585e-07, Validation Loss: 1.02228259160036e-06\n",
      "Epoch 3899, Training Loss: 1.5260754935297882e-06, Validation Loss: 1.8134720415560693e-06\n",
      "Epoch 3900, Training Loss: 1.428957602911396e-06, Validation Loss: 1.415558586628484e-06\n",
      "Epoch 3901, Training Loss: 7.991137636054191e-07, Validation Loss: 1.1172409902364904e-06\n",
      "Epoch 3902, Training Loss: 8.467975476378342e-07, Validation Loss: 8.79340995093435e-07\n",
      "Epoch 3903, Training Loss: 2.09659583561006e-06, Validation Loss: 9.866704825905978e-07\n",
      "Epoch 3904, Training Loss: 1.350350657958188e-06, Validation Loss: 1.6190327268461237e-06\n",
      "Epoch 3905, Training Loss: 4.672865088650724e-06, Validation Loss: 1.5736488116101771e-06\n",
      "Epoch 3906, Training Loss: 9.982550182030536e-07, Validation Loss: 1.0709795355327005e-06\n",
      "Epoch 3907, Training Loss: 1.235080958394974e-06, Validation Loss: 8.855179415042735e-07\n",
      "Epoch 3908, Training Loss: 9.713990039017517e-07, Validation Loss: 8.902927425259324e-07\n",
      "Epoch 3909, Training Loss: 9.458246381655044e-07, Validation Loss: 1.0923697055416128e-06\n",
      "Epoch 3910, Training Loss: 8.844025387588772e-07, Validation Loss: 9.36530955331578e-07\n",
      "Epoch 3911, Training Loss: 1.1518400242493954e-06, Validation Loss: 1.1903381391675794e-06\n",
      "Epoch 3912, Training Loss: 4.7377929490721726e-07, Validation Loss: 8.99573341416871e-07\n",
      "Epoch 3913, Training Loss: 1.0375629244663287e-06, Validation Loss: 9.768810998000057e-07\n",
      "Epoch 3914, Training Loss: 1.2371021966828266e-06, Validation Loss: 1.197663340561376e-06\n",
      "Epoch 3915, Training Loss: 1.0490678050700808e-06, Validation Loss: 1.0914586414908012e-06\n",
      "Epoch 3916, Training Loss: 5.569681889028288e-07, Validation Loss: 8.594386105151163e-07\n",
      "Epoch 3917, Training Loss: 1.1507127055665478e-06, Validation Loss: 1.5961737476949577e-06\n",
      "Epoch 3918, Training Loss: 7.407483053611941e-07, Validation Loss: 1.3154660860561256e-06\n",
      "Epoch 3919, Training Loss: 7.581369345643907e-07, Validation Loss: 1.0160080274794277e-06\n",
      "Epoch 3920, Training Loss: 1.5365435501735192e-06, Validation Loss: 1.1507617898587341e-06\n",
      "Epoch 3921, Training Loss: 2.112409219989786e-06, Validation Loss: 4.0233660601402945e-06\n",
      "Epoch 3922, Training Loss: 1.235065838045557e-06, Validation Loss: 1.4158230319886442e-06\n",
      "Epoch 3923, Training Loss: 9.36895958147943e-07, Validation Loss: 1.1218524855466618e-06\n",
      "Epoch 3924, Training Loss: 9.134218998951837e-07, Validation Loss: 9.153825664514003e-07\n",
      "Epoch 3925, Training Loss: 5.590818545897491e-07, Validation Loss: 9.029362990914144e-07\n",
      "Epoch 3926, Training Loss: 7.580341616630903e-07, Validation Loss: 8.849348154642499e-07\n",
      "Epoch 3927, Training Loss: 6.006018793414114e-07, Validation Loss: 9.030113800151996e-07\n",
      "Epoch 3928, Training Loss: 7.707589020355954e-07, Validation Loss: 8.65579923886619e-07\n",
      "Epoch 3929, Training Loss: 1.117462602451269e-06, Validation Loss: 8.881317202971111e-07\n",
      "Epoch 3930, Training Loss: 9.314430826634634e-07, Validation Loss: 8.926320492502006e-07\n",
      "Epoch 3931, Training Loss: 5.488426495503518e-07, Validation Loss: 9.266630382889897e-07\n",
      "Epoch 3932, Training Loss: 1.0264980119245593e-06, Validation Loss: 1.0344045980308442e-06\n",
      "Epoch 3933, Training Loss: 1.1260406154178781e-06, Validation Loss: 1.5724631237876595e-06\n",
      "Epoch 3934, Training Loss: 6.349467298605305e-07, Validation Loss: 9.017795409484968e-07\n",
      "Epoch 3935, Training Loss: 1.0694991487980587e-06, Validation Loss: 8.820980336031715e-07\n",
      "Epoch 3936, Training Loss: 7.152301577662001e-07, Validation Loss: 1.00494801541966e-06\n",
      "Epoch 3937, Training Loss: 7.353689852607204e-07, Validation Loss: 9.692954261113792e-07\n",
      "Epoch 3938, Training Loss: 6.755773824806965e-07, Validation Loss: 9.682653330276531e-07\n",
      "Epoch 3939, Training Loss: 1.24829864489584e-06, Validation Loss: 1.393496728532963e-06\n",
      "Epoch 3940, Training Loss: 7.362756946349691e-07, Validation Loss: 9.128617557942295e-07\n",
      "Epoch 3941, Training Loss: 1.1560508710317663e-06, Validation Loss: 9.559934395531327e-07\n",
      "Epoch 3942, Training Loss: 2.6700358830566984e-06, Validation Loss: 1.1117699730119543e-05\n",
      "Epoch 3943, Training Loss: 1.9972626432718243e-06, Validation Loss: 1.3861792277931522e-06\n",
      "Epoch 3944, Training Loss: 3.5051270970143378e-06, Validation Loss: 3.234120251050673e-06\n",
      "Epoch 3945, Training Loss: 1.5357456959463889e-06, Validation Loss: 1.4595242607362134e-06\n",
      "Epoch 3946, Training Loss: 1.2822283679270186e-06, Validation Loss: 9.080784402077214e-07\n",
      "Epoch 3947, Training Loss: 7.25475274521159e-07, Validation Loss: 1.3101672460990789e-06\n",
      "Epoch 3948, Training Loss: 1.0738057198977913e-06, Validation Loss: 9.500195327656251e-07\n",
      "Epoch 3949, Training Loss: 1.035242348734755e-06, Validation Loss: 1.0356250939390046e-06\n",
      "Epoch 3950, Training Loss: 9.376668685945333e-07, Validation Loss: 8.80355521045713e-07\n",
      "Epoch 3951, Training Loss: 1.1508071793286945e-06, Validation Loss: 1.195144737202951e-06\n",
      "Epoch 3952, Training Loss: 1.964908733498305e-06, Validation Loss: 9.687417073080863e-07\n",
      "Epoch 3953, Training Loss: 7.070318019941624e-07, Validation Loss: 9.228742077290112e-07\n",
      "Epoch 3954, Training Loss: 2.1338807982829167e-06, Validation Loss: 1.649372549378707e-06\n",
      "Epoch 3955, Training Loss: 7.032051030364528e-07, Validation Loss: 8.638922842365107e-07\n",
      "Epoch 3956, Training Loss: 9.576242518960498e-07, Validation Loss: 1.0693378981390903e-06\n",
      "Epoch 3957, Training Loss: 3.652463192338473e-06, Validation Loss: 2.509627365493746e-06\n",
      "Epoch 3958, Training Loss: 5.295606797517394e-07, Validation Loss: 9.72266524626506e-07\n",
      "Epoch 3959, Training Loss: 8.556351531296968e-06, Validation Loss: 3.783583699526383e-06\n",
      "Epoch 3960, Training Loss: 1.0987737368850503e-06, Validation Loss: 1.3465796079834232e-06\n",
      "Epoch 3961, Training Loss: 5.809084768770845e-07, Validation Loss: 8.739697675768891e-07\n",
      "Epoch 3962, Training Loss: 1.4875486158416606e-06, Validation Loss: 1.0172450992181758e-06\n",
      "Epoch 3963, Training Loss: 7.97735083324369e-07, Validation Loss: 8.931821713495257e-07\n",
      "Epoch 3964, Training Loss: 9.822970241657458e-07, Validation Loss: 9.018717395256491e-07\n",
      "Epoch 3965, Training Loss: 3.4432075608492596e-06, Validation Loss: 1.9424537691277223e-06\n",
      "Epoch 3966, Training Loss: 1.4804390957579017e-06, Validation Loss: 9.364852664380037e-07\n",
      "Epoch 3967, Training Loss: 9.57528641265526e-07, Validation Loss: 9.762516329773909e-07\n",
      "Epoch 3968, Training Loss: 6.673737971141236e-07, Validation Loss: 8.70792963228645e-07\n",
      "Epoch 3969, Training Loss: 1.993098976527108e-06, Validation Loss: 1.1074388258230044e-06\n",
      "Epoch 3970, Training Loss: 7.594341013827943e-07, Validation Loss: 8.763202728167329e-07\n",
      "Epoch 3971, Training Loss: 5.939073162153363e-07, Validation Loss: 9.539962315490585e-07\n",
      "Epoch 3972, Training Loss: 6.937631269465783e-07, Validation Loss: 9.97227508779436e-07\n",
      "Epoch 3973, Training Loss: 6.961881808820181e-07, Validation Loss: 1.2380963985784419e-06\n",
      "Epoch 3974, Training Loss: 1.0689090004234458e-06, Validation Loss: 1.0480774377518486e-06\n",
      "Epoch 3975, Training Loss: 1.917202553158859e-06, Validation Loss: 1.3194062792356406e-06\n",
      "Epoch 3976, Training Loss: 9.779594165593153e-07, Validation Loss: 9.401638334374919e-07\n",
      "Epoch 3977, Training Loss: 6.689478482257982e-07, Validation Loss: 1.1888324594649345e-06\n",
      "Epoch 3978, Training Loss: 5.895989261262002e-07, Validation Loss: 9.180109333654655e-07\n",
      "Epoch 3979, Training Loss: 1.2451648672140436e-06, Validation Loss: 1.0195651108602808e-06\n",
      "Epoch 3980, Training Loss: 2.6999400688509922e-06, Validation Loss: 1.6069849300420522e-06\n",
      "Epoch 3981, Training Loss: 1.2086622973583871e-06, Validation Loss: 1.1068693150490078e-06\n",
      "Epoch 3982, Training Loss: 1.4752444030818879e-06, Validation Loss: 9.454285916012416e-07\n",
      "Epoch 3983, Training Loss: 8.887595868145581e-07, Validation Loss: 1.0897550802525122e-06\n",
      "Epoch 3984, Training Loss: 1.7724495364745962e-06, Validation Loss: 1.5194942175272027e-06\n",
      "Epoch 3985, Training Loss: 9.325037240159872e-07, Validation Loss: 1.0295712491929088e-06\n",
      "Epoch 3986, Training Loss: 1.5010982679086737e-06, Validation Loss: 1.4129623279186952e-06\n",
      "Epoch 3987, Training Loss: 8.158025934790203e-07, Validation Loss: 8.779154600856339e-07\n",
      "Epoch 3988, Training Loss: 8.89759746769414e-07, Validation Loss: 9.540928926966513e-07\n",
      "Epoch 3989, Training Loss: 1.9133462956233416e-06, Validation Loss: 1.2501246825212646e-06\n",
      "Epoch 3990, Training Loss: 3.9246369851753116e-06, Validation Loss: 2.6215788120159007e-06\n",
      "Epoch 3991, Training Loss: 1.3112382930557942e-06, Validation Loss: 1.4708005873489459e-06\n",
      "Epoch 3992, Training Loss: 1.679361957940273e-06, Validation Loss: 2.2122469849268696e-06\n",
      "Epoch 3993, Training Loss: 6.464844659603841e-07, Validation Loss: 8.966471091950153e-07\n",
      "Epoch 3994, Training Loss: 5.939663765275327e-07, Validation Loss: 8.741375083422575e-07\n",
      "Epoch 3995, Training Loss: 6.779448540328303e-07, Validation Loss: 8.706329973395609e-07\n",
      "Epoch 3996, Training Loss: 1.3459911087920773e-06, Validation Loss: 9.210547914567339e-07\n",
      "Epoch 3997, Training Loss: 8.255919397015532e-07, Validation Loss: 1.0113714267483293e-06\n",
      "Epoch 3998, Training Loss: 1.0463747912581312e-06, Validation Loss: 1.2334744214504504e-06\n",
      "Epoch 3999, Training Loss: 1.1270307140875957e-06, Validation Loss: 9.478854134045647e-07\n",
      "Epoch 4000, Training Loss: 8.816514309728518e-07, Validation Loss: 9.76929386111968e-07\n",
      "Epoch 4001, Training Loss: 2.1277633095451165e-06, Validation Loss: 1.2595630517694914e-06\n",
      "Epoch 4002, Training Loss: 2.990471330122091e-06, Validation Loss: 1.9233609170216e-06\n",
      "Epoch 4003, Training Loss: 1.7763294408723596e-06, Validation Loss: 1.3917383573857624e-06\n",
      "Epoch 4004, Training Loss: 1.2658375680985046e-06, Validation Loss: 8.675620834101847e-07\n",
      "Epoch 4005, Training Loss: 8.716017418919364e-07, Validation Loss: 8.722611867388605e-07\n",
      "Epoch 4006, Training Loss: 4.6610679760306084e-07, Validation Loss: 8.852002834628597e-07\n",
      "Epoch 4007, Training Loss: 7.735274039077922e-07, Validation Loss: 9.146278703184951e-07\n",
      "Epoch 4008, Training Loss: 2.380752903263783e-06, Validation Loss: 9.046051527157091e-07\n",
      "Epoch 4009, Training Loss: 9.55915538725094e-07, Validation Loss: 8.532685032365106e-07\n",
      "Epoch 4010, Training Loss: 1.2235068425070494e-06, Validation Loss: 1.077261768265309e-06\n",
      "Epoch 4011, Training Loss: 1.026547920446319e-06, Validation Loss: 8.700082425467215e-07\n",
      "Epoch 4012, Training Loss: 6.671103847111226e-07, Validation Loss: 8.603409829537419e-07\n",
      "Epoch 4013, Training Loss: 7.772373464831617e-07, Validation Loss: 9.287817853068781e-07\n",
      "Epoch 4014, Training Loss: 1.216473492604564e-06, Validation Loss: 8.722111969346324e-07\n",
      "Epoch 4015, Training Loss: 1.125314156524837e-06, Validation Loss: 1.2915652698622789e-06\n",
      "Epoch 4016, Training Loss: 8.533676236766041e-07, Validation Loss: 8.589370383074282e-07\n",
      "Epoch 4017, Training Loss: 2.7365199457562994e-06, Validation Loss: 1.0038228553384892e-06\n",
      "Epoch 4018, Training Loss: 1.9570279619074427e-06, Validation Loss: 1.342093746223524e-06\n",
      "Epoch 4019, Training Loss: 1.9621188585006166e-06, Validation Loss: 2.7008313519351303e-06\n",
      "Epoch 4020, Training Loss: 1.7800069826989784e-06, Validation Loss: 1.1855359850565588e-06\n",
      "Epoch 4021, Training Loss: 7.860459163566702e-07, Validation Loss: 8.947047873134811e-07\n",
      "Epoch 4022, Training Loss: 9.507675144959649e-07, Validation Loss: 9.254772165042746e-07\n",
      "Epoch 4023, Training Loss: 8.390384209633339e-07, Validation Loss: 9.211262204624024e-07\n",
      "Epoch 4024, Training Loss: 5.906328510718595e-07, Validation Loss: 9.673946165803732e-07\n",
      "Epoch 4025, Training Loss: 5.393708875089942e-07, Validation Loss: 9.301107705005341e-07\n",
      "Epoch 4026, Training Loss: 1.4188863133313134e-06, Validation Loss: 9.122009868939119e-07\n",
      "Epoch 4027, Training Loss: 1.5042513950902503e-05, Validation Loss: 9.722263425568032e-06\n",
      "Epoch 4028, Training Loss: 5.023091034672689e-06, Validation Loss: 5.130102072665294e-06\n",
      "Epoch 4029, Training Loss: 1.1550682756933384e-06, Validation Loss: 1.0610387635111624e-06\n",
      "Epoch 4030, Training Loss: 5.367857056626235e-07, Validation Loss: 9.655505547033598e-07\n",
      "Epoch 4031, Training Loss: 1.4633942555519752e-06, Validation Loss: 9.904269600127149e-07\n",
      "Epoch 4032, Training Loss: 2.7025475901609752e-06, Validation Loss: 1.7417081673244003e-06\n",
      "Epoch 4033, Training Loss: 1.1169040590175427e-06, Validation Loss: 9.501855119280987e-07\n",
      "Epoch 4034, Training Loss: 7.738452723060618e-07, Validation Loss: 1.031622042800029e-06\n",
      "Epoch 4035, Training Loss: 1.610811864338757e-06, Validation Loss: 2.8542940888736824e-06\n",
      "Epoch 4036, Training Loss: 9.887937721941853e-07, Validation Loss: 8.740754881896203e-07\n",
      "Epoch 4037, Training Loss: 6.5731937866075896e-06, Validation Loss: 2.1195721395777547e-06\n",
      "Epoch 4038, Training Loss: 1.3662704077432863e-05, Validation Loss: 1.728296058083207e-05\n",
      "Epoch 4039, Training Loss: 4.908076789433835e-07, Validation Loss: 8.53526758638005e-07\n",
      "Epoch 4040, Training Loss: 5.642329483634967e-07, Validation Loss: 8.893899167881958e-07\n",
      "Epoch 4041, Training Loss: 7.939470378914848e-07, Validation Loss: 1.2644725968570197e-06\n",
      "Epoch 4042, Training Loss: 1.771036636455392e-06, Validation Loss: 1.7983625818174962e-06\n",
      "Epoch 4043, Training Loss: 9.802565728023183e-07, Validation Loss: 1.5388670013855553e-06\n",
      "Epoch 4044, Training Loss: 1.9002025055669947e-06, Validation Loss: 2.026350205648964e-06\n",
      "Epoch 4045, Training Loss: 8.035444807319436e-07, Validation Loss: 8.975664664109906e-07\n",
      "Epoch 4046, Training Loss: 1.2399616480252007e-06, Validation Loss: 9.907421435551083e-07\n",
      "Epoch 4047, Training Loss: 7.772661092531052e-07, Validation Loss: 8.52725339930246e-07\n",
      "Epoch 4048, Training Loss: 8.344601951648656e-07, Validation Loss: 8.841229516877603e-07\n",
      "Epoch 4049, Training Loss: 9.368544624521746e-07, Validation Loss: 1.1101725244973687e-06\n",
      "Epoch 4050, Training Loss: 2.9163825274736155e-06, Validation Loss: 5.372631870501024e-06\n",
      "Epoch 4051, Training Loss: 1.4131592251942493e-06, Validation Loss: 9.107874036675013e-07\n",
      "Epoch 4052, Training Loss: 7.786772471263248e-07, Validation Loss: 8.79944959315311e-07\n",
      "Epoch 4053, Training Loss: 1.0857720553758554e-06, Validation Loss: 8.696138817337848e-07\n",
      "Epoch 4054, Training Loss: 9.897128165903268e-07, Validation Loss: 1.49447099870109e-06\n",
      "Epoch 4055, Training Loss: 5.028394980399753e-07, Validation Loss: 8.75423101550933e-07\n",
      "Epoch 4056, Training Loss: 3.5780935832008254e-06, Validation Loss: 1.670297730988808e-06\n",
      "Epoch 4057, Training Loss: 1.5043522125779418e-06, Validation Loss: 2.8365166101347585e-06\n",
      "Epoch 4058, Training Loss: 1.6736548786866479e-06, Validation Loss: 1.1275097386471757e-06\n",
      "Epoch 4059, Training Loss: 1.243037445419759e-06, Validation Loss: 1.3748942577222129e-06\n",
      "Epoch 4060, Training Loss: 4.970504505763529e-07, Validation Loss: 8.978063787670725e-07\n",
      "Epoch 4061, Training Loss: 6.481350283138454e-07, Validation Loss: 9.876459035292396e-07\n",
      "Epoch 4062, Training Loss: 9.392660444973444e-07, Validation Loss: 9.869948088719281e-07\n",
      "Epoch 4063, Training Loss: 1.2861312370660016e-06, Validation Loss: 1.1851989930115323e-06\n",
      "Epoch 4064, Training Loss: 9.77754893938254e-07, Validation Loss: 1.0534186874251153e-06\n",
      "Epoch 4065, Training Loss: 1.531911721031065e-06, Validation Loss: 1.5057441333929265e-06\n",
      "Epoch 4066, Training Loss: 8.378028155675565e-07, Validation Loss: 8.804831500896352e-07\n",
      "Epoch 4067, Training Loss: 9.934727813742938e-07, Validation Loss: 8.893785037520745e-07\n",
      "Epoch 4068, Training Loss: 9.900493296299828e-07, Validation Loss: 8.714937676367803e-07\n",
      "Epoch 4069, Training Loss: 5.002118541597156e-07, Validation Loss: 8.672162674200452e-07\n",
      "Epoch 4070, Training Loss: 8.186725608538836e-07, Validation Loss: 8.888822382577001e-07\n",
      "Epoch 4071, Training Loss: 7.755884325888474e-07, Validation Loss: 9.296863172523306e-07\n",
      "Epoch 4072, Training Loss: 9.757736734172795e-07, Validation Loss: 9.590792556907802e-07\n",
      "Epoch 4073, Training Loss: 5.204823878557363e-07, Validation Loss: 9.31775328464045e-07\n",
      "Epoch 4074, Training Loss: 8.730137324164389e-07, Validation Loss: 8.711487420236049e-07\n",
      "Epoch 4075, Training Loss: 1.4764941624889616e-06, Validation Loss: 2.4980737429255306e-06\n",
      "Epoch 4076, Training Loss: 1.7234804090549005e-06, Validation Loss: 1.5042956283940359e-06\n",
      "Epoch 4077, Training Loss: 7.35436515242327e-07, Validation Loss: 1.083962966140751e-06\n",
      "Epoch 4078, Training Loss: 6.354623565130169e-07, Validation Loss: 9.106008915397493e-07\n",
      "Epoch 4079, Training Loss: 9.178897926176433e-07, Validation Loss: 9.134814868236396e-07\n",
      "Epoch 4080, Training Loss: 1.5714774690422928e-06, Validation Loss: 1.674819342914451e-06\n",
      "Epoch 4081, Training Loss: 1.0442812481414876e-06, Validation Loss: 9.396656566284159e-07\n",
      "Epoch 4082, Training Loss: 9.254579254047712e-07, Validation Loss: 8.829222582888433e-07\n",
      "Epoch 4083, Training Loss: 1.2507638302849955e-06, Validation Loss: 1.0025780018443172e-06\n",
      "Epoch 4084, Training Loss: 1.2999524869883317e-06, Validation Loss: 9.551015105539677e-07\n",
      "Epoch 4085, Training Loss: 8.163178790709935e-07, Validation Loss: 8.63076584977488e-07\n",
      "Epoch 4086, Training Loss: 9.553843938192585e-07, Validation Loss: 8.962162369852003e-07\n",
      "Epoch 4087, Training Loss: 1.2219517202538555e-06, Validation Loss: 1.2900115314023772e-06\n",
      "Epoch 4088, Training Loss: 7.19160084372561e-07, Validation Loss: 1.0874181439350164e-06\n",
      "Epoch 4089, Training Loss: 1.1335894214425934e-06, Validation Loss: 1.262201870631427e-06\n",
      "Epoch 4090, Training Loss: 1.0377229955338407e-06, Validation Loss: 9.310682086434563e-07\n",
      "Epoch 4091, Training Loss: 6.027361223459593e-07, Validation Loss: 8.813073812923869e-07\n",
      "Epoch 4092, Training Loss: 9.100673992179509e-07, Validation Loss: 8.864037971280902e-07\n",
      "Epoch 4093, Training Loss: 1.0014784947998123e-06, Validation Loss: 8.901163139500082e-07\n",
      "Epoch 4094, Training Loss: 5.073997044746648e-07, Validation Loss: 9.761298017811624e-07\n",
      "Epoch 4095, Training Loss: 9.463415153732058e-07, Validation Loss: 8.823684203678127e-07\n",
      "Epoch 4096, Training Loss: 1.059595774677291e-06, Validation Loss: 1.052693985066129e-06\n",
      "Epoch 4097, Training Loss: 2.68122494162526e-06, Validation Loss: 3.419930024415842e-06\n",
      "Epoch 4098, Training Loss: 1.0470083680047537e-06, Validation Loss: 1.0096694408330685e-06\n",
      "Epoch 4099, Training Loss: 1.7710524389258353e-06, Validation Loss: 1.0192422261208117e-06\n",
      "Epoch 4100, Training Loss: 8.887195690476801e-07, Validation Loss: 1.0084068110984028e-06\n",
      "Epoch 4101, Training Loss: 1.4378068726728088e-06, Validation Loss: 9.37509057324466e-07\n",
      "Epoch 4102, Training Loss: 1.6163252212209045e-06, Validation Loss: 1.4031715642312908e-06\n",
      "Epoch 4103, Training Loss: 2.597696948214434e-06, Validation Loss: 1.0178157539207194e-06\n",
      "Epoch 4104, Training Loss: 4.086101398570463e-06, Validation Loss: 4.870849237889908e-06\n",
      "Epoch 4105, Training Loss: 9.496716302237473e-07, Validation Loss: 9.214820349202716e-07\n",
      "Epoch 4106, Training Loss: 1.0400663086329587e-06, Validation Loss: 1.0581072493771025e-06\n",
      "Epoch 4107, Training Loss: 6.59147190162912e-07, Validation Loss: 1.0154136089692605e-06\n",
      "Epoch 4108, Training Loss: 1.8436085156281479e-06, Validation Loss: 9.456205402408147e-07\n",
      "Epoch 4109, Training Loss: 9.736326092024683e-07, Validation Loss: 1.0185280428718245e-06\n",
      "Epoch 4110, Training Loss: 9.044524631462991e-07, Validation Loss: 9.326100074509772e-07\n",
      "Epoch 4111, Training Loss: 5.510260052687954e-07, Validation Loss: 8.783647215035103e-07\n",
      "Epoch 4112, Training Loss: 4.920462970403605e-07, Validation Loss: 1.0985071372679331e-06\n",
      "Epoch 4113, Training Loss: 7.079726742631465e-07, Validation Loss: 9.929247655589765e-07\n",
      "Epoch 4114, Training Loss: 5.496939934346301e-07, Validation Loss: 8.65108084226565e-07\n",
      "Epoch 4115, Training Loss: 1.6263940779026598e-05, Validation Loss: 2.4698986622451037e-06\n",
      "Epoch 4116, Training Loss: 1.2098587376385694e-06, Validation Loss: 8.968499567455201e-07\n",
      "Epoch 4117, Training Loss: 4.688789374540647e-07, Validation Loss: 8.924672618081931e-07\n",
      "Epoch 4118, Training Loss: 3.932277195417555e-06, Validation Loss: 2.6992499216787432e-06\n",
      "Epoch 4119, Training Loss: 1.4056962527320138e-06, Validation Loss: 1.1781733296426927e-06\n",
      "Epoch 4120, Training Loss: 1.394484911543259e-06, Validation Loss: 1.4218006848298668e-06\n",
      "Epoch 4121, Training Loss: 5.942406460235361e-07, Validation Loss: 8.487404064543376e-07\n",
      "Epoch 4122, Training Loss: 1.4891314776832587e-06, Validation Loss: 9.777090774185323e-07\n",
      "Epoch 4123, Training Loss: 6.1069272305758204e-06, Validation Loss: 4.569991906870073e-06\n",
      "Epoch 4124, Training Loss: 1.2439243164408254e-06, Validation Loss: 8.941277502173854e-07\n",
      "Epoch 4125, Training Loss: 1.210239133797586e-06, Validation Loss: 1.109502724010532e-06\n",
      "Epoch 4126, Training Loss: 9.896202755044214e-07, Validation Loss: 8.946648933711206e-07\n",
      "Epoch 4127, Training Loss: 8.094288546089956e-07, Validation Loss: 1.0171177475122079e-06\n",
      "Epoch 4128, Training Loss: 4.1141083784168586e-07, Validation Loss: 8.78319532614786e-07\n",
      "Epoch 4129, Training Loss: 1.5854746379773133e-05, Validation Loss: 4.892007761734432e-06\n",
      "Epoch 4130, Training Loss: 1.0757528343674494e-06, Validation Loss: 8.801657386110764e-07\n",
      "Epoch 4131, Training Loss: 6.36820175259345e-07, Validation Loss: 8.872103650530659e-07\n",
      "Epoch 4132, Training Loss: 1.052014567903825e-06, Validation Loss: 8.868805589937455e-07\n",
      "Epoch 4133, Training Loss: 6.747101792825561e-07, Validation Loss: 9.78762139226138e-07\n",
      "Epoch 4134, Training Loss: 1.1399760069252807e-06, Validation Loss: 9.825150423920065e-07\n",
      "Epoch 4135, Training Loss: 2.433454938000068e-06, Validation Loss: 1.954405056668368e-06\n",
      "Epoch 4136, Training Loss: 7.684061529289465e-07, Validation Loss: 8.796927061443764e-07\n",
      "Epoch 4137, Training Loss: 2.951451733679278e-06, Validation Loss: 2.0455062241896607e-06\n",
      "Epoch 4138, Training Loss: 1.4824290701653808e-06, Validation Loss: 8.726335509589981e-07\n",
      "Epoch 4139, Training Loss: 8.722901725377596e-07, Validation Loss: 8.506914137930377e-07\n",
      "Epoch 4140, Training Loss: 1.0476727538843988e-06, Validation Loss: 9.293626394204473e-07\n",
      "Epoch 4141, Training Loss: 8.360146921404521e-07, Validation Loss: 9.50754485586096e-07\n",
      "Epoch 4142, Training Loss: 1.2388375125738094e-06, Validation Loss: 1.0011700716416033e-06\n",
      "Epoch 4143, Training Loss: 7.833525046407885e-07, Validation Loss: 8.623742285991939e-07\n",
      "Epoch 4144, Training Loss: 1.069714699042379e-06, Validation Loss: 1.0107179071942164e-06\n",
      "Epoch 4145, Training Loss: 2.4224905246228445e-06, Validation Loss: 1.0398184220865664e-06\n",
      "Epoch 4146, Training Loss: 7.122154102034983e-07, Validation Loss: 1.0882391441143928e-06\n",
      "Epoch 4147, Training Loss: 8.995500593300676e-07, Validation Loss: 9.848835831072989e-07\n",
      "Epoch 4148, Training Loss: 2.1555410967266653e-06, Validation Loss: 1.4988756380945773e-06\n",
      "Epoch 4149, Training Loss: 7.877893040131312e-07, Validation Loss: 1.4906087668136325e-06\n",
      "Epoch 4150, Training Loss: 1.4585742746930919e-06, Validation Loss: 9.774236897842716e-07\n",
      "Epoch 4151, Training Loss: 8.137678378261626e-06, Validation Loss: 5.456253434696374e-06\n",
      "Epoch 4152, Training Loss: 1.277067440241808e-06, Validation Loss: 9.955334376766464e-07\n",
      "Epoch 4153, Training Loss: 7.751672228550888e-07, Validation Loss: 9.188418335322535e-07\n",
      "Epoch 4154, Training Loss: 1.0551625564403366e-06, Validation Loss: 9.127304301177814e-07\n",
      "Epoch 4155, Training Loss: 1.341060851700604e-06, Validation Loss: 1.115580451138899e-06\n",
      "Epoch 4156, Training Loss: 2.1620023744617356e-06, Validation Loss: 1.1634420285323811e-06\n",
      "Epoch 4157, Training Loss: 1.1387824088160414e-06, Validation Loss: 9.45180976237784e-07\n",
      "Epoch 4158, Training Loss: 1.7119876929427846e-06, Validation Loss: 1.1430515831011835e-06\n",
      "Epoch 4159, Training Loss: 7.903287837507378e-07, Validation Loss: 1.0371930689598809e-06\n",
      "Epoch 4160, Training Loss: 2.994127953570569e-06, Validation Loss: 2.337483427799021e-06\n",
      "Epoch 4161, Training Loss: 1.03012007457437e-06, Validation Loss: 1.197862477902105e-06\n",
      "Epoch 4162, Training Loss: 1.3210956240072846e-06, Validation Loss: 9.232006411994992e-07\n",
      "Epoch 4163, Training Loss: 8.387254410990863e-07, Validation Loss: 8.578945071957414e-07\n",
      "Epoch 4164, Training Loss: 1.3999427892485983e-06, Validation Loss: 1.0784227643204416e-06\n",
      "Epoch 4165, Training Loss: 5.837470098413178e-07, Validation Loss: 8.495931196495085e-07\n",
      "Epoch 4166, Training Loss: 8.329260481332312e-07, Validation Loss: 1.0227115555729831e-06\n",
      "Epoch 4167, Training Loss: 5.417165311882854e-07, Validation Loss: 8.871282259507521e-07\n",
      "Epoch 4168, Training Loss: 7.175345899668173e-07, Validation Loss: 8.908690627432424e-07\n",
      "Epoch 4169, Training Loss: 1.070467760655447e-06, Validation Loss: 9.572829026400407e-07\n",
      "Epoch 4170, Training Loss: 9.37161416914023e-07, Validation Loss: 8.835768017486157e-07\n",
      "Epoch 4171, Training Loss: 1.0266234085065662e-06, Validation Loss: 9.553176138449268e-07\n",
      "Epoch 4172, Training Loss: 1.942936705745524e-06, Validation Loss: 1.0713199285882687e-06\n",
      "Epoch 4173, Training Loss: 2.3483353288611397e-06, Validation Loss: 1.6782640834242753e-06\n",
      "Epoch 4174, Training Loss: 8.790864285401767e-07, Validation Loss: 9.372440364699472e-07\n",
      "Epoch 4175, Training Loss: 1.3307752624314162e-06, Validation Loss: 9.263806581060589e-07\n",
      "Epoch 4176, Training Loss: 9.612192570784828e-07, Validation Loss: 1.0490957016114917e-06\n",
      "Epoch 4177, Training Loss: 1.013356381918129e-06, Validation Loss: 8.649985295795406e-07\n",
      "Epoch 4178, Training Loss: 9.451206892663322e-07, Validation Loss: 9.453129490964692e-07\n",
      "Epoch 4179, Training Loss: 1.11144186121237e-06, Validation Loss: 8.819994356156475e-07\n",
      "Epoch 4180, Training Loss: 1.7629481590120122e-06, Validation Loss: 1.163594820564769e-06\n",
      "Epoch 4181, Training Loss: 1.620428406567953e-06, Validation Loss: 1.1373275391645382e-06\n",
      "Epoch 4182, Training Loss: 1.267834363716247e-06, Validation Loss: 1.2478727989838742e-06\n",
      "Epoch 4183, Training Loss: 1.3500746263162e-06, Validation Loss: 1.4370302090929339e-06\n",
      "Epoch 4184, Training Loss: 7.596977411594708e-07, Validation Loss: 8.579265366519695e-07\n",
      "Epoch 4185, Training Loss: 1.9237327251175884e-06, Validation Loss: 1.174751320707453e-06\n",
      "Epoch 4186, Training Loss: 9.441300790058449e-07, Validation Loss: 1.082553155760495e-06\n",
      "Epoch 4187, Training Loss: 5.663965225721768e-07, Validation Loss: 2.372031673846444e-06\n",
      "Epoch 4188, Training Loss: 7.253833018694422e-07, Validation Loss: 8.794860513520179e-07\n",
      "Epoch 4189, Training Loss: 9.819144679568126e-07, Validation Loss: 1.1059397689217467e-06\n",
      "Epoch 4190, Training Loss: 1.7047168512362987e-06, Validation Loss: 1.8846262147252772e-06\n",
      "Epoch 4191, Training Loss: 2.0508382476691622e-06, Validation Loss: 1.4993663108482437e-06\n",
      "Epoch 4192, Training Loss: 1.0506539638299728e-06, Validation Loss: 8.803694598123436e-07\n",
      "Epoch 4193, Training Loss: 6.121407523096423e-07, Validation Loss: 9.380601253055221e-07\n",
      "Epoch 4194, Training Loss: 8.453034752164967e-07, Validation Loss: 9.93021747491723e-07\n",
      "Epoch 4195, Training Loss: 6.206611260495265e-07, Validation Loss: 1.1423427005915201e-06\n",
      "Epoch 4196, Training Loss: 5.309177026902034e-07, Validation Loss: 9.035162422620418e-07\n",
      "Epoch 4197, Training Loss: 6.226008508747327e-07, Validation Loss: 8.60694901303429e-07\n",
      "Epoch 4198, Training Loss: 7.514587423429475e-07, Validation Loss: 1.0800717824880339e-06\n",
      "Epoch 4199, Training Loss: 1.7563000938025652e-06, Validation Loss: 1.1721065376312567e-06\n",
      "Epoch 4200, Training Loss: 8.927606245379138e-07, Validation Loss: 1.5030723475204113e-06\n",
      "Epoch 4201, Training Loss: 7.649537110410165e-06, Validation Loss: 7.058616496590492e-06\n",
      "Epoch 4202, Training Loss: 1.1055577715524123e-06, Validation Loss: 9.559040398807544e-07\n",
      "Epoch 4203, Training Loss: 1.8236148662253981e-06, Validation Loss: 1.4275794338590557e-06\n",
      "Epoch 4204, Training Loss: 5.779340881417738e-07, Validation Loss: 8.858602916751326e-07\n",
      "Epoch 4205, Training Loss: 7.75575927036698e-07, Validation Loss: 9.896844915509145e-07\n",
      "Epoch 4206, Training Loss: 7.517529638789711e-07, Validation Loss: 8.660653516582006e-07\n",
      "Epoch 4207, Training Loss: 9.547243280394468e-07, Validation Loss: 9.352402060265457e-07\n",
      "Epoch 4208, Training Loss: 1.4477558352155029e-06, Validation Loss: 1.0416614631354304e-06\n",
      "Epoch 4209, Training Loss: 1.2371500588415074e-06, Validation Loss: 9.55169416715907e-07\n",
      "Epoch 4210, Training Loss: 7.874045877542812e-07, Validation Loss: 8.843788261827424e-07\n",
      "Epoch 4211, Training Loss: 1.0572620112725417e-06, Validation Loss: 9.181418975250108e-07\n",
      "Epoch 4212, Training Loss: 1.0313633538316935e-06, Validation Loss: 1.978318681820638e-06\n",
      "Epoch 4213, Training Loss: 7.921520932541171e-07, Validation Loss: 9.240781634694928e-07\n",
      "Epoch 4214, Training Loss: 4.362801462320931e-07, Validation Loss: 8.639180822782037e-07\n",
      "Epoch 4215, Training Loss: 1.2133455129514914e-06, Validation Loss: 1.0041668282622357e-06\n",
      "Epoch 4216, Training Loss: 5.585507096839137e-07, Validation Loss: 9.126200117766443e-07\n",
      "Epoch 4217, Training Loss: 2.28283170145005e-06, Validation Loss: 1.6147532993840472e-06\n",
      "Epoch 4218, Training Loss: 9.043041018230724e-07, Validation Loss: 9.140600630367644e-07\n",
      "Epoch 4219, Training Loss: 8.662681239002268e-07, Validation Loss: 9.221142523126514e-07\n",
      "Epoch 4220, Training Loss: 7.80674952238769e-07, Validation Loss: 9.154638099983221e-07\n",
      "Epoch 4221, Training Loss: 1.0928180927294306e-05, Validation Loss: 9.92697101873479e-06\n",
      "Epoch 4222, Training Loss: 1.2525727015599841e-06, Validation Loss: 1.745828159496312e-06\n",
      "Epoch 4223, Training Loss: 7.9636333794042e-07, Validation Loss: 9.485121334590447e-07\n",
      "Epoch 4224, Training Loss: 7.066489615681348e-07, Validation Loss: 1.1192957364525515e-06\n",
      "Epoch 4225, Training Loss: 9.902468036671053e-07, Validation Loss: 8.432219562932422e-07\n",
      "Epoch 4226, Training Loss: 8.923569794205832e-07, Validation Loss: 8.624018385701205e-07\n",
      "Epoch 4227, Training Loss: 8.759309366723755e-07, Validation Loss: 8.632288156358572e-07\n",
      "Epoch 4228, Training Loss: 7.235759653667628e-07, Validation Loss: 1.0464475823334924e-06\n",
      "Epoch 4229, Training Loss: 6.874026325931482e-07, Validation Loss: 8.54234310396165e-07\n",
      "Epoch 4230, Training Loss: 1.2357392051853822e-06, Validation Loss: 9.804835424188095e-07\n",
      "Epoch 4231, Training Loss: 3.3652868296485394e-06, Validation Loss: 9.690970116234256e-07\n",
      "Epoch 4232, Training Loss: 6.495770321635064e-07, Validation Loss: 9.044317071520651e-07\n",
      "Epoch 4233, Training Loss: 2.2420047116611386e-06, Validation Loss: 1.3520092699914681e-06\n",
      "Epoch 4234, Training Loss: 1.1371737400622806e-06, Validation Loss: 1.172165292726879e-06\n",
      "Epoch 4235, Training Loss: 2.8697029392787954e-06, Validation Loss: 2.8436414670798103e-06\n",
      "Epoch 4236, Training Loss: 9.930403166436008e-07, Validation Loss: 1.101934186078272e-06\n",
      "Epoch 4237, Training Loss: 4.863096364715602e-06, Validation Loss: 3.1740044354951265e-06\n",
      "Epoch 4238, Training Loss: 9.136232392847887e-07, Validation Loss: 1.1872638041333317e-06\n",
      "Epoch 4239, Training Loss: 1.4848905038888915e-06, Validation Loss: 1.1159596932316887e-06\n",
      "Epoch 4240, Training Loss: 7.671155799471308e-07, Validation Loss: 8.768888972678672e-07\n",
      "Epoch 4241, Training Loss: 1.0386835356257507e-06, Validation Loss: 1.3973624876088699e-06\n",
      "Epoch 4242, Training Loss: 1.319312445957621e-06, Validation Loss: 9.041721839975062e-07\n",
      "Epoch 4243, Training Loss: 6.028308234817814e-07, Validation Loss: 8.653642782394906e-07\n",
      "Epoch 4244, Training Loss: 6.153172762424219e-07, Validation Loss: 8.581745662120588e-07\n",
      "Epoch 4245, Training Loss: 2.1771718365926063e-06, Validation Loss: 1.8710029145869002e-06\n",
      "Epoch 4246, Training Loss: 8.591532605350949e-07, Validation Loss: 8.689468372476131e-07\n",
      "Epoch 4247, Training Loss: 1.037077254295582e-06, Validation Loss: 9.732421117506843e-07\n",
      "Epoch 4248, Training Loss: 1.0826330480995239e-06, Validation Loss: 1.0361566934111609e-06\n",
      "Epoch 4249, Training Loss: 2.3503780539613217e-06, Validation Loss: 2.5049971424494193e-06\n",
      "Epoch 4250, Training Loss: 8.278172458631161e-07, Validation Loss: 9.078531994276954e-07\n",
      "Epoch 4251, Training Loss: 8.202770800380677e-07, Validation Loss: 8.688549509472045e-07\n",
      "Epoch 4252, Training Loss: 5.137828225088015e-07, Validation Loss: 9.745891036360194e-07\n",
      "Epoch 4253, Training Loss: 1.8880230072682025e-06, Validation Loss: 3.1523189550547625e-06\n",
      "Epoch 4254, Training Loss: 2.1857454157725442e-06, Validation Loss: 1.975207330134792e-06\n",
      "Epoch 4255, Training Loss: 3.3278411137871444e-06, Validation Loss: 2.410673812609142e-06\n",
      "Epoch 4256, Training Loss: 5.286637474455347e-07, Validation Loss: 9.067184411361169e-07\n",
      "Epoch 4257, Training Loss: 2.5302435915364185e-06, Validation Loss: 2.6706476862394773e-06\n",
      "Epoch 4258, Training Loss: 7.292109671652725e-07, Validation Loss: 9.190830922084185e-07\n",
      "Epoch 4259, Training Loss: 4.781293228006689e-07, Validation Loss: 8.494714718366665e-07\n",
      "Epoch 4260, Training Loss: 8.288445201287686e-07, Validation Loss: 1.2709302911033118e-06\n",
      "Epoch 4261, Training Loss: 6.003166390655679e-07, Validation Loss: 8.530329216610318e-07\n",
      "Epoch 4262, Training Loss: 1.687456460786052e-06, Validation Loss: 1.7991743988882466e-06\n",
      "Epoch 4263, Training Loss: 2.105444991684635e-06, Validation Loss: 2.689051297771573e-06\n",
      "Epoch 4264, Training Loss: 1.4663422689409344e-06, Validation Loss: 1.020787990028343e-06\n",
      "Epoch 4265, Training Loss: 7.495171985283378e-07, Validation Loss: 8.96228976971195e-07\n",
      "Epoch 4266, Training Loss: 2.190221721320995e-06, Validation Loss: 1.0368911429682417e-06\n",
      "Epoch 4267, Training Loss: 6.058759254301549e-07, Validation Loss: 8.715700525910715e-07\n",
      "Epoch 4268, Training Loss: 1.1145584721816704e-06, Validation Loss: 1.1304620004480898e-06\n",
      "Epoch 4269, Training Loss: 1.8292229242433677e-06, Validation Loss: 1.2317620199743485e-06\n",
      "Epoch 4270, Training Loss: 1.165247340395581e-06, Validation Loss: 1.0158514499287857e-06\n",
      "Epoch 4271, Training Loss: 2.9499506126740016e-06, Validation Loss: 2.0535741887620917e-06\n",
      "Epoch 4272, Training Loss: 7.484184152417583e-07, Validation Loss: 9.978358110231149e-07\n",
      "Epoch 4273, Training Loss: 1.7090109167838818e-06, Validation Loss: 8.728527315788655e-07\n",
      "Epoch 4274, Training Loss: 1.4435759112529922e-06, Validation Loss: 9.733344589534634e-07\n",
      "Epoch 4275, Training Loss: 5.804817533316964e-07, Validation Loss: 8.475306249875427e-07\n",
      "Epoch 4276, Training Loss: 7.650177167306538e-07, Validation Loss: 9.256790816049891e-07\n",
      "Epoch 4277, Training Loss: 8.425546980106446e-07, Validation Loss: 8.426649588556849e-07\n",
      "Epoch 4278, Training Loss: 1.978123236767715e-06, Validation Loss: 9.457637498124047e-07\n",
      "Epoch 4279, Training Loss: 1.2580512702697888e-06, Validation Loss: 9.322644983066815e-07\n",
      "Epoch 4280, Training Loss: 1.205887429023278e-06, Validation Loss: 1.0104803315733276e-06\n",
      "Epoch 4281, Training Loss: 9.056130920725991e-07, Validation Loss: 8.94384947317521e-07\n",
      "Epoch 4282, Training Loss: 1.6061696896940703e-06, Validation Loss: 9.185574464507829e-07\n",
      "Epoch 4283, Training Loss: 7.362519909293042e-07, Validation Loss: 1.1972646896995851e-06\n",
      "Epoch 4284, Training Loss: 1.8675243609322933e-06, Validation Loss: 1.041741870866859e-06\n",
      "Epoch 4285, Training Loss: 7.870296485634753e-07, Validation Loss: 9.533221594784276e-07\n",
      "Epoch 4286, Training Loss: 8.227752346101624e-07, Validation Loss: 9.652008648523278e-07\n",
      "Epoch 4287, Training Loss: 1.935503632921609e-06, Validation Loss: 9.076064979036595e-07\n",
      "Epoch 4288, Training Loss: 2.3338559458352393e-06, Validation Loss: 2.2265722129455395e-06\n",
      "Epoch 4289, Training Loss: 7.917004722912679e-07, Validation Loss: 8.66499587767408e-07\n",
      "Epoch 4290, Training Loss: 9.306194783675892e-07, Validation Loss: 8.839371437666942e-07\n",
      "Epoch 4291, Training Loss: 1.1981145462414133e-06, Validation Loss: 1.5155040408795094e-06\n",
      "Epoch 4292, Training Loss: 1.2883523368145688e-06, Validation Loss: 9.33883415892657e-07\n",
      "Epoch 4293, Training Loss: 6.748422265445697e-07, Validation Loss: 1.0108690795512267e-06\n",
      "Epoch 4294, Training Loss: 8.799314628049615e-07, Validation Loss: 9.686868575725754e-07\n",
      "Epoch 4295, Training Loss: 1.1407776128180558e-06, Validation Loss: 9.789424633865541e-07\n",
      "Epoch 4296, Training Loss: 1.8411844848742476e-06, Validation Loss: 1.3992840326777204e-06\n",
      "Epoch 4297, Training Loss: 1.2466721273085568e-06, Validation Loss: 8.886261794741817e-07\n",
      "Epoch 4298, Training Loss: 1.2394373243296286e-06, Validation Loss: 9.29519981641794e-07\n",
      "Epoch 4299, Training Loss: 6.820363296355936e-07, Validation Loss: 8.96966172210231e-07\n",
      "Epoch 4300, Training Loss: 1.3292335552250734e-06, Validation Loss: 1.5308975935258091e-06\n",
      "Epoch 4301, Training Loss: 1.5794082628417527e-06, Validation Loss: 1.4286802041308752e-06\n",
      "Epoch 4302, Training Loss: 1.2427364026734722e-06, Validation Loss: 9.83280364080549e-07\n",
      "Epoch 4303, Training Loss: 9.46779607602366e-07, Validation Loss: 1.0781962201244418e-06\n",
      "Epoch 4304, Training Loss: 8.816141416900791e-07, Validation Loss: 8.775676295936948e-07\n",
      "Epoch 4305, Training Loss: 6.689824090244656e-07, Validation Loss: 8.435104841643348e-07\n",
      "Epoch 4306, Training Loss: 7.970621709318948e-07, Validation Loss: 9.772569303825137e-07\n",
      "Epoch 4307, Training Loss: 8.830436399875907e-07, Validation Loss: 1.1314954576122384e-06\n",
      "Epoch 4308, Training Loss: 1.2385596619424177e-06, Validation Loss: 1.0753027054538304e-06\n",
      "Epoch 4309, Training Loss: 1.882140054476622e-06, Validation Loss: 8.709900402669854e-07\n",
      "Epoch 4310, Training Loss: 7.501711252189125e-07, Validation Loss: 9.61301283580139e-07\n",
      "Epoch 4311, Training Loss: 1.6452288491564104e-06, Validation Loss: 9.58119484795325e-07\n",
      "Epoch 4312, Training Loss: 2.5230674509657547e-06, Validation Loss: 3.4949491098026335e-06\n",
      "Epoch 4313, Training Loss: 9.301038517151028e-07, Validation Loss: 1.1013041502783786e-06\n",
      "Epoch 4314, Training Loss: 2.499972197256284e-06, Validation Loss: 1.0356649156503964e-06\n",
      "Epoch 4315, Training Loss: 1.0468282880538027e-06, Validation Loss: 8.694273985708322e-07\n",
      "Epoch 4316, Training Loss: 1.2136828217990114e-06, Validation Loss: 8.808527908945073e-07\n",
      "Epoch 4317, Training Loss: 1.1876616099470994e-06, Validation Loss: 1.2658529071321572e-06\n",
      "Epoch 4318, Training Loss: 8.080280053945899e-07, Validation Loss: 1.196526111934467e-06\n",
      "Epoch 4319, Training Loss: 1.3089934327581432e-06, Validation Loss: 8.87671770506189e-07\n",
      "Epoch 4320, Training Loss: 1.1080421700171428e-06, Validation Loss: 1.349288134255762e-06\n",
      "Epoch 4321, Training Loss: 1.7530239801999414e-06, Validation Loss: 8.93174723051355e-07\n",
      "Epoch 4322, Training Loss: 8.247019422924495e-07, Validation Loss: 8.619852744281288e-07\n",
      "Epoch 4323, Training Loss: 6.596669663849752e-07, Validation Loss: 9.180880805656293e-07\n",
      "Epoch 4324, Training Loss: 6.352220225380734e-06, Validation Loss: 5.137359375500411e-06\n",
      "Epoch 4325, Training Loss: 7.929884304758161e-07, Validation Loss: 8.928354035036652e-07\n",
      "Epoch 4326, Training Loss: 7.96907079347875e-06, Validation Loss: 5.38082231924488e-06\n",
      "Epoch 4327, Training Loss: 7.745344987597491e-07, Validation Loss: 9.121952270625174e-07\n",
      "Epoch 4328, Training Loss: 6.213704182300717e-07, Validation Loss: 8.609683682934585e-07\n",
      "Epoch 4329, Training Loss: 8.289418360618583e-07, Validation Loss: 9.327430811530703e-07\n",
      "Epoch 4330, Training Loss: 1.1637746410997352e-06, Validation Loss: 9.772478712794354e-07\n",
      "Epoch 4331, Training Loss: 7.188602921814891e-07, Validation Loss: 1.1846984758466632e-06\n",
      "Epoch 4332, Training Loss: 9.163459253613837e-07, Validation Loss: 8.885530064255282e-07\n",
      "Epoch 4333, Training Loss: 1.239521679963218e-06, Validation Loss: 1.0070102021496021e-06\n",
      "Epoch 4334, Training Loss: 6.719941438859678e-07, Validation Loss: 8.376674522885844e-07\n",
      "Epoch 4335, Training Loss: 9.310670066042803e-07, Validation Loss: 8.744166509847332e-07\n",
      "Epoch 4336, Training Loss: 8.832721505314112e-07, Validation Loss: 9.63762840404047e-07\n",
      "Epoch 4337, Training Loss: 7.287660537258489e-07, Validation Loss: 8.836963802075692e-07\n",
      "Epoch 4338, Training Loss: 1.2336615782260196e-06, Validation Loss: 1.216473739891539e-06\n",
      "Epoch 4339, Training Loss: 6.102047791500809e-07, Validation Loss: 8.607716265226714e-07\n",
      "Epoch 4340, Training Loss: 1.4267311598814558e-06, Validation Loss: 8.750665776365081e-07\n",
      "Epoch 4341, Training Loss: 5.31843284079514e-07, Validation Loss: 8.750835843184872e-07\n",
      "Epoch 4342, Training Loss: 1.448521061320207e-06, Validation Loss: 1.482755397009201e-06\n",
      "Epoch 4343, Training Loss: 7.974012987688184e-07, Validation Loss: 1.2042332953100482e-06\n",
      "Epoch 4344, Training Loss: 1.7279141957260435e-06, Validation Loss: 1.0211660785979958e-06\n",
      "Epoch 4345, Training Loss: 1.0629698863340309e-06, Validation Loss: 9.0435960869747e-07\n",
      "Epoch 4346, Training Loss: 8.939920803641144e-07, Validation Loss: 8.778282713326013e-07\n",
      "Epoch 4347, Training Loss: 8.432593858742621e-07, Validation Loss: 1.3362306140441862e-06\n",
      "Epoch 4348, Training Loss: 7.114769005056587e-07, Validation Loss: 8.934284689956396e-07\n",
      "Epoch 4349, Training Loss: 1.800610334612429e-05, Validation Loss: 2.267743383625457e-05\n",
      "Epoch 4350, Training Loss: 2.2841400095785502e-06, Validation Loss: 1.6156362275717926e-06\n",
      "Epoch 4351, Training Loss: 1.1262993666605325e-06, Validation Loss: 1.0274939766902792e-06\n",
      "Epoch 4352, Training Loss: 6.833531074335042e-07, Validation Loss: 8.428415719011754e-07\n",
      "Epoch 4353, Training Loss: 9.055556802195497e-07, Validation Loss: 8.907101482763772e-07\n",
      "Epoch 4354, Training Loss: 8.69859320573596e-07, Validation Loss: 8.537313145685854e-07\n",
      "Epoch 4355, Training Loss: 5.011274311073066e-07, Validation Loss: 8.703733335246948e-07\n",
      "Epoch 4356, Training Loss: 3.1552779091725824e-06, Validation Loss: 3.111930731288658e-06\n",
      "Epoch 4357, Training Loss: 6.794226692363736e-07, Validation Loss: 1.0785890830951885e-06\n",
      "Epoch 4358, Training Loss: 1.176414116343949e-06, Validation Loss: 9.734348002598527e-07\n",
      "Epoch 4359, Training Loss: 7.929119192340295e-07, Validation Loss: 9.457585041061999e-07\n",
      "Epoch 4360, Training Loss: 3.8850893702147005e-07, Validation Loss: 9.296667077210636e-07\n",
      "Epoch 4361, Training Loss: 7.507401278417092e-07, Validation Loss: 9.945274896497167e-07\n",
      "Epoch 4362, Training Loss: 1.919219130286365e-06, Validation Loss: 1.9312232659138514e-06\n",
      "Epoch 4363, Training Loss: 1.3414592103799805e-06, Validation Loss: 9.799799638556718e-07\n",
      "Epoch 4364, Training Loss: 5.843813823958044e-07, Validation Loss: 9.646953536129486e-07\n",
      "Epoch 4365, Training Loss: 8.202134722523624e-07, Validation Loss: 8.554004974864435e-07\n",
      "Epoch 4366, Training Loss: 1.1691372492350638e-06, Validation Loss: 9.9530123267266e-07\n",
      "Epoch 4367, Training Loss: 5.262685363049968e-07, Validation Loss: 9.119782477674058e-07\n",
      "Epoch 4368, Training Loss: 1.2016068922093837e-06, Validation Loss: 1.0235229932048612e-06\n",
      "Epoch 4369, Training Loss: 7.421459713441436e-07, Validation Loss: 1.1157389505039825e-06\n",
      "Epoch 4370, Training Loss: 7.753414479338971e-07, Validation Loss: 8.751515482289951e-07\n",
      "Epoch 4371, Training Loss: 8.061198855102703e-07, Validation Loss: 8.371177819496152e-07\n",
      "Epoch 4372, Training Loss: 9.440241228730883e-07, Validation Loss: 9.598040561680247e-07\n",
      "Epoch 4373, Training Loss: 9.692170124253607e-07, Validation Loss: 1.0501269357272848e-06\n",
      "Epoch 4374, Training Loss: 8.401107152167242e-07, Validation Loss: 9.555776675984053e-07\n",
      "Epoch 4375, Training Loss: 9.245013075087627e-07, Validation Loss: 9.392155313353967e-07\n",
      "Epoch 4376, Training Loss: 7.466039164683025e-07, Validation Loss: 9.318188209206734e-07\n",
      "Epoch 4377, Training Loss: 2.4310465960297734e-06, Validation Loss: 1.269975391362999e-06\n",
      "Epoch 4378, Training Loss: 1.7063664472516393e-06, Validation Loss: 1.4794609595503443e-06\n",
      "Epoch 4379, Training Loss: 7.329604159167502e-07, Validation Loss: 8.784789472675311e-07\n",
      "Epoch 4380, Training Loss: 1.2586419870785903e-06, Validation Loss: 8.605336486531307e-07\n",
      "Epoch 4381, Training Loss: 6.030477948115731e-07, Validation Loss: 8.507380605163229e-07\n",
      "Epoch 4382, Training Loss: 4.4975237756261777e-07, Validation Loss: 8.771291457966524e-07\n",
      "Epoch 4383, Training Loss: 7.330268090299796e-07, Validation Loss: 9.154823112639513e-07\n",
      "Epoch 4384, Training Loss: 1.0402632142358925e-06, Validation Loss: 1.0809187157574805e-06\n",
      "Epoch 4385, Training Loss: 1.3863167396266363e-06, Validation Loss: 9.93908889514694e-07\n",
      "Epoch 4386, Training Loss: 9.130030207415984e-07, Validation Loss: 1.2267044680203262e-06\n",
      "Epoch 4387, Training Loss: 9.82294523055316e-07, Validation Loss: 8.517721244930194e-07\n",
      "Epoch 4388, Training Loss: 7.176732879088377e-07, Validation Loss: 1.2356377418550756e-06\n",
      "Epoch 4389, Training Loss: 1.3985203395350254e-06, Validation Loss: 9.4553046170595e-07\n",
      "Epoch 4390, Training Loss: 6.985002301007626e-07, Validation Loss: 8.659721782345178e-07\n",
      "Epoch 4391, Training Loss: 1.0977902320519206e-06, Validation Loss: 9.207161643487803e-07\n",
      "Epoch 4392, Training Loss: 9.432539513909433e-07, Validation Loss: 8.736185835042671e-07\n",
      "Epoch 4393, Training Loss: 1.5878674730629427e-06, Validation Loss: 2.156536016587246e-06\n",
      "Epoch 4394, Training Loss: 8.900568673197995e-07, Validation Loss: 1.4067290371886959e-06\n",
      "Epoch 4395, Training Loss: 8.186968329937372e-07, Validation Loss: 1.4467936090102068e-06\n",
      "Epoch 4396, Training Loss: 5.9529947975534014e-06, Validation Loss: 2.1649657529526667e-06\n",
      "Epoch 4397, Training Loss: 1.073485691449605e-06, Validation Loss: 9.410623046796958e-07\n",
      "Epoch 4398, Training Loss: 6.199551307872753e-07, Validation Loss: 8.93308891079776e-07\n",
      "Epoch 4399, Training Loss: 1.5078733213158557e-06, Validation Loss: 8.804018868104438e-07\n",
      "Epoch 4400, Training Loss: 6.396562071131484e-07, Validation Loss: 1.2075697183318433e-06\n",
      "Epoch 4401, Training Loss: 1.3137055248080287e-06, Validation Loss: 1.208860293848079e-06\n",
      "Epoch 4402, Training Loss: 7.529958452323626e-07, Validation Loss: 9.036014183235748e-07\n",
      "Epoch 4403, Training Loss: 1.5813915297258063e-06, Validation Loss: 8.843675643066681e-07\n",
      "Epoch 4404, Training Loss: 8.140152658597799e-07, Validation Loss: 1.2971316714298432e-06\n",
      "Epoch 4405, Training Loss: 5.839883669978008e-07, Validation Loss: 8.928702684327265e-07\n",
      "Epoch 4406, Training Loss: 4.031697699247161e-06, Validation Loss: 3.113322889614998e-06\n",
      "Epoch 4407, Training Loss: 1.0303500630470808e-06, Validation Loss: 1.0390653999380378e-06\n",
      "Epoch 4408, Training Loss: 7.634361622876895e-07, Validation Loss: 8.737998548136243e-07\n",
      "Epoch 4409, Training Loss: 3.2331868169421796e-06, Validation Loss: 4.194835753785891e-06\n",
      "Epoch 4410, Training Loss: 7.099819754330383e-07, Validation Loss: 9.093075603849087e-07\n",
      "Epoch 4411, Training Loss: 3.2825755624799058e-06, Validation Loss: 2.2987134688247253e-06\n",
      "Epoch 4412, Training Loss: 7.156567107813316e-07, Validation Loss: 8.75264229807147e-07\n",
      "Epoch 4413, Training Loss: 1.2335016208453453e-06, Validation Loss: 1.520279296105429e-06\n",
      "Epoch 4414, Training Loss: 5.903591500100447e-07, Validation Loss: 9.061532986159329e-07\n",
      "Epoch 4415, Training Loss: 1.1202112091268646e-06, Validation Loss: 8.860025213313521e-07\n",
      "Epoch 4416, Training Loss: 5.422504045782262e-07, Validation Loss: 9.207379424383739e-07\n",
      "Epoch 4417, Training Loss: 9.203054105455521e-07, Validation Loss: 9.72411166326397e-07\n",
      "Epoch 4418, Training Loss: 4.904372303826676e-07, Validation Loss: 8.488940697840981e-07\n",
      "Epoch 4419, Training Loss: 2.0043294171046e-06, Validation Loss: 1.8256581930692882e-06\n",
      "Epoch 4420, Training Loss: 8.731878438084095e-07, Validation Loss: 9.706564318907935e-07\n",
      "Epoch 4421, Training Loss: 1.345704163213668e-06, Validation Loss: 1.0288389209453253e-06\n",
      "Epoch 4422, Training Loss: 7.996208069016575e-07, Validation Loss: 9.937254974389293e-07\n",
      "Epoch 4423, Training Loss: 7.111236755008576e-07, Validation Loss: 8.560680705802048e-07\n",
      "Epoch 4424, Training Loss: 2.034770659520291e-06, Validation Loss: 1.0622155022968394e-06\n",
      "Epoch 4425, Training Loss: 1.1388394796085777e-06, Validation Loss: 8.83470956044157e-07\n",
      "Epoch 4426, Training Loss: 1.3505828064808156e-06, Validation Loss: 8.630944739996696e-07\n",
      "Epoch 4427, Training Loss: 6.134788463896257e-07, Validation Loss: 8.35181050998489e-07\n",
      "Epoch 4428, Training Loss: 7.938102726257057e-07, Validation Loss: 9.104511239755107e-07\n",
      "Epoch 4429, Training Loss: 7.138032742659561e-07, Validation Loss: 1.0707609466998912e-06\n",
      "Epoch 4430, Training Loss: 3.877860763168428e-06, Validation Loss: 1.8374599375228908e-06\n",
      "Epoch 4431, Training Loss: 1.2024659099552082e-06, Validation Loss: 1.330874967236338e-06\n",
      "Epoch 4432, Training Loss: 1.4752483821212081e-06, Validation Loss: 1.4322515523133823e-06\n",
      "Epoch 4433, Training Loss: 1.761903149599675e-06, Validation Loss: 1.1692155581047145e-06\n",
      "Epoch 4434, Training Loss: 8.178744792530779e-07, Validation Loss: 1.1058640348417346e-06\n",
      "Epoch 4435, Training Loss: 1.463786020394764e-06, Validation Loss: 1.239721421582075e-06\n",
      "Epoch 4436, Training Loss: 7.181613455031766e-07, Validation Loss: 9.674184625756153e-07\n",
      "Epoch 4437, Training Loss: 1.4095334108787938e-06, Validation Loss: 8.922266758394947e-07\n",
      "Epoch 4438, Training Loss: 2.271381845275755e-06, Validation Loss: 1.2779620626052592e-06\n",
      "Epoch 4439, Training Loss: 6.597708193112339e-07, Validation Loss: 9.190580410964906e-07\n",
      "Epoch 4440, Training Loss: 1.0134930334970704e-06, Validation Loss: 8.987116838916394e-07\n",
      "Epoch 4441, Training Loss: 1.341785718977917e-06, Validation Loss: 9.959591983234839e-07\n",
      "Epoch 4442, Training Loss: 6.544330517499475e-07, Validation Loss: 8.500701892661576e-07\n",
      "Epoch 4443, Training Loss: 1.1801229220509413e-06, Validation Loss: 1.0331210551679077e-06\n",
      "Epoch 4444, Training Loss: 4.714167516794987e-06, Validation Loss: 5.207756140888398e-06\n",
      "Epoch 4445, Training Loss: 7.189384518824227e-07, Validation Loss: 1.2149912501314295e-06\n",
      "Epoch 4446, Training Loss: 7.557754315712373e-07, Validation Loss: 9.156186044034998e-07\n",
      "Epoch 4447, Training Loss: 5.835057663716725e-07, Validation Loss: 8.807216674285651e-07\n",
      "Epoch 4448, Training Loss: 5.907947979721939e-06, Validation Loss: 2.7142622772893226e-06\n",
      "Epoch 4449, Training Loss: 6.232244231796358e-07, Validation Loss: 9.398687116392964e-07\n",
      "Epoch 4450, Training Loss: 6.870586162222025e-07, Validation Loss: 8.541638808571065e-07\n",
      "Epoch 4451, Training Loss: 1.052187712957675e-06, Validation Loss: 1.7213573971513276e-06\n",
      "Epoch 4452, Training Loss: 8.441959380434128e-07, Validation Loss: 8.641002272383233e-07\n",
      "Epoch 4453, Training Loss: 1.7755951375875156e-06, Validation Loss: 1.2819889955834157e-06\n",
      "Epoch 4454, Training Loss: 8.480694191348448e-07, Validation Loss: 8.459991213559154e-07\n",
      "Epoch 4455, Training Loss: 1.0649739579093875e-06, Validation Loss: 9.481773297046144e-07\n",
      "Epoch 4456, Training Loss: 5.111803034196782e-07, Validation Loss: 8.996932240062184e-07\n",
      "Epoch 4457, Training Loss: 1.6782798866188386e-06, Validation Loss: 8.590061115697538e-07\n",
      "Epoch 4458, Training Loss: 5.78241099447041e-07, Validation Loss: 1.2080357478341639e-06\n",
      "Epoch 4459, Training Loss: 1.127945779444417e-06, Validation Loss: 2.83278568038985e-06\n",
      "Epoch 4460, Training Loss: 1.1640739785434562e-06, Validation Loss: 8.346418920757442e-07\n",
      "Epoch 4461, Training Loss: 6.217871373337402e-07, Validation Loss: 9.013537354062203e-07\n",
      "Epoch 4462, Training Loss: 1.2132848496548831e-05, Validation Loss: 5.563220644602737e-06\n",
      "Epoch 4463, Training Loss: 2.032894144576858e-06, Validation Loss: 2.6304283801147474e-06\n",
      "Epoch 4464, Training Loss: 8.107605822260666e-07, Validation Loss: 9.454583216144557e-07\n",
      "Epoch 4465, Training Loss: 1.028691940518911e-06, Validation Loss: 8.974882840813074e-07\n",
      "Epoch 4466, Training Loss: 2.08773644772009e-06, Validation Loss: 1.2207282818841343e-06\n",
      "Epoch 4467, Training Loss: 7.443683784913446e-07, Validation Loss: 8.532182999979167e-07\n",
      "Epoch 4468, Training Loss: 1.6931617210502736e-06, Validation Loss: 1.1484191398724928e-06\n",
      "Epoch 4469, Training Loss: 1.01424302556552e-06, Validation Loss: 1.0045297968445316e-06\n",
      "Epoch 4470, Training Loss: 1.335446427219722e-06, Validation Loss: 1.401855900129918e-06\n",
      "Epoch 4471, Training Loss: 6.565345529452316e-07, Validation Loss: 9.264575723206174e-07\n",
      "Epoch 4472, Training Loss: 6.10665779277042e-07, Validation Loss: 8.500720926155395e-07\n",
      "Epoch 4473, Training Loss: 1.2355960734566906e-06, Validation Loss: 9.272936521176616e-07\n",
      "Epoch 4474, Training Loss: 5.03034812027181e-07, Validation Loss: 8.370542921954676e-07\n",
      "Epoch 4475, Training Loss: 1.2137862768213381e-06, Validation Loss: 9.185248663013058e-07\n",
      "Epoch 4476, Training Loss: 1.6946645473581157e-06, Validation Loss: 1.0769222747477928e-06\n",
      "Epoch 4477, Training Loss: 2.765167664620094e-06, Validation Loss: 2.465224262088703e-06\n",
      "Epoch 4478, Training Loss: 1.0878859484364511e-06, Validation Loss: 1.6240142661149694e-06\n",
      "Epoch 4479, Training Loss: 1.6820471273604198e-06, Validation Loss: 9.432276678268e-07\n",
      "Epoch 4480, Training Loss: 1.6298780565193738e-06, Validation Loss: 9.10408646373042e-07\n",
      "Epoch 4481, Training Loss: 7.349608495132998e-07, Validation Loss: 9.852367270106616e-07\n",
      "Epoch 4482, Training Loss: 8.911389386412338e-07, Validation Loss: 2.5533558461349087e-06\n",
      "Epoch 4483, Training Loss: 1.4368096117323148e-06, Validation Loss: 1.004792085594345e-06\n",
      "Epoch 4484, Training Loss: 7.660548249077692e-07, Validation Loss: 9.321866934245408e-07\n",
      "Epoch 4485, Training Loss: 5.937485525464581e-07, Validation Loss: 9.510141258670626e-07\n",
      "Epoch 4486, Training Loss: 4.893623781754286e-07, Validation Loss: 9.132506522167257e-07\n",
      "Epoch 4487, Training Loss: 5.173570798433502e-07, Validation Loss: 8.496751122985553e-07\n",
      "Epoch 4488, Training Loss: 7.346436632360565e-07, Validation Loss: 8.527836513213475e-07\n",
      "Epoch 4489, Training Loss: 1.1920143379029469e-06, Validation Loss: 1.0011294351141978e-06\n",
      "Epoch 4490, Training Loss: 7.330315838771639e-07, Validation Loss: 8.840022463170794e-07\n",
      "Epoch 4491, Training Loss: 8.683998089509259e-07, Validation Loss: 8.836042859036949e-07\n",
      "Epoch 4492, Training Loss: 5.055769634054741e-07, Validation Loss: 9.278629498193524e-07\n",
      "Epoch 4493, Training Loss: 1.3252381450001849e-06, Validation Loss: 1.2230390081356653e-06\n",
      "Epoch 4494, Training Loss: 7.753059207971091e-07, Validation Loss: 9.637291215758929e-07\n",
      "Epoch 4495, Training Loss: 9.048323477145459e-07, Validation Loss: 9.027743891211992e-07\n",
      "Epoch 4496, Training Loss: 1.7525420616948395e-06, Validation Loss: 1.1777499055500186e-06\n",
      "Epoch 4497, Training Loss: 8.292507800433668e-07, Validation Loss: 1.1044522117699787e-06\n",
      "Epoch 4498, Training Loss: 1.8439139921611059e-06, Validation Loss: 1.5191031420466403e-06\n",
      "Epoch 4499, Training Loss: 1.5731961866549682e-06, Validation Loss: 1.5116909365504073e-06\n",
      "Epoch 4500, Training Loss: 2.2586250452150125e-06, Validation Loss: 1.2755634118947902e-06\n",
      "Epoch 4501, Training Loss: 6.974241841817275e-07, Validation Loss: 8.865375616406494e-07\n",
      "Epoch 4502, Training Loss: 6.868706350360299e-07, Validation Loss: 8.518679286446093e-07\n",
      "Epoch 4503, Training Loss: 7.720963708379713e-07, Validation Loss: 1.2738556212526958e-06\n",
      "Epoch 4504, Training Loss: 9.084535008696548e-07, Validation Loss: 1.0584790013537922e-06\n",
      "Epoch 4505, Training Loss: 1.4965092987040407e-06, Validation Loss: 8.636780799502135e-07\n",
      "Epoch 4506, Training Loss: 2.0378670342324767e-06, Validation Loss: 1.4599290597387407e-06\n",
      "Epoch 4507, Training Loss: 1.578629735377035e-06, Validation Loss: 1.816764720127481e-06\n",
      "Epoch 4508, Training Loss: 6.062008992557821e-07, Validation Loss: 9.274844124763386e-07\n",
      "Epoch 4509, Training Loss: 7.207851808743726e-07, Validation Loss: 1.038213780345075e-06\n",
      "Epoch 4510, Training Loss: 1.8065579752146732e-06, Validation Loss: 1.394050792198399e-06\n",
      "Epoch 4511, Training Loss: 1.1099810762971174e-05, Validation Loss: 8.452223963706657e-06\n",
      "Epoch 4512, Training Loss: 1.1198792435607174e-06, Validation Loss: 9.094548626826567e-07\n",
      "Epoch 4513, Training Loss: 1.22176936656615e-06, Validation Loss: 9.86578516093901e-07\n",
      "Epoch 4514, Training Loss: 2.784719754345133e-06, Validation Loss: 1.7017927291000807e-06\n",
      "Epoch 4515, Training Loss: 6.891244765938609e-07, Validation Loss: 8.397871706229183e-07\n",
      "Epoch 4516, Training Loss: 4.555685677587462e-07, Validation Loss: 8.514637755571089e-07\n",
      "Epoch 4517, Training Loss: 1.0019383580583963e-06, Validation Loss: 8.906336413793104e-07\n",
      "Epoch 4518, Training Loss: 6.408506010302517e-07, Validation Loss: 9.419906697672605e-07\n",
      "Epoch 4519, Training Loss: 8.065419478953118e-07, Validation Loss: 8.712645175854247e-07\n",
      "Epoch 4520, Training Loss: 2.2317658476822544e-06, Validation Loss: 2.0744484035490738e-06\n",
      "Epoch 4521, Training Loss: 3.231206392229069e-06, Validation Loss: 1.5818511348356154e-06\n",
      "Epoch 4522, Training Loss: 1.8673904378374573e-06, Validation Loss: 1.9033162388834726e-06\n",
      "Epoch 4523, Training Loss: 3.0263213375292253e-06, Validation Loss: 1.8281500470733891e-06\n",
      "Epoch 4524, Training Loss: 5.298397240949271e-07, Validation Loss: 8.929727621857724e-07\n",
      "Epoch 4525, Training Loss: 1.5209484445222188e-06, Validation Loss: 2.1083744350165664e-06\n",
      "Epoch 4526, Training Loss: 1.2968524742973386e-06, Validation Loss: 2.009624236747487e-06\n",
      "Epoch 4527, Training Loss: 1.6245588767560548e-06, Validation Loss: 8.641440652812148e-07\n",
      "Epoch 4528, Training Loss: 1.2365757129373378e-06, Validation Loss: 9.199150262375158e-07\n",
      "Epoch 4529, Training Loss: 7.7170324175313e-07, Validation Loss: 1.0618855619132618e-06\n",
      "Epoch 4530, Training Loss: 9.81088987828116e-07, Validation Loss: 8.827670120327946e-07\n",
      "Epoch 4531, Training Loss: 8.155184332281351e-07, Validation Loss: 8.802641171902448e-07\n",
      "Epoch 4532, Training Loss: 9.365391520077537e-07, Validation Loss: 1.1065997259025306e-06\n",
      "Epoch 4533, Training Loss: 1.18149068839557e-06, Validation Loss: 1.35526751050255e-06\n",
      "Epoch 4534, Training Loss: 1.004417299554916e-06, Validation Loss: 9.645544885608418e-07\n",
      "Epoch 4535, Training Loss: 6.997931336627516e-07, Validation Loss: 8.505227262407884e-07\n",
      "Epoch 4536, Training Loss: 1.015901034406852e-06, Validation Loss: 8.893557920907897e-07\n",
      "Epoch 4537, Training Loss: 7.109774742275476e-07, Validation Loss: 8.902105498387396e-07\n",
      "Epoch 4538, Training Loss: 1.3692570064449683e-06, Validation Loss: 1.0330076968996295e-06\n",
      "Epoch 4539, Training Loss: 7.987614480953198e-07, Validation Loss: 8.740615095963904e-07\n",
      "Epoch 4540, Training Loss: 7.631123821738584e-07, Validation Loss: 8.64006638712859e-07\n",
      "Epoch 4541, Training Loss: 4.795044219463307e-07, Validation Loss: 9.391087593004466e-07\n",
      "Epoch 4542, Training Loss: 1.012676875689067e-06, Validation Loss: 8.81354828530316e-07\n",
      "Epoch 4543, Training Loss: 2.7348773983248975e-06, Validation Loss: 1.3420747855037639e-06\n",
      "Epoch 4544, Training Loss: 8.268081046480802e-07, Validation Loss: 1.156303572614002e-06\n",
      "Epoch 4545, Training Loss: 4.2927299546136055e-06, Validation Loss: 8.727455868031527e-07\n",
      "Epoch 4546, Training Loss: 1.564315198265831e-06, Validation Loss: 1.6185638070188814e-06\n",
      "Epoch 4547, Training Loss: 1.384680444971309e-06, Validation Loss: 1.0951012188218607e-06\n",
      "Epoch 4548, Training Loss: 1.079813955584541e-06, Validation Loss: 1.6330434152460973e-06\n",
      "Epoch 4549, Training Loss: 6.756631591997575e-07, Validation Loss: 9.145592818544996e-07\n",
      "Epoch 4550, Training Loss: 6.592360364265915e-07, Validation Loss: 8.857880741027998e-07\n",
      "Epoch 4551, Training Loss: 1.1168989431098453e-06, Validation Loss: 9.796040378703556e-07\n",
      "Epoch 4552, Training Loss: 1.8393957361695357e-06, Validation Loss: 1.64511910515201e-06\n",
      "Epoch 4553, Training Loss: 8.783827070146799e-07, Validation Loss: 8.45903882742034e-07\n",
      "Epoch 4554, Training Loss: 7.94537868387124e-07, Validation Loss: 9.078704521294396e-07\n",
      "Epoch 4555, Training Loss: 7.441751108672179e-07, Validation Loss: 8.536885940238616e-07\n",
      "Epoch 4556, Training Loss: 4.355839791969629e-06, Validation Loss: 2.123165386596863e-06\n",
      "Epoch 4557, Training Loss: 6.353550929816265e-07, Validation Loss: 8.898909562246045e-07\n",
      "Epoch 4558, Training Loss: 6.262868623707618e-07, Validation Loss: 8.55111168286072e-07\n",
      "Epoch 4559, Training Loss: 5.110063625579642e-07, Validation Loss: 8.476001052053002e-07\n",
      "Epoch 4560, Training Loss: 2.385900188528467e-06, Validation Loss: 2.77447734724716e-06\n",
      "Epoch 4561, Training Loss: 5.928685595790739e-07, Validation Loss: 9.845341838094112e-07\n",
      "Epoch 4562, Training Loss: 8.997923828246712e-07, Validation Loss: 8.98900318415069e-07\n",
      "Epoch 4563, Training Loss: 9.784881740415585e-07, Validation Loss: 8.453345761698735e-07\n",
      "Epoch 4564, Training Loss: 1.8719176750892075e-06, Validation Loss: 1.271945558554455e-06\n",
      "Epoch 4565, Training Loss: 1.791851445887005e-06, Validation Loss: 9.773170138762496e-07\n",
      "Epoch 4566, Training Loss: 1.9777030502154958e-06, Validation Loss: 1.6920144582927257e-06\n",
      "Epoch 4567, Training Loss: 1.7268233705181046e-06, Validation Loss: 9.530487687020265e-07\n",
      "Epoch 4568, Training Loss: 1.014630925055826e-06, Validation Loss: 9.270509911831448e-07\n",
      "Epoch 4569, Training Loss: 7.430743380609783e-07, Validation Loss: 8.610355845500814e-07\n",
      "Epoch 4570, Training Loss: 9.340029691884411e-07, Validation Loss: 1.134747973411745e-06\n",
      "Epoch 4571, Training Loss: 2.17444721783977e-06, Validation Loss: 9.062292539146007e-07\n",
      "Epoch 4572, Training Loss: 3.91813784972328e-07, Validation Loss: 8.434216021472311e-07\n",
      "Epoch 4573, Training Loss: 5.464631840368384e-07, Validation Loss: 9.460741148793847e-07\n",
      "Epoch 4574, Training Loss: 7.492616873605584e-07, Validation Loss: 8.437482327593852e-07\n",
      "Epoch 4575, Training Loss: 9.789316663955105e-07, Validation Loss: 8.746486720622433e-07\n",
      "Epoch 4576, Training Loss: 6.948272925910715e-07, Validation Loss: 1.3821317866508264e-06\n",
      "Epoch 4577, Training Loss: 8.699024647285114e-07, Validation Loss: 8.928590936320805e-07\n",
      "Epoch 4578, Training Loss: 1.4834549801889807e-06, Validation Loss: 1.2016496071421776e-06\n",
      "Epoch 4579, Training Loss: 7.141849209801876e-07, Validation Loss: 8.551618353235165e-07\n",
      "Epoch 4580, Training Loss: 6.615168786083814e-07, Validation Loss: 1.401960776599776e-06\n",
      "Epoch 4581, Training Loss: 6.550673674610152e-07, Validation Loss: 9.187284810569352e-07\n",
      "Epoch 4582, Training Loss: 2.5685239961603656e-06, Validation Loss: 1.2752648170361142e-06\n",
      "Epoch 4583, Training Loss: 1.2097204944439e-06, Validation Loss: 9.149436498116366e-07\n",
      "Epoch 4584, Training Loss: 2.3688467081228737e-06, Validation Loss: 3.848879744909473e-06\n",
      "Epoch 4585, Training Loss: 7.43246800993802e-07, Validation Loss: 9.395929788778324e-07\n",
      "Epoch 4586, Training Loss: 1.206873207593162e-06, Validation Loss: 1.1488357588665734e-06\n",
      "Epoch 4587, Training Loss: 1.4300828752311645e-06, Validation Loss: 9.249285967728894e-07\n",
      "Epoch 4588, Training Loss: 3.334187567816116e-06, Validation Loss: 1.4071841089453358e-06\n",
      "Epoch 4589, Training Loss: 6.083471362217097e-07, Validation Loss: 8.542973960913607e-07\n",
      "Epoch 4590, Training Loss: 3.2304985779774142e-06, Validation Loss: 7.693850078980014e-06\n",
      "Epoch 4591, Training Loss: 1.9167582649970427e-06, Validation Loss: 1.0657537514978998e-06\n",
      "Epoch 4592, Training Loss: 1.7866855159809347e-06, Validation Loss: 2.2269202568718443e-06\n",
      "Epoch 4593, Training Loss: 8.696273425812251e-07, Validation Loss: 8.467176154722061e-07\n",
      "Epoch 4594, Training Loss: 7.411836122628301e-07, Validation Loss: 9.085930532732581e-07\n",
      "Epoch 4595, Training Loss: 2.991144356201403e-06, Validation Loss: 1.2259482148484223e-06\n",
      "Epoch 4596, Training Loss: 1.2096957107132766e-06, Validation Loss: 8.788686852984805e-07\n",
      "Epoch 4597, Training Loss: 2.3922291347844293e-06, Validation Loss: 9.063205813754104e-07\n",
      "Epoch 4598, Training Loss: 2.147683971998049e-06, Validation Loss: 1.5610304082575412e-06\n",
      "Epoch 4599, Training Loss: 1.1824248531411286e-06, Validation Loss: 1.0925445703804284e-06\n",
      "Epoch 4600, Training Loss: 9.304376362706535e-07, Validation Loss: 1.6783249203648561e-06\n",
      "Epoch 4601, Training Loss: 1.2975773415746517e-06, Validation Loss: 8.758578756812897e-07\n",
      "Epoch 4602, Training Loss: 6.844400104455417e-07, Validation Loss: 8.947792664935584e-07\n",
      "Epoch 4603, Training Loss: 8.172263505912269e-07, Validation Loss: 8.53588730274603e-07\n",
      "Epoch 4604, Training Loss: 6.298770358625916e-07, Validation Loss: 9.581993114204652e-07\n",
      "Epoch 4605, Training Loss: 1.883453023765469e-06, Validation Loss: 1.75802533537425e-06\n",
      "Epoch 4606, Training Loss: 3.860022388835205e-06, Validation Loss: 7.20679778331345e-06\n",
      "Epoch 4607, Training Loss: 1.4147370848149876e-06, Validation Loss: 8.747432819589472e-07\n",
      "Epoch 4608, Training Loss: 2.362548912060447e-06, Validation Loss: 1.7335258330691014e-06\n",
      "Epoch 4609, Training Loss: 1.1864397038152674e-06, Validation Loss: 8.543124903724582e-07\n",
      "Epoch 4610, Training Loss: 6.052748631191207e-07, Validation Loss: 8.503008884626342e-07\n",
      "Epoch 4611, Training Loss: 9.819389106269227e-07, Validation Loss: 1.2490429518982834e-06\n",
      "Epoch 4612, Training Loss: 1.558771828058525e-06, Validation Loss: 1.0164006692315188e-06\n",
      "Epoch 4613, Training Loss: 1.5674368114559911e-06, Validation Loss: 9.148173133218884e-07\n",
      "Epoch 4614, Training Loss: 1.3556231124312035e-06, Validation Loss: 8.831542775560534e-07\n",
      "Epoch 4615, Training Loss: 1.492146452619636e-06, Validation Loss: 1.271339792359995e-06\n",
      "Epoch 4616, Training Loss: 6.201258315741143e-07, Validation Loss: 8.501177766213039e-07\n",
      "Epoch 4617, Training Loss: 9.898220696413773e-07, Validation Loss: 2.496475930902712e-06\n",
      "Epoch 4618, Training Loss: 5.840970516146626e-07, Validation Loss: 8.621763112196103e-07\n",
      "Epoch 4619, Training Loss: 1.24962343761581e-06, Validation Loss: 1.01717702831894e-06\n",
      "Epoch 4620, Training Loss: 9.561183560435893e-07, Validation Loss: 8.679531807953767e-07\n",
      "Epoch 4621, Training Loss: 1.8651065829544677e-06, Validation Loss: 1.3717639279377274e-06\n",
      "Epoch 4622, Training Loss: 6.633401312683418e-07, Validation Loss: 8.508118563081639e-07\n",
      "Epoch 4623, Training Loss: 1.3238086467026733e-06, Validation Loss: 1.2697975605287228e-06\n",
      "Epoch 4624, Training Loss: 9.26681536839169e-07, Validation Loss: 1.004796279697301e-06\n",
      "Epoch 4625, Training Loss: 1.0994408512488008e-06, Validation Loss: 8.927691935927921e-07\n",
      "Epoch 4626, Training Loss: 2.514751486160094e-06, Validation Loss: 1.0708965740111131e-06\n",
      "Epoch 4627, Training Loss: 1.1572053608688293e-06, Validation Loss: 9.515688412404855e-07\n",
      "Epoch 4628, Training Loss: 4.511272891249973e-06, Validation Loss: 2.0462569034479765e-06\n",
      "Epoch 4629, Training Loss: 5.458971372718224e-07, Validation Loss: 9.91870053843984e-07\n",
      "Epoch 4630, Training Loss: 1.5179257388808765e-06, Validation Loss: 2.3333444499252102e-06\n",
      "Epoch 4631, Training Loss: 8.714574732948677e-07, Validation Loss: 8.513719044632201e-07\n",
      "Epoch 4632, Training Loss: 6.136693286862283e-07, Validation Loss: 9.586981779538274e-07\n",
      "Epoch 4633, Training Loss: 1.261113652617496e-06, Validation Loss: 8.809557821179832e-07\n",
      "Epoch 4634, Training Loss: 4.793034236172389e-07, Validation Loss: 8.928210327994632e-07\n",
      "Epoch 4635, Training Loss: 1.7226554973603925e-06, Validation Loss: 1.0970544490278863e-06\n",
      "Epoch 4636, Training Loss: 1.6003577911760658e-06, Validation Loss: 1.3035444537633055e-06\n",
      "Epoch 4637, Training Loss: 4.053462703268451e-07, Validation Loss: 8.529791906908987e-07\n",
      "Epoch 4638, Training Loss: 1.1541831099748379e-06, Validation Loss: 8.458599628735842e-07\n",
      "Epoch 4639, Training Loss: 9.46225441111892e-07, Validation Loss: 8.716464624560601e-07\n",
      "Epoch 4640, Training Loss: 1.1727797755156644e-06, Validation Loss: 8.429023481967075e-07\n",
      "Epoch 4641, Training Loss: 2.5627496142988093e-06, Validation Loss: 1.6098349276359557e-06\n",
      "Epoch 4642, Training Loss: 8.451519306618138e-07, Validation Loss: 9.429189894162962e-07\n",
      "Epoch 4643, Training Loss: 7.535859367635567e-07, Validation Loss: 9.327737986849147e-07\n",
      "Epoch 4644, Training Loss: 7.705564257776132e-07, Validation Loss: 8.430550759634466e-07\n",
      "Epoch 4645, Training Loss: 8.243790716733201e-07, Validation Loss: 8.61716915923567e-07\n",
      "Epoch 4646, Training Loss: 1.0856711014639586e-06, Validation Loss: 9.858249132011439e-07\n",
      "Epoch 4647, Training Loss: 1.0861112968996167e-06, Validation Loss: 1.0743977288987069e-06\n",
      "Epoch 4648, Training Loss: 9.67449864219816e-07, Validation Loss: 8.65116621239134e-07\n",
      "Epoch 4649, Training Loss: 8.919093943404732e-07, Validation Loss: 8.808337331426692e-07\n",
      "Epoch 4650, Training Loss: 8.129793513944605e-07, Validation Loss: 8.366433802625376e-07\n",
      "Epoch 4651, Training Loss: 7.105902568582678e-07, Validation Loss: 1.0026118066617197e-06\n",
      "Epoch 4652, Training Loss: 1.1147292298119282e-06, Validation Loss: 9.387059331628196e-07\n",
      "Epoch 4653, Training Loss: 7.902449397079181e-07, Validation Loss: 8.682530841388665e-07\n",
      "Epoch 4654, Training Loss: 2.307157728864695e-06, Validation Loss: 2.6934629459223997e-06\n",
      "Epoch 4655, Training Loss: 1.453221557312645e-06, Validation Loss: 1.2057369757174131e-06\n",
      "Epoch 4656, Training Loss: 5.745475846197223e-06, Validation Loss: 1.0198785739297946e-05\n",
      "Epoch 4657, Training Loss: 7.051125976431649e-06, Validation Loss: 6.7816468237802475e-06\n",
      "Epoch 4658, Training Loss: 8.533670552424155e-07, Validation Loss: 8.682850744926153e-07\n",
      "Epoch 4659, Training Loss: 1.1687776577673503e-06, Validation Loss: 1.100487657564882e-06\n",
      "Epoch 4660, Training Loss: 1.6675625147399842e-06, Validation Loss: 2.9066310078308845e-06\n",
      "Epoch 4661, Training Loss: 7.732214157840644e-07, Validation Loss: 8.848457139482756e-07\n",
      "Epoch 4662, Training Loss: 5.257795123725373e-07, Validation Loss: 8.321949322435334e-07\n",
      "Epoch 4663, Training Loss: 7.580861165479291e-07, Validation Loss: 8.517189433109851e-07\n",
      "Epoch 4664, Training Loss: 1.6453698208351852e-06, Validation Loss: 1.193033989719572e-06\n",
      "Epoch 4665, Training Loss: 9.96673179542995e-07, Validation Loss: 1.0874342398550837e-06\n",
      "Epoch 4666, Training Loss: 9.55786845224793e-07, Validation Loss: 9.822166193307972e-07\n",
      "Epoch 4667, Training Loss: 1.7233104472325067e-06, Validation Loss: 1.7991643546199279e-06\n",
      "Epoch 4668, Training Loss: 7.075931875988317e-07, Validation Loss: 1.0206950888737748e-06\n",
      "Epoch 4669, Training Loss: 2.166684680560138e-06, Validation Loss: 1.1406382459701988e-06\n",
      "Epoch 4670, Training Loss: 1.1514273410284659e-06, Validation Loss: 8.484647348810123e-07\n",
      "Epoch 4671, Training Loss: 8.220933409575082e-07, Validation Loss: 1.0086979674345278e-06\n",
      "Epoch 4672, Training Loss: 8.539279292563151e-07, Validation Loss: 9.843487332415578e-07\n",
      "Epoch 4673, Training Loss: 5.97497546550585e-07, Validation Loss: 8.87682347183727e-07\n",
      "Epoch 4674, Training Loss: 7.115333460205875e-07, Validation Loss: 1.0496829236362133e-06\n",
      "Epoch 4675, Training Loss: 6.807922545704059e-07, Validation Loss: 1.2730030796739614e-06\n",
      "Epoch 4676, Training Loss: 9.747845979291014e-07, Validation Loss: 9.810867209705015e-07\n",
      "Epoch 4677, Training Loss: 6.013960955897346e-07, Validation Loss: 9.968496924788976e-07\n",
      "Epoch 4678, Training Loss: 1.1282270406809403e-06, Validation Loss: 8.542180770743303e-07\n",
      "Epoch 4679, Training Loss: 7.537357191722549e-07, Validation Loss: 8.553448933989379e-07\n",
      "Epoch 4680, Training Loss: 6.581683464901289e-07, Validation Loss: 9.762980847313699e-07\n",
      "Epoch 4681, Training Loss: 1.0028604719991563e-06, Validation Loss: 1.2529201893621402e-06\n",
      "Epoch 4682, Training Loss: 1.0773778740258422e-06, Validation Loss: 1.0041202515975592e-06\n",
      "Epoch 4683, Training Loss: 1.937003617058508e-06, Validation Loss: 2.065635273892017e-06\n",
      "Epoch 4684, Training Loss: 9.900259101414122e-07, Validation Loss: 1.190797468025614e-06\n",
      "Epoch 4685, Training Loss: 5.525969299924327e-07, Validation Loss: 8.657151388115091e-07\n",
      "Epoch 4686, Training Loss: 7.451766350641265e-07, Validation Loss: 8.864232194743409e-07\n",
      "Epoch 4687, Training Loss: 7.764064662296732e-07, Validation Loss: 1.006564023310504e-06\n",
      "Epoch 4688, Training Loss: 6.612444849452004e-07, Validation Loss: 8.458102102186513e-07\n",
      "Epoch 4689, Training Loss: 5.30401280229853e-07, Validation Loss: 8.859284684769162e-07\n",
      "Epoch 4690, Training Loss: 7.095854925864842e-07, Validation Loss: 1.1236635217593772e-06\n",
      "Epoch 4691, Training Loss: 1.111182086788176e-06, Validation Loss: 9.56373702474072e-07\n",
      "Epoch 4692, Training Loss: 2.8787235351046547e-06, Validation Loss: 2.74064947458842e-06\n",
      "Epoch 4693, Training Loss: 4.463894583750516e-06, Validation Loss: 3.699934188644313e-06\n",
      "Epoch 4694, Training Loss: 6.837292403361062e-07, Validation Loss: 1.148473972410154e-06\n",
      "Epoch 4695, Training Loss: 1.007126115837309e-06, Validation Loss: 9.8879104497729e-07\n",
      "Epoch 4696, Training Loss: 7.657663445570506e-07, Validation Loss: 8.717231844167625e-07\n",
      "Epoch 4697, Training Loss: 1.1933971109101549e-06, Validation Loss: 1.215396599465146e-06\n",
      "Epoch 4698, Training Loss: 1.0325370567443315e-06, Validation Loss: 9.56958901307554e-07\n",
      "Epoch 4699, Training Loss: 1.2035843610647134e-06, Validation Loss: 1.1093402494077582e-06\n",
      "Epoch 4700, Training Loss: 6.993931265242281e-07, Validation Loss: 9.093581718461444e-07\n",
      "Epoch 4701, Training Loss: 4.235976689415111e-07, Validation Loss: 8.51648692267503e-07\n",
      "Epoch 4702, Training Loss: 6.514433152915444e-07, Validation Loss: 8.912023051139539e-07\n",
      "Epoch 4703, Training Loss: 6.365553417708725e-07, Validation Loss: 8.633475495898776e-07\n",
      "Epoch 4704, Training Loss: 9.431980743102031e-07, Validation Loss: 9.02704790147764e-07\n",
      "Epoch 4705, Training Loss: 1.934216470544925e-06, Validation Loss: 8.617185902700034e-07\n",
      "Epoch 4706, Training Loss: 9.496813504483725e-07, Validation Loss: 8.745502400792259e-07\n",
      "Epoch 4707, Training Loss: 2.1856228613614803e-06, Validation Loss: 8.451076263761667e-07\n",
      "Epoch 4708, Training Loss: 1.0278677109454293e-06, Validation Loss: 1.0368717555607804e-06\n",
      "Epoch 4709, Training Loss: 2.345140046600136e-06, Validation Loss: 8.662339691518413e-07\n",
      "Epoch 4710, Training Loss: 8.343147328560008e-07, Validation Loss: 9.89512374747444e-07\n",
      "Epoch 4711, Training Loss: 8.572570777687361e-07, Validation Loss: 8.792662088864837e-07\n",
      "Epoch 4712, Training Loss: 1.0072324130305788e-06, Validation Loss: 1.0033894678979047e-06\n",
      "Epoch 4713, Training Loss: 1.0060919066745555e-06, Validation Loss: 8.578067141645809e-07\n",
      "Epoch 4714, Training Loss: 5.612497488982626e-07, Validation Loss: 9.639560917354741e-07\n",
      "Epoch 4715, Training Loss: 1.2320966789047816e-06, Validation Loss: 8.754820074445425e-07\n",
      "Epoch 4716, Training Loss: 1.1871234164573252e-06, Validation Loss: 2.5829720204461744e-06\n",
      "Epoch 4717, Training Loss: 7.173898666223977e-07, Validation Loss: 9.649876833855103e-07\n",
      "Epoch 4718, Training Loss: 1.009119841910433e-06, Validation Loss: 9.80158236584169e-07\n",
      "Epoch 4719, Training Loss: 1.1657759841909865e-06, Validation Loss: 9.134027433009677e-07\n",
      "Epoch 4720, Training Loss: 1.2976427115063416e-06, Validation Loss: 8.44736270221564e-07\n",
      "Epoch 4721, Training Loss: 1.3441683677228866e-06, Validation Loss: 8.875888899050099e-07\n",
      "Epoch 4722, Training Loss: 1.1785971310018795e-06, Validation Loss: 1.0430069910311849e-06\n",
      "Epoch 4723, Training Loss: 1.477303840147215e-06, Validation Loss: 1.263346095705636e-06\n",
      "Epoch 4724, Training Loss: 1.100997906178236e-06, Validation Loss: 8.646384808300563e-07\n",
      "Epoch 4725, Training Loss: 1.0244179975416046e-06, Validation Loss: 1.4064377624589533e-06\n",
      "Epoch 4726, Training Loss: 3.4143881748605054e-06, Validation Loss: 2.9149650336372235e-06\n",
      "Epoch 4727, Training Loss: 1.0533938166190637e-06, Validation Loss: 8.566629579580627e-07\n",
      "Epoch 4728, Training Loss: 2.359276095376117e-06, Validation Loss: 8.768517966183506e-07\n",
      "Epoch 4729, Training Loss: 2.6097945919900667e-06, Validation Loss: 4.136034695371436e-06\n",
      "Epoch 4730, Training Loss: 1.043222937369137e-06, Validation Loss: 8.939636903349334e-07\n",
      "Epoch 4731, Training Loss: 1.4459795920629404e-06, Validation Loss: 1.9181362682259064e-06\n",
      "Epoch 4732, Training Loss: 1.2185471405246062e-06, Validation Loss: 1.0450372963874358e-06\n",
      "Epoch 4733, Training Loss: 1.359360794594977e-06, Validation Loss: 9.463565004932154e-07\n",
      "Epoch 4734, Training Loss: 1.410201434737246e-06, Validation Loss: 8.547443732727429e-07\n",
      "Epoch 4735, Training Loss: 7.479407031496521e-07, Validation Loss: 8.587964263352562e-07\n",
      "Epoch 4736, Training Loss: 1.0335101023883908e-06, Validation Loss: 1.6996684684515926e-06\n",
      "Epoch 4737, Training Loss: 1.5659823020541808e-06, Validation Loss: 9.06662600442406e-07\n",
      "Epoch 4738, Training Loss: 2.011202468565898e-06, Validation Loss: 9.082721728979088e-07\n",
      "Epoch 4739, Training Loss: 1.5218604403344216e-06, Validation Loss: 9.139766489401861e-07\n",
      "Epoch 4740, Training Loss: 1.0966140280288528e-06, Validation Loss: 8.691372431495571e-07\n",
      "Epoch 4741, Training Loss: 6.90874344400072e-07, Validation Loss: 1.0003395257505183e-06\n",
      "Epoch 4742, Training Loss: 5.932330395808094e-07, Validation Loss: 9.723994104194628e-07\n",
      "Epoch 4743, Training Loss: 1.0295569836671348e-06, Validation Loss: 8.783531676260519e-07\n",
      "Epoch 4744, Training Loss: 2.5441290745220613e-06, Validation Loss: 1.856655639649092e-06\n",
      "Epoch 4745, Training Loss: 7.33850356482435e-07, Validation Loss: 1.2059337491684498e-06\n",
      "Epoch 4746, Training Loss: 1.6393294117733603e-06, Validation Loss: 8.496605629177468e-07\n",
      "Epoch 4747, Training Loss: 4.089297362952493e-07, Validation Loss: 8.317719623551201e-07\n",
      "Epoch 4748, Training Loss: 1.1501738299557474e-06, Validation Loss: 1.0758785745443895e-06\n",
      "Epoch 4749, Training Loss: 5.99060797412676e-07, Validation Loss: 8.57488308831342e-07\n",
      "Epoch 4750, Training Loss: 9.425547204955365e-07, Validation Loss: 2.995675302395719e-06\n",
      "Epoch 4751, Training Loss: 7.7262984632398e-07, Validation Loss: 8.872665325059203e-07\n",
      "Epoch 4752, Training Loss: 3.2236994229606353e-06, Validation Loss: 2.210873160594442e-06\n",
      "Epoch 4753, Training Loss: 1.034113211062504e-06, Validation Loss: 8.557352820516942e-07\n",
      "Epoch 4754, Training Loss: 3.3351620913890656e-06, Validation Loss: 1.377609450967258e-06\n",
      "Epoch 4755, Training Loss: 1.4428030681301607e-06, Validation Loss: 1.0133955339995644e-06\n",
      "Epoch 4756, Training Loss: 7.31320028535265e-07, Validation Loss: 8.771977085543883e-07\n",
      "Epoch 4757, Training Loss: 1.3198267652114737e-06, Validation Loss: 1.942738887751408e-06\n",
      "Epoch 4758, Training Loss: 1.2398168109939434e-06, Validation Loss: 8.71580803781526e-07\n",
      "Epoch 4759, Training Loss: 7.519837481595459e-07, Validation Loss: 8.592696941409514e-07\n",
      "Epoch 4760, Training Loss: 1.8303032902622363e-06, Validation Loss: 8.574772002876747e-07\n",
      "Epoch 4761, Training Loss: 1.3499437727659824e-06, Validation Loss: 9.58727140942947e-07\n",
      "Epoch 4762, Training Loss: 6.641872687396244e-07, Validation Loss: 8.698347651217982e-07\n",
      "Epoch 4763, Training Loss: 8.7670912307658e-07, Validation Loss: 8.392519561627645e-07\n",
      "Epoch 4764, Training Loss: 7.495253839806537e-07, Validation Loss: 1.4363813461734177e-06\n",
      "Epoch 4765, Training Loss: 9.386187684867764e-07, Validation Loss: 8.473694110776635e-07\n",
      "Epoch 4766, Training Loss: 1.6665884459143854e-06, Validation Loss: 1.4229278340686378e-06\n",
      "Epoch 4767, Training Loss: 6.371234348989674e-07, Validation Loss: 9.593958659305752e-07\n",
      "Epoch 4768, Training Loss: 6.419591045414563e-07, Validation Loss: 8.951789369162887e-07\n",
      "Epoch 4769, Training Loss: 7.596595423819963e-07, Validation Loss: 8.338240987522386e-07\n",
      "Epoch 4770, Training Loss: 6.633661087107612e-07, Validation Loss: 8.916944635808006e-07\n",
      "Epoch 4771, Training Loss: 1.0115500117535703e-06, Validation Loss: 9.145876507031711e-07\n",
      "Epoch 4772, Training Loss: 1.2734034271488781e-06, Validation Loss: 9.360537904931508e-07\n",
      "Epoch 4773, Training Loss: 8.873402634890226e-07, Validation Loss: 8.885653481455309e-07\n",
      "Epoch 4774, Training Loss: 7.258010441546503e-07, Validation Loss: 9.669051485813325e-07\n",
      "Epoch 4775, Training Loss: 6.522495823446661e-07, Validation Loss: 8.541380326701045e-07\n",
      "Epoch 4776, Training Loss: 7.178177838795818e-07, Validation Loss: 8.620022979458977e-07\n",
      "Epoch 4777, Training Loss: 6.596737875952385e-07, Validation Loss: 1.228453163114221e-06\n",
      "Epoch 4778, Training Loss: 6.028768666510587e-07, Validation Loss: 9.333243241190717e-07\n",
      "Epoch 4779, Training Loss: 6.429546601793845e-07, Validation Loss: 9.286914386665887e-07\n",
      "Epoch 4780, Training Loss: 3.2679042760719312e-06, Validation Loss: 3.3681150533165395e-06\n",
      "Epoch 4781, Training Loss: 9.185425255964219e-07, Validation Loss: 9.543621418687955e-07\n",
      "Epoch 4782, Training Loss: 2.265591774630593e-06, Validation Loss: 9.543058583757133e-07\n",
      "Epoch 4783, Training Loss: 4.378463472676231e-06, Validation Loss: 9.735106768104719e-07\n",
      "Epoch 4784, Training Loss: 1.8069102907247725e-06, Validation Loss: 1.2187079780731229e-06\n",
      "Epoch 4785, Training Loss: 2.9154502954042982e-06, Validation Loss: 9.172991760994597e-07\n",
      "Epoch 4786, Training Loss: 7.143122502384358e-07, Validation Loss: 8.654520645725413e-07\n",
      "Epoch 4787, Training Loss: 9.978405159927206e-07, Validation Loss: 1.1125773220813643e-06\n",
      "Epoch 4788, Training Loss: 1.6473853747811518e-06, Validation Loss: 1.3010431644757934e-06\n",
      "Epoch 4789, Training Loss: 1.9627691472123843e-06, Validation Loss: 1.5737084830796412e-06\n",
      "Epoch 4790, Training Loss: 1.214019107464992e-06, Validation Loss: 1.1335464959719138e-06\n",
      "Epoch 4791, Training Loss: 9.414097235094232e-07, Validation Loss: 8.369602001350683e-07\n",
      "Epoch 4792, Training Loss: 8.150068424583878e-07, Validation Loss: 9.207446050663603e-07\n",
      "Epoch 4793, Training Loss: 1.035338755173143e-06, Validation Loss: 1.6203565673483127e-06\n",
      "Epoch 4794, Training Loss: 1.36230562475248e-06, Validation Loss: 8.555226610537452e-07\n",
      "Epoch 4795, Training Loss: 2.0262127691239584e-06, Validation Loss: 2.3269415022273306e-06\n",
      "Epoch 4796, Training Loss: 3.118314225503127e-06, Validation Loss: 2.5049789909337433e-06\n",
      "Epoch 4797, Training Loss: 1.2231944310769904e-06, Validation Loss: 1.0509770134877685e-06\n",
      "Epoch 4798, Training Loss: 1.0860378552024486e-06, Validation Loss: 8.695066469861639e-07\n",
      "Epoch 4799, Training Loss: 9.125359383688192e-07, Validation Loss: 8.920412095220316e-07\n",
      "Epoch 4800, Training Loss: 7.613282377860742e-07, Validation Loss: 8.244197402430351e-07\n",
      "Epoch 4801, Training Loss: 8.65387562498654e-07, Validation Loss: 8.854443068291292e-07\n",
      "Epoch 4802, Training Loss: 7.162638553381839e-07, Validation Loss: 9.240784837115564e-07\n",
      "Epoch 4803, Training Loss: 7.718403480794223e-07, Validation Loss: 8.752034341414053e-07\n",
      "Epoch 4804, Training Loss: 2.454439254506724e-06, Validation Loss: 1.7161076880111763e-06\n",
      "Epoch 4805, Training Loss: 7.925825116217311e-07, Validation Loss: 1.089772492622713e-06\n",
      "Epoch 4806, Training Loss: 6.642202379225637e-07, Validation Loss: 1.0364141334535515e-06\n",
      "Epoch 4807, Training Loss: 7.843665912332654e-07, Validation Loss: 8.664158454634463e-07\n",
      "Epoch 4808, Training Loss: 7.806510780028475e-07, Validation Loss: 8.475327787014095e-07\n",
      "Epoch 4809, Training Loss: 8.130173227982596e-07, Validation Loss: 1.1693355491310297e-06\n",
      "Epoch 4810, Training Loss: 1.0092429647556855e-06, Validation Loss: 1.5401955881321892e-06\n",
      "Epoch 4811, Training Loss: 1.3529156603908632e-06, Validation Loss: 1.0080113275374287e-06\n",
      "Epoch 4812, Training Loss: 4.727793907477462e-07, Validation Loss: 8.874233875755467e-07\n",
      "Epoch 4813, Training Loss: 6.410634227904666e-07, Validation Loss: 8.776431996605302e-07\n",
      "Epoch 4814, Training Loss: 9.396916880177741e-07, Validation Loss: 1.0086917977512214e-06\n",
      "Epoch 4815, Training Loss: 9.037546533363638e-07, Validation Loss: 8.922857781506502e-07\n",
      "Epoch 4816, Training Loss: 8.705744676262839e-07, Validation Loss: 1.0027066356047192e-06\n",
      "Epoch 4817, Training Loss: 1.1222666671528714e-06, Validation Loss: 9.469814625654006e-07\n",
      "Epoch 4818, Training Loss: 8.299543878820259e-07, Validation Loss: 8.431708678179661e-07\n",
      "Epoch 4819, Training Loss: 8.283557235699845e-07, Validation Loss: 1.0811698003797887e-06\n",
      "Epoch 4820, Training Loss: 1.938839659487712e-06, Validation Loss: 1.546203565670164e-06\n",
      "Epoch 4821, Training Loss: 1.061721832229523e-06, Validation Loss: 9.08237333494077e-07\n",
      "Epoch 4822, Training Loss: 7.405361657220055e-07, Validation Loss: 8.767945102190888e-07\n",
      "Epoch 4823, Training Loss: 9.27988367038779e-07, Validation Loss: 8.370136326772525e-07\n",
      "Epoch 4824, Training Loss: 1.1617117934292764e-06, Validation Loss: 8.575232450862218e-07\n",
      "Epoch 4825, Training Loss: 2.265952844027197e-06, Validation Loss: 1.7202004033073547e-06\n",
      "Epoch 4826, Training Loss: 6.252366802073084e-07, Validation Loss: 8.882247590344765e-07\n",
      "Epoch 4827, Training Loss: 1.8709752112044953e-06, Validation Loss: 1.66156298514782e-06\n",
      "Epoch 4828, Training Loss: 1.8303444448974915e-06, Validation Loss: 1.4262146726123668e-06\n",
      "Epoch 4829, Training Loss: 5.203346518101171e-07, Validation Loss: 8.983632827931516e-07\n",
      "Epoch 4830, Training Loss: 1.188464239021414e-06, Validation Loss: 1.1705781698012264e-06\n",
      "Epoch 4831, Training Loss: 1.7408000303476001e-06, Validation Loss: 1.1076068406677873e-06\n",
      "Epoch 4832, Training Loss: 1.4649425793322735e-05, Validation Loss: 2.3647140227069122e-05\n",
      "Epoch 4833, Training Loss: 1.0270151733493549e-06, Validation Loss: 8.860331103318991e-07\n",
      "Epoch 4834, Training Loss: 1.969952336366987e-06, Validation Loss: 9.978946783573352e-07\n",
      "Epoch 4835, Training Loss: 5.953888830845244e-06, Validation Loss: 3.7565273806029356e-06\n",
      "Epoch 4836, Training Loss: 1.216698478856415e-06, Validation Loss: 9.296072009888961e-07\n",
      "Epoch 4837, Training Loss: 1.5936878980937763e-06, Validation Loss: 9.204019409894876e-07\n",
      "Epoch 4838, Training Loss: 1.135264369622746e-06, Validation Loss: 1.2444215323426808e-06\n",
      "Epoch 4839, Training Loss: 1.284767904508044e-06, Validation Loss: 8.89299592233566e-07\n",
      "Epoch 4840, Training Loss: 5.178049491405545e-07, Validation Loss: 8.551102236715509e-07\n",
      "Epoch 4841, Training Loss: 1.007201262837043e-06, Validation Loss: 8.704114814327402e-07\n",
      "Epoch 4842, Training Loss: 1.6780235227997764e-06, Validation Loss: 1.56150848313376e-06\n",
      "Epoch 4843, Training Loss: 1.0599231927699293e-06, Validation Loss: 1.04350721673776e-06\n",
      "Epoch 4844, Training Loss: 2.2818055640527746e-06, Validation Loss: 1.6726217263770132e-06\n",
      "Epoch 4845, Training Loss: 4.172900844423566e-06, Validation Loss: 4.361267562214212e-06\n",
      "Epoch 4846, Training Loss: 6.790534712308727e-07, Validation Loss: 9.57000819527334e-07\n",
      "Epoch 4847, Training Loss: 5.715109523407591e-07, Validation Loss: 8.895985607381544e-07\n",
      "Epoch 4848, Training Loss: 9.556773648000672e-07, Validation Loss: 9.236659670382237e-07\n",
      "Epoch 4849, Training Loss: 7.041797971396591e-07, Validation Loss: 8.363310389816022e-07\n",
      "Epoch 4850, Training Loss: 6.855021865703748e-07, Validation Loss: 8.897903620193105e-07\n",
      "Epoch 4851, Training Loss: 9.064271466741047e-07, Validation Loss: 1.3130774817863353e-06\n",
      "Epoch 4852, Training Loss: 2.0114689505135175e-06, Validation Loss: 1.0742896152504536e-06\n",
      "Epoch 4853, Training Loss: 9.740123232404585e-07, Validation Loss: 9.847500170036157e-07\n",
      "Epoch 4854, Training Loss: 7.495932550227735e-07, Validation Loss: 8.548597847832399e-07\n",
      "Epoch 4855, Training Loss: 5.725125333810865e-07, Validation Loss: 9.058093626223056e-07\n",
      "Epoch 4856, Training Loss: 1.6148140957739088e-06, Validation Loss: 1.8643225463849006e-06\n",
      "Epoch 4857, Training Loss: 2.8675904104602523e-06, Validation Loss: 3.7375023792635734e-06\n",
      "Epoch 4858, Training Loss: 2.8561503313540015e-06, Validation Loss: 3.1997472239367383e-06\n",
      "Epoch 4859, Training Loss: 7.348136819018691e-07, Validation Loss: 8.926607365306357e-07\n",
      "Epoch 4860, Training Loss: 1.2219846894367947e-06, Validation Loss: 1.0629405504231187e-06\n",
      "Epoch 4861, Training Loss: 1.1076956980105024e-05, Validation Loss: 2.6264083787751517e-06\n",
      "Epoch 4862, Training Loss: 2.565521754149813e-06, Validation Loss: 1.0287286380146256e-06\n",
      "Epoch 4863, Training Loss: 5.664387572323903e-07, Validation Loss: 8.803646293889504e-07\n",
      "Epoch 4864, Training Loss: 4.968219400325324e-07, Validation Loss: 8.634025084864763e-07\n",
      "Epoch 4865, Training Loss: 1.2146679182478692e-06, Validation Loss: 9.250056820607946e-07\n",
      "Epoch 4866, Training Loss: 9.71556119111483e-07, Validation Loss: 1.4489314892694418e-06\n",
      "Epoch 4867, Training Loss: 5.158323688192468e-07, Validation Loss: 8.73509612860011e-07\n",
      "Epoch 4868, Training Loss: 7.314946515180054e-07, Validation Loss: 1.0028661856679018e-06\n",
      "Epoch 4869, Training Loss: 1.1900231129402528e-06, Validation Loss: 8.59753978165016e-07\n",
      "Epoch 4870, Training Loss: 2.984595539601287e-06, Validation Loss: 5.859970231216095e-06\n",
      "Epoch 4871, Training Loss: 1.8114004660674254e-06, Validation Loss: 1.3383734099919427e-06\n",
      "Epoch 4872, Training Loss: 1.0586250027699862e-06, Validation Loss: 8.422675931258384e-07\n",
      "Epoch 4873, Training Loss: 5.369547579903156e-07, Validation Loss: 9.2488116600869e-07\n",
      "Epoch 4874, Training Loss: 9.11444431039854e-07, Validation Loss: 9.29032284478178e-07\n",
      "Epoch 4875, Training Loss: 2.0032134671055246e-06, Validation Loss: 1.3851541903341347e-06\n",
      "Epoch 4876, Training Loss: 8.413325645051373e-07, Validation Loss: 8.547366063617785e-07\n",
      "Epoch 4877, Training Loss: 1.2888779110653559e-06, Validation Loss: 1.365321559509898e-06\n",
      "Epoch 4878, Training Loss: 6.616032806050498e-07, Validation Loss: 8.847405494596931e-07\n",
      "Epoch 4879, Training Loss: 5.974537771180621e-07, Validation Loss: 9.112801560937662e-07\n",
      "Epoch 4880, Training Loss: 4.0887752561502566e-07, Validation Loss: 8.329628972415678e-07\n",
      "Epoch 4881, Training Loss: 6.963023224670906e-07, Validation Loss: 1.0810235382803998e-06\n",
      "Epoch 4882, Training Loss: 1.8154927374780527e-06, Validation Loss: 1.753773222080086e-06\n",
      "Epoch 4883, Training Loss: 5.470662927109515e-07, Validation Loss: 9.089597970550982e-07\n",
      "Epoch 4884, Training Loss: 1.1120100680273026e-06, Validation Loss: 8.549647278721368e-07\n",
      "Epoch 4885, Training Loss: 1.2321347639954183e-06, Validation Loss: 8.691678459083838e-07\n",
      "Epoch 4886, Training Loss: 6.391059628185758e-07, Validation Loss: 9.564565010686625e-07\n",
      "Epoch 4887, Training Loss: 5.881130391571787e-07, Validation Loss: 8.635669041795716e-07\n",
      "Epoch 4888, Training Loss: 1.1486345101729967e-06, Validation Loss: 1.1909384567212084e-06\n",
      "Epoch 4889, Training Loss: 6.787638540117769e-07, Validation Loss: 8.846883999676083e-07\n",
      "Epoch 4890, Training Loss: 2.2923459255252965e-06, Validation Loss: 1.6163201444156862e-06\n",
      "Epoch 4891, Training Loss: 2.2008516680216417e-06, Validation Loss: 1.8080124686858439e-06\n",
      "Epoch 4892, Training Loss: 1.210980371979531e-06, Validation Loss: 1.0217314385352056e-06\n",
      "Epoch 4893, Training Loss: 3.590598396385758e-07, Validation Loss: 8.483346463635595e-07\n",
      "Epoch 4894, Training Loss: 1.088294425244385e-06, Validation Loss: 9.029325392794197e-07\n",
      "Epoch 4895, Training Loss: 7.594866247018217e-07, Validation Loss: 8.496413176188326e-07\n",
      "Epoch 4896, Training Loss: 5.756818381996709e-07, Validation Loss: 1.0227381785683697e-06\n",
      "Epoch 4897, Training Loss: 1.2632738162210444e-06, Validation Loss: 8.775112391118849e-07\n",
      "Epoch 4898, Training Loss: 6.977951443332131e-07, Validation Loss: 1.3616164493492517e-06\n",
      "Epoch 4899, Training Loss: 1.582699724167469e-06, Validation Loss: 8.357071614601253e-07\n",
      "Epoch 4900, Training Loss: 6.023958576406585e-07, Validation Loss: 8.882879133400409e-07\n",
      "Epoch 4901, Training Loss: 1.2039974990329938e-06, Validation Loss: 1.0444710700406633e-06\n",
      "Epoch 4902, Training Loss: 1.3003136700717732e-06, Validation Loss: 1.2111586152001387e-06\n",
      "Epoch 4903, Training Loss: 9.924604000843829e-07, Validation Loss: 1.0801955827475327e-06\n",
      "Epoch 4904, Training Loss: 1.6479738178531989e-06, Validation Loss: 9.79015620504568e-07\n",
      "Epoch 4905, Training Loss: 3.866683982778341e-06, Validation Loss: 2.8707169158050345e-06\n",
      "Epoch 4906, Training Loss: 8.163590337062487e-07, Validation Loss: 9.512475495649845e-07\n",
      "Epoch 4907, Training Loss: 8.523168162355432e-07, Validation Loss: 9.430719411171409e-07\n",
      "Epoch 4908, Training Loss: 1.4444269709201762e-06, Validation Loss: 1.2564399121344018e-06\n",
      "Epoch 4909, Training Loss: 8.81674623087747e-07, Validation Loss: 9.437402580631572e-07\n",
      "Epoch 4910, Training Loss: 6.318863370324834e-07, Validation Loss: 8.762558527494339e-07\n",
      "Epoch 4911, Training Loss: 1.2771964748026221e-06, Validation Loss: 8.618458360734233e-07\n",
      "Epoch 4912, Training Loss: 9.151755762104585e-07, Validation Loss: 8.861048753292408e-07\n",
      "Epoch 4913, Training Loss: 7.365305236817221e-07, Validation Loss: 8.946172784994148e-07\n",
      "Epoch 4914, Training Loss: 9.418269542038615e-07, Validation Loss: 1.2120413551166879e-06\n",
      "Epoch 4915, Training Loss: 7.70986105180782e-07, Validation Loss: 1.3038224390810144e-06\n",
      "Epoch 4916, Training Loss: 8.847679282553145e-07, Validation Loss: 9.453967067879806e-07\n",
      "Epoch 4917, Training Loss: 1.0253132813886623e-06, Validation Loss: 8.449006898106189e-07\n",
      "Epoch 4918, Training Loss: 6.586100198546774e-07, Validation Loss: 8.814889328361785e-07\n",
      "Epoch 4919, Training Loss: 8.021449389161717e-07, Validation Loss: 8.324943660857275e-07\n",
      "Epoch 4920, Training Loss: 7.464594773409772e-07, Validation Loss: 8.514776511442708e-07\n",
      "Epoch 4921, Training Loss: 8.457274702777795e-07, Validation Loss: 1.712219704606735e-06\n",
      "Epoch 4922, Training Loss: 5.259386171019287e-07, Validation Loss: 8.645839274406496e-07\n",
      "Epoch 4923, Training Loss: 1.5307581406887039e-06, Validation Loss: 1.0497127519487155e-06\n",
      "Epoch 4924, Training Loss: 1.674515715421876e-06, Validation Loss: 9.949639417471097e-07\n",
      "Epoch 4925, Training Loss: 8.619426807854325e-07, Validation Loss: 8.673326494323527e-07\n",
      "Epoch 4926, Training Loss: 8.256583896582015e-07, Validation Loss: 1.0213222946317352e-06\n",
      "Epoch 4927, Training Loss: 1.1444703886809293e-06, Validation Loss: 9.171322859940444e-07\n",
      "Epoch 4928, Training Loss: 9.063929837793694e-07, Validation Loss: 8.997998448811217e-07\n",
      "Epoch 4929, Training Loss: 5.068779955763603e-07, Validation Loss: 8.437267531882556e-07\n",
      "Epoch 4930, Training Loss: 7.602930054417811e-07, Validation Loss: 8.873898801904282e-07\n",
      "Epoch 4931, Training Loss: 1.2233384723003837e-06, Validation Loss: 1.48210578028078e-06\n",
      "Epoch 4932, Training Loss: 5.167646577319829e-07, Validation Loss: 1.001874050953815e-06\n",
      "Epoch 4933, Training Loss: 9.041822295330348e-07, Validation Loss: 8.267864345428795e-07\n",
      "Epoch 4934, Training Loss: 5.414160000327684e-07, Validation Loss: 8.976378184789106e-07\n",
      "Epoch 4935, Training Loss: 1.9418475858401507e-06, Validation Loss: 8.960446656251087e-07\n",
      "Epoch 4936, Training Loss: 1.2365742804831825e-05, Validation Loss: 2.813622937670968e-06\n",
      "Epoch 4937, Training Loss: 5.827068889630027e-07, Validation Loss: 9.77450626685672e-07\n",
      "Epoch 4938, Training Loss: 1.0539170034462586e-06, Validation Loss: 8.808230594330531e-07\n",
      "Epoch 4939, Training Loss: 9.904263151838677e-07, Validation Loss: 1.0090841386314585e-06\n",
      "Epoch 4940, Training Loss: 6.682090543108643e-07, Validation Loss: 8.918095363841745e-07\n",
      "Epoch 4941, Training Loss: 8.112032787721546e-07, Validation Loss: 8.729333820715191e-07\n",
      "Epoch 4942, Training Loss: 2.045436303887982e-06, Validation Loss: 2.8709706387585097e-06\n",
      "Epoch 4943, Training Loss: 7.27458768778888e-07, Validation Loss: 9.6281662449163e-07\n",
      "Epoch 4944, Training Loss: 1.2401919775584247e-06, Validation Loss: 2.093042048785441e-06\n",
      "Epoch 4945, Training Loss: 1.2290990980545757e-06, Validation Loss: 1.1831964272436855e-06\n",
      "Epoch 4946, Training Loss: 1.0602237807688653e-06, Validation Loss: 1.069411567571994e-06\n",
      "Epoch 4947, Training Loss: 1.1180172805325128e-06, Validation Loss: 1.4324172070701024e-06\n",
      "Epoch 4948, Training Loss: 8.741748729335086e-07, Validation Loss: 8.480989793419224e-07\n",
      "Epoch 4949, Training Loss: 4.897062240161176e-07, Validation Loss: 8.391370578723071e-07\n",
      "Epoch 4950, Training Loss: 1.1355275546520716e-06, Validation Loss: 9.465130735181076e-07\n",
      "Epoch 4951, Training Loss: 7.650475595255557e-07, Validation Loss: 8.714241918351847e-07\n",
      "Epoch 4952, Training Loss: 9.368685596200521e-07, Validation Loss: 8.96923510319226e-07\n",
      "Epoch 4953, Training Loss: 1.8982990468430216e-06, Validation Loss: 1.4057461207030545e-06\n",
      "Epoch 4954, Training Loss: 8.063434506766498e-07, Validation Loss: 9.096007636272027e-07\n",
      "Epoch 4955, Training Loss: 1.3941588576926733e-06, Validation Loss: 1.07252340640956e-06\n",
      "Epoch 4956, Training Loss: 1.3089311323710717e-06, Validation Loss: 8.901712755620566e-07\n",
      "Epoch 4957, Training Loss: 9.268287044505996e-07, Validation Loss: 8.3993029330011e-07\n",
      "Epoch 4958, Training Loss: 1.130474856836372e-06, Validation Loss: 1.5674670434653853e-06\n",
      "Epoch 4959, Training Loss: 7.675298547837883e-07, Validation Loss: 9.271070999822803e-07\n",
      "Epoch 4960, Training Loss: 6.487231871687982e-07, Validation Loss: 1.402428325667886e-06\n",
      "Epoch 4961, Training Loss: 2.3321595108427573e-06, Validation Loss: 2.6146548179895035e-06\n",
      "Epoch 4962, Training Loss: 2.687158257685951e-06, Validation Loss: 2.8199596913886137e-06\n",
      "Epoch 4963, Training Loss: 9.086613204090099e-07, Validation Loss: 1.0299389783210194e-06\n",
      "Epoch 4964, Training Loss: 9.664184972280054e-07, Validation Loss: 8.876458280025589e-07\n",
      "Epoch 4965, Training Loss: 6.675711006209895e-07, Validation Loss: 9.004276205215105e-07\n",
      "Epoch 4966, Training Loss: 2.237648004665971e-06, Validation Loss: 9.932957141245424e-07\n",
      "Epoch 4967, Training Loss: 1.4543659290211508e-06, Validation Loss: 2.0430870269366636e-06\n",
      "Epoch 4968, Training Loss: 1.152517711489054e-06, Validation Loss: 1.0365455072839786e-06\n",
      "Epoch 4969, Training Loss: 5.963267994957278e-07, Validation Loss: 9.91790055240347e-07\n",
      "Epoch 4970, Training Loss: 9.504707918495114e-07, Validation Loss: 1.1953539789139576e-06\n",
      "Epoch 4971, Training Loss: 6.104063459133613e-07, Validation Loss: 9.596312729931373e-07\n",
      "Epoch 4972, Training Loss: 5.544260375245358e-07, Validation Loss: 1.0420373990773963e-06\n",
      "Epoch 4973, Training Loss: 7.840725402274984e-07, Validation Loss: 1.1093435827130813e-06\n",
      "Epoch 4974, Training Loss: 9.174730166705558e-07, Validation Loss: 8.972361844238097e-07\n",
      "Epoch 4975, Training Loss: 9.493874131294433e-07, Validation Loss: 8.549547547486066e-07\n",
      "Epoch 4976, Training Loss: 1.0088697308674455e-06, Validation Loss: 8.791253094386775e-07\n",
      "Epoch 4977, Training Loss: 1.0999879123119172e-06, Validation Loss: 1.0876455713763685e-06\n",
      "Epoch 4978, Training Loss: 1.5571090443700086e-06, Validation Loss: 9.516786232611855e-07\n",
      "Epoch 4979, Training Loss: 6.662453415628988e-07, Validation Loss: 8.481971115392657e-07\n",
      "Epoch 4980, Training Loss: 1.6487647371832281e-06, Validation Loss: 1.0181564491153514e-06\n",
      "Epoch 4981, Training Loss: 8.900673833522887e-07, Validation Loss: 9.076862151866819e-07\n",
      "Epoch 4982, Training Loss: 6.389079203472647e-07, Validation Loss: 1.0099606599866024e-06\n",
      "Epoch 4983, Training Loss: 1.354112328044721e-06, Validation Loss: 9.411795594376159e-07\n",
      "Epoch 4984, Training Loss: 6.441093205467041e-07, Validation Loss: 9.28868666852007e-07\n",
      "Epoch 4985, Training Loss: 7.900115406300756e-07, Validation Loss: 9.464054649435442e-07\n",
      "Epoch 4986, Training Loss: 8.979638437267567e-07, Validation Loss: 9.545351716065378e-07\n",
      "Epoch 4987, Training Loss: 8.068765851021453e-07, Validation Loss: 9.613906866920872e-07\n",
      "Epoch 4988, Training Loss: 1.0454863286213367e-06, Validation Loss: 1.677150378921275e-06\n",
      "Epoch 4989, Training Loss: 1.3055631598035689e-06, Validation Loss: 3.0625844409056807e-06\n",
      "Epoch 4990, Training Loss: 9.63363163464237e-07, Validation Loss: 1.6258351230239577e-06\n",
      "Epoch 4991, Training Loss: 1.017198087538418e-06, Validation Loss: 9.491446397752987e-07\n",
      "Epoch 4992, Training Loss: 1.0488887483006692e-06, Validation Loss: 9.877319613878849e-07\n",
      "Epoch 4993, Training Loss: 1.189744125440484e-06, Validation Loss: 8.601708638162812e-07\n",
      "Epoch 4994, Training Loss: 2.4050559659372084e-06, Validation Loss: 2.924069892409314e-06\n",
      "Epoch 4995, Training Loss: 8.963691016106168e-07, Validation Loss: 8.547404088968523e-07\n",
      "Epoch 4996, Training Loss: 1.137176013799035e-06, Validation Loss: 1.0541228457759933e-06\n",
      "Epoch 4997, Training Loss: 1.7641377780819312e-06, Validation Loss: 1.1996753513882534e-06\n",
      "Epoch 4998, Training Loss: 3.568133251974359e-06, Validation Loss: 1.9056345206029323e-06\n",
      "Epoch 4999, Training Loss: 2.608079284982523e-06, Validation Loss: 8.869684959437532e-07\n",
      "Epoch 5000, Training Loss: 1.063035597326234e-06, Validation Loss: 8.899545620189798e-07\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Instantiate model, optimizer, and loss function\n",
    "model = ICN(num_layers=5, in_channels=3).cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()  # Mean Squared Error loss for simplicity\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, _ in train_loader:\n",
    "        images = images.cuda()\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        \n",
    "        loss = criterion(output, images)  # Assuming reconstructing input\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, _ in val_loader:\n",
    "            images = images.cuda()\n",
    "            output = model(images)\n",
    "            val_loss += criterion(output, images).item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Training Loss: {loss.item()}, Validation Loss: {val_loss/len(val_loader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-12.972879..14.507242].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-12.345912..11.991253].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-12.498028..11.196629].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-14.855697..11.253397].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-12.660419..11.714074].\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_images(model, num_samples):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        latent_samples = torch.randn(num_samples, 3, 32, 32).cuda()  # Random latent samples\n",
    "        generated_images = model.inverse(latent_samples)\n",
    "    return generated_images.cpu()\n",
    "\n",
    "# Generate and visualize images\n",
    "num_samples = 5\n",
    "generated_images = generate_images(model, num_samples)\n",
    "\n",
    "# Plot the generated images\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(num_samples):\n",
    "    plt.subplot(1, num_samples, i + 1)\n",
    "    plt.imshow((generated_images[i].permute(1, 2, 0) + 1) / 2)  # Denormalize\n",
    "    plt.axis('off')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
